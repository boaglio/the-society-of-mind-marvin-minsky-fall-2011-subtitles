1
00:00:00,000 --> 00:00:02,400
The following content is provided under a Creative

2
00:00:02,400 --> 00:00:03,780
Commons license.

3
00:00:03,780 --> 00:00:06,000
Your support will help MIT OpenCourseWare

4
00:00:06,000 --> 00:00:10,080
continue to offer high quality educational resources for free.

5
00:00:10,080 --> 00:00:12,640
To make a donation or to view additional materials

6
00:00:12,640 --> 00:00:16,600
from hundreds of MIT courses, visit MIT OpenCourseWare

7
00:00:16,600 --> 00:00:17,800
at ocw.mit.edu.

8
00:00:22,600 --> 00:00:25,560
I presume everyone has an urgent question to ask.

9
00:00:30,520 --> 00:00:36,440
Maybe I'll have to point to someone.

10
00:00:36,440 --> 00:00:39,320
There's one over there.

11
00:00:39,320 --> 00:00:40,680
Oh, good.

12
00:00:40,680 --> 00:00:43,840
So I don't know if that's exactly what's said,

13
00:00:43,840 --> 00:00:47,200
but you said that maybe the game lights are associated

14
00:00:47,200 --> 00:00:48,640
to the glia cells.

15
00:00:48,640 --> 00:00:51,520
Is that right?

16
00:00:51,520 --> 00:00:57,080
Oh, I don't want to speculate on how the brain works,

17
00:00:57,080 --> 00:01:05,560
because there's this huge community of neuroscientists

18
00:01:05,560 --> 00:01:10,080
who write papers about, and they're very strange papers,

19
00:01:10,080 --> 00:01:13,920
because they talk about how maybe it's not the neuron.

20
00:01:13,920 --> 00:01:17,320
And I just downloaded a long paper

21
00:01:17,320 --> 00:01:23,800
by someone whose name I won't mention about the idea

22
00:01:23,800 --> 00:01:29,320
that a typical neuron has 100,000 connections.

23
00:01:29,320 --> 00:01:31,840
And so something awesomely important

24
00:01:31,840 --> 00:01:35,840
must go on inside the neuron's body.

25
00:01:35,840 --> 00:01:38,440
And it's got all these little fibers and things.

26
00:01:38,440 --> 00:01:43,220
And presumably, if it's dealing with 100,000 signals

27
00:01:43,220 --> 00:01:46,680
or something, then it must be very complicated.

28
00:01:46,680 --> 00:01:50,520
So maybe the neuron isn't smart enough to do that.

29
00:01:50,520 --> 00:01:55,200
So maybe the other cells nearby that support the neurons

30
00:01:55,200 --> 00:01:59,640
and feed them and send chemicals to and fro around there

31
00:01:59,640 --> 00:02:01,140
have something to do with it.

32
00:02:01,140 --> 00:02:03,220
How many of you have read such articles?

33
00:02:06,000 --> 00:02:11,480
It's a very strange community, because I

34
00:02:11,480 --> 00:02:21,840
think the problem is that history of that science

35
00:02:21,840 --> 00:02:27,080
started, first, it was generally thought

36
00:02:27,080 --> 00:02:29,760
that all the neurons were connected.

37
00:02:29,760 --> 00:02:36,420
And then around 1890 was the first clear idea

38
00:02:36,420 --> 00:02:39,340
that nerve cells weren't arranged

39
00:02:39,340 --> 00:02:42,500
in a continuous network.

40
00:02:42,500 --> 00:02:44,420
I think it was generally believed

41
00:02:44,420 --> 00:02:46,300
that they were all connected to each other,

42
00:02:46,300 --> 00:02:49,740
because as far as you could tell with the microscopes

43
00:02:49,740 --> 00:02:58,780
of the time, it didn't show enough.

44
00:02:58,780 --> 00:03:06,380
And then the hypothesis that the neurons are separate

45
00:03:06,380 --> 00:03:10,660
and there are little gaps called synapses,

46
00:03:10,660 --> 00:03:15,580
as far as I can tell, started around the 1890s.

47
00:03:15,580 --> 00:03:24,300
And from then on, as far as I can see,

48
00:03:24,300 --> 00:03:28,740
neurology and psychology became more and more separate.

49
00:03:28,740 --> 00:03:34,540
And the neurologists got obsessed with chemicals,

50
00:03:34,540 --> 00:03:41,740
hormones, epinephrine, and there are about a dozen chemicals

51
00:03:41,740 --> 00:03:45,260
involved that you can detect when

52
00:03:45,260 --> 00:03:48,580
parts of the brain are activated.

53
00:03:48,580 --> 00:03:55,620
And so a whole bunch of folklore grew up

54
00:03:55,620 --> 00:03:58,300
about the roles of these chemicals.

55
00:03:58,300 --> 00:04:00,140
And one thought of some chemicals

56
00:04:00,140 --> 00:04:03,340
as inhibitory and excitatory.

57
00:04:03,380 --> 00:04:09,040
And that idea still spreads, although what

58
00:04:09,040 --> 00:04:10,740
we know about the nervous system now,

59
00:04:10,740 --> 00:04:13,060
and I think I mentioned this before,

60
00:04:13,060 --> 00:04:16,980
is that in general, if you trace a neural pathway

61
00:04:16,980 --> 00:04:20,620
from one part of the brain to another, what happens

62
00:04:20,620 --> 00:04:22,980
is that the connections tend to alternate.

63
00:04:22,980 --> 00:04:25,380
Not always, but frequently.

64
00:04:25,380 --> 00:04:28,740
So that this connection might inhibit this neuron,

65
00:04:28,740 --> 00:04:31,820
and then you look at the output of that neuron,

66
00:04:31,820 --> 00:04:35,940
and that might tend to excite neurons in the next brain

67
00:04:35,940 --> 00:04:36,900
center.

68
00:04:36,900 --> 00:04:43,700
And then most of those cells would tend to inhibit.

69
00:04:43,700 --> 00:04:47,420
I mean, each brain center gets inputs from several others.

70
00:04:47,420 --> 00:04:49,160
And so it's not that a brain center

71
00:04:49,160 --> 00:04:53,300
is excitatory or inhibitory, but the connections

72
00:04:53,300 --> 00:04:57,620
from one brain center to another tend to have this effect.

73
00:04:57,620 --> 00:05:03,020
And that's probably necessary from a systems dynamic point

74
00:05:03,020 --> 00:05:07,540
of view, because if all neurons tended to either do nothing

75
00:05:07,540 --> 00:05:13,660
or excite the next brain center, then what would happen?

76
00:05:13,660 --> 00:05:16,540
As soon as you got a certain level of excitement,

77
00:05:16,540 --> 00:05:20,540
then more and more brain centers would get activated,

78
00:05:20,540 --> 00:05:21,980
and the whole thing would explode.

79
00:05:21,980 --> 00:05:24,860
And that's more or less what happens

80
00:05:24,860 --> 00:05:28,340
in an epileptic seizure, where if you

81
00:05:28,340 --> 00:05:31,700
get enough electrical and chemical activity of one

82
00:05:31,700 --> 00:05:34,780
kind or another, mostly electrical, I think,

83
00:05:34,780 --> 00:05:38,740
but I don't know, then whole large parts of the brain

84
00:05:38,740 --> 00:05:44,340
start to fire synchronously, and the thing spreads very much

85
00:05:44,340 --> 00:05:46,220
like a forest fire.

86
00:05:46,220 --> 00:05:53,340
So that's a long rant.

87
00:05:53,460 --> 00:05:55,380
I guess I've repeated it several times,

88
00:05:55,380 --> 00:05:59,180
but it's hard to communicate with that community

89
00:05:59,180 --> 00:06:04,700
because they really want to find the secret of thinking

90
00:06:04,700 --> 00:06:08,180
and knowledge in the brain cells rather than

91
00:06:08,180 --> 00:06:11,500
in the architecture of the interconnections.

92
00:06:11,500 --> 00:06:17,220
So my inclination is to find an intermediate level,

93
00:06:17,220 --> 00:06:22,040
such as at least in the cortex, which

94
00:06:22,040 --> 00:06:29,280
is what distinguishes the, does it start in mammals?

95
00:06:29,280 --> 00:06:32,600
I think so.

96
00:06:32,600 --> 00:06:36,260
I think if I'm, rather than a neurology book,

97
00:06:36,260 --> 00:06:40,720
I'm thinking of Carl Sagan's book, which

98
00:06:40,720 --> 00:06:43,600
there's a sort of triune theory that's very popular, which

99
00:06:43,600 --> 00:06:48,540
is that the brain consists of three major divisions,

100
00:06:48,540 --> 00:06:52,700
and the, I forget what the lowest level one is called,

101
00:06:52,700 --> 00:07:02,680
but the middle level is sort of the amphibian and then

102
00:07:02,680 --> 00:07:08,060
the mammalian, and it's in the mammalian development

103
00:07:08,060 --> 00:07:12,340
that most of the large parts of the brain are cortex.

104
00:07:12,340 --> 00:07:19,820
And the cortex isn't so much like a tangled neural net,

105
00:07:19,820 --> 00:07:23,900
but it's divided mainly into columns.

106
00:07:23,900 --> 00:07:27,580
And each column, these vertical columns,

107
00:07:27,580 --> 00:07:29,660
tend to have six or seven layers.

108
00:07:29,660 --> 00:07:34,540
I think six is the standard, and the whole thing

109
00:07:34,540 --> 00:07:39,960
is about four millimeters, four or five millimeters thick.

110
00:07:42,900 --> 00:07:43,900
Maybe a little more.

111
00:07:46,780 --> 00:07:50,100
And in each of these columns, there's

112
00:07:50,100 --> 00:07:53,320
major columns, which have about 1,000 neurons,

113
00:07:53,320 --> 00:07:56,500
and one of these columns is made of maybe 10 or 20

114
00:07:56,500 --> 00:08:04,620
of these many columns that are 50 or 100 or whatever.

115
00:08:04,620 --> 00:08:08,380
And so my inclination is to suspect

116
00:08:08,380 --> 00:08:12,340
that since these are the animals that think and plan

117
00:08:12,340 --> 00:08:16,420
many steps ahead and do all the sorts of things

118
00:08:16,420 --> 00:08:20,340
we take for granted in humans, that we

119
00:08:20,340 --> 00:08:27,620
want to look there for the architecture of memory

120
00:08:27,620 --> 00:08:30,140
and problem solving systems.

121
00:08:30,140 --> 00:08:34,220
In the animals without cortexes, you

122
00:08:34,220 --> 00:08:36,980
can account for most of their behavior

123
00:08:36,980 --> 00:08:42,340
in terms of fairly low-level immediate stimulus response

124
00:08:42,340 --> 00:08:47,660
reflexes and large major states, like turning on some parts

125
00:08:47,660 --> 00:08:51,940
of some big blocks of these reflexes when it's hungry

126
00:08:51,940 --> 00:08:56,460
and turn on other blocks when there's an environmental threat

127
00:08:56,460 --> 00:08:59,900
and so forth or whatever.

128
00:09:02,500 --> 00:09:05,780
Anyway, I forget what.

129
00:09:05,780 --> 00:09:07,280
Yes.

130
00:09:07,280 --> 00:09:11,780
So in chapter three, you talked about the stages

131
00:09:11,780 --> 00:09:14,780
we go through when we face something

132
00:09:14,780 --> 00:09:17,780
like your car breaks down and you can't go to work.

133
00:09:17,780 --> 00:09:19,780
That's an example given in the book.

134
00:09:19,780 --> 00:09:24,780
I'm wondering, how do you decide that we transition

135
00:09:24,780 --> 00:09:26,780
from one stage to another?

136
00:09:26,780 --> 00:09:34,780
And why do we go through stages of denial, bargaining,

137
00:09:34,780 --> 00:09:42,780
frustration, depression, and then only the last stage seems

138
00:09:42,780 --> 00:09:43,780
productive?

139
00:09:43,780 --> 00:09:46,780
I guess my main question is, how do we

140
00:09:46,780 --> 00:09:50,780
decide that we should transition from one stage to another?

141
00:09:50,780 --> 00:09:51,780
What marks the end?

142
00:09:51,780 --> 00:09:54,780
And why do we start over?

143
00:09:54,780 --> 00:09:55,780
That's a beautiful question.

144
00:10:00,500 --> 00:10:04,740
I think it's fairly well understood in the invertebrates

145
00:10:04,740 --> 00:10:08,140
that there are different centers in the brain

146
00:10:08,140 --> 00:10:10,980
for different activities.

147
00:10:10,980 --> 00:10:16,340
And I'm not sure how much is known

148
00:10:16,340 --> 00:10:19,180
about how these things switch.

149
00:10:19,180 --> 00:10:25,580
How does an animal decide whether it's time to,

150
00:10:25,580 --> 00:10:29,540
for example, most animals are either diurnal or nocturnal.

151
00:10:29,540 --> 00:10:35,420
So some stimulus comes along like it's getting dark.

152
00:10:35,420 --> 00:10:39,980
And a nocturnal animal might then start waking up.

153
00:10:39,980 --> 00:10:42,580
And it turns on some part of the brain

154
00:10:42,580 --> 00:10:45,620
and turns off some other parts.

155
00:10:45,620 --> 00:10:48,220
And it starts to sneak around looking for food

156
00:10:48,220 --> 00:10:50,420
or whatever it does at night.

157
00:10:50,420 --> 00:10:57,300
Whereas a daily animal, when it starts to get dark,

158
00:10:57,300 --> 00:11:02,220
that might trigger some brain center to turn on.

159
00:11:02,220 --> 00:11:07,460
And it looks for its place to sleep and goes and hides.

160
00:11:07,460 --> 00:11:10,900
So some of these are due to external things.

161
00:11:10,900 --> 00:11:13,860
Then, of course, there are internal clocks.

162
00:11:13,860 --> 00:11:17,860
So for lots of animals, if you put it

163
00:11:17,860 --> 00:11:22,980
in a box that's dimly illuminated

164
00:11:22,980 --> 00:11:26,700
and it has a 24-hour cycle of some sort,

165
00:11:26,700 --> 00:11:31,540
it might persist in that cycle for quite a few days

166
00:11:31,540 --> 00:11:40,020
and go to sleep every 24 hours for half the time and so on.

167
00:11:40,020 --> 00:11:44,780
A friend of mine once decided he would see about this.

168
00:11:44,780 --> 00:11:52,740
And it's a famous AI theorist named Ray Solomonoff.

169
00:11:52,740 --> 00:11:59,620
And he put black paint on all his windows

170
00:11:59,620 --> 00:12:06,580
and found that he had a 25- or 26-hour natural cycle, which

171
00:12:06,580 --> 00:12:12,540
was very nice.

172
00:12:12,540 --> 00:12:17,580
And this persisted for several months.

173
00:12:17,580 --> 00:12:22,060
I had another friend who lived in the New York subways

174
00:12:22,060 --> 00:12:26,460
because his apartment was in a building that

175
00:12:26,460 --> 00:12:28,740
had an entrance to the subway.

176
00:12:28,740 --> 00:12:32,740
And he stayed out of daylight for six months.

177
00:12:32,740 --> 00:12:39,300
But anyway, he too found that he preferred

178
00:12:39,300 --> 00:12:45,500
to be on a 25- or 26-hour day than 24.

179
00:12:45,500 --> 00:12:47,660
I'm rambling.

180
00:12:47,660 --> 00:12:52,500
So we apparently have several different systems.

181
00:12:52,500 --> 00:12:54,220
So there's dead reckoning system,

182
00:12:54,220 --> 00:12:58,300
where some internal clocks are regulating your behavior.

183
00:12:58,300 --> 00:13:02,380
And then there are other systems where people are very much

184
00:13:02,380 --> 00:13:09,780
affected by the amount of light and so forth.

185
00:13:09,780 --> 00:13:12,380
So we probably have four or five ways

186
00:13:12,380 --> 00:13:15,580
of doing almost everything that's important.

187
00:13:15,580 --> 00:13:19,500
And then people get various disorders

188
00:13:19,500 --> 00:13:25,060
where some of these systems fail and a person doesn't

189
00:13:25,060 --> 00:13:26,620
have a regular sleep cycle.

190
00:13:26,620 --> 00:13:32,220
And there are disorders where people fall asleep.

191
00:13:32,220 --> 00:13:35,780
What's it called when you fall asleep every few minutes?

192
00:13:35,780 --> 00:13:36,500
Narcolepsy.

193
00:13:36,500 --> 00:13:42,060
Narcolepsy and all sorts of wonderful disorders

194
00:13:42,060 --> 00:13:44,860
just because the brain has evolved

195
00:13:44,860 --> 00:13:49,500
so many different ways of doing anything that's very important.

196
00:13:55,180 --> 00:13:56,980
Yeah?

197
00:13:56,980 --> 00:14:00,100
Can you describe the best piece of criticism

198
00:14:00,100 --> 00:14:02,500
for the Society of Mind Theory?

199
00:14:02,500 --> 00:14:03,500
Best piece of what?

200
00:14:03,500 --> 00:14:05,660
The best criticism.

201
00:14:05,660 --> 00:14:06,160
Oh.

202
00:14:13,060 --> 00:14:18,460
It reminds me of an article I recently

203
00:14:18,460 --> 00:14:27,260
read about the possibility of a virus for what's

204
00:14:27,260 --> 00:14:28,300
the disorder where?

205
00:14:30,900 --> 00:14:32,100
Alzheimer's?

206
00:14:32,100 --> 00:14:32,600
No.

207
00:14:36,100 --> 00:14:39,580
Actually, there isn't any generally accepted

208
00:14:39,580 --> 00:14:42,620
cause for Alzheimer's, as far as I know.

209
00:14:45,620 --> 00:14:46,100
What?

210
00:14:46,100 --> 00:14:48,740
Somebody did an experiment where they injected Alzheimer's

211
00:14:48,740 --> 00:14:52,700
infected matter into someone and they got the same lack.

212
00:14:52,700 --> 00:14:55,780
Oh, well, right.

213
00:14:58,820 --> 00:15:01,620
I wonder if that's a popular theory.

214
00:15:05,020 --> 00:15:08,380
No, what's the one where people are?

215
00:15:08,380 --> 00:15:10,060
Fibromyalgia.

216
00:15:10,060 --> 00:15:11,020
Say it again?

217
00:15:11,020 --> 00:15:12,300
Fibromyalgia.

218
00:15:12,300 --> 00:15:14,620
Yes, right.

219
00:15:14,620 --> 00:15:18,300
Which is not recognized by most theorists

220
00:15:18,300 --> 00:15:21,980
to be a definite disease.

221
00:15:21,980 --> 00:15:28,580
But there's been an episode in which somebody,

222
00:15:28,580 --> 00:15:32,500
I forget what her name is, was pretty sure

223
00:15:32,500 --> 00:15:35,620
that she had found a virus for it.

224
00:15:35,620 --> 00:15:41,820
And every now and then, somebody revives that theory

225
00:15:41,820 --> 00:15:46,260
and tries to get more evidence for it.

226
00:15:46,260 --> 00:15:51,940
Anyway, there must be disorders where the programming is bad

227
00:15:51,940 --> 00:15:56,500
rather than a biochemical disorder,

228
00:15:56,500 --> 00:16:04,100
because whatever the brain is, the adult brain

229
00:16:04,100 --> 00:16:07,300
certainly has a very large component

230
00:16:07,300 --> 00:16:10,620
of what we would, in any other case,

231
00:16:10,620 --> 00:16:13,420
consider to be software, namely lots of things

232
00:16:13,420 --> 00:16:18,640
that you've learned, including ways for one part of the brain

233
00:16:18,640 --> 00:16:23,180
to discover how to modulate or turn on or turn off

234
00:16:23,180 --> 00:16:25,300
other parts of the brain.

235
00:16:25,300 --> 00:16:30,980
And since we've only had this kind of cortex

236
00:16:30,980 --> 00:16:33,700
for four or five million years, it's

237
00:16:33,700 --> 00:16:36,620
probably still got lots of bugs.

238
00:16:36,620 --> 00:16:42,820
Evolution never knows what, when you make a new innovation,

239
00:16:42,820 --> 00:16:45,380
you don't know what's going to come after that that

240
00:16:45,380 --> 00:16:51,580
might find bugs and ways to get short-range advantages,

241
00:16:51,580 --> 00:16:55,220
short-term advantages at the expense of longer-term

242
00:16:55,220 --> 00:16:56,760
advantages.

243
00:16:56,760 --> 00:17:01,480
So lots of mental diseases might be software bugs.

244
00:17:01,480 --> 00:17:03,720
And a few of them are known to be

245
00:17:03,720 --> 00:17:12,480
connected to abnormal secretions of chemicals and so forth.

246
00:17:12,480 --> 00:17:15,880
But even in those cases, it's hard to be sure

247
00:17:15,880 --> 00:17:19,800
that the overproduction or underproduction

248
00:17:19,800 --> 00:17:30,680
of a neurologically important chemical is a biological

249
00:17:30,680 --> 00:17:33,520
disorder or a functional disorder

250
00:17:33,520 --> 00:17:35,920
because some part of the nervous system

251
00:17:35,920 --> 00:17:42,080
might have found some trick to cause abnormal secretions

252
00:17:42,080 --> 00:17:44,560
of some substance.

253
00:17:44,560 --> 00:17:49,760
So that's the sort of thing that we

254
00:17:49,760 --> 00:17:54,200
can expect to learn a great deal more about in the next

255
00:17:54,200 --> 00:18:00,520
generation because of the lower cost and greater resolution

256
00:18:00,520 --> 00:18:08,120
of brain scanning techniques and what's his name,

257
00:18:08,120 --> 00:18:12,280
and new synthetic ways of putting in fluorescent

258
00:18:12,280 --> 00:18:17,720
chemicals into a normal brain without injuring it much

259
00:18:17,720 --> 00:18:23,440
so that you can now do sort of macrochemical experiments

260
00:18:23,440 --> 00:18:27,400
of seeing what chemicals are being secreted

261
00:18:27,400 --> 00:18:32,040
in the brain with new kinds of scanning techniques.

262
00:18:32,040 --> 00:18:34,800
So neuroscience is going to be very exciting

263
00:18:34,800 --> 00:18:39,800
in the next generation with all the great new instruments.

264
00:18:43,200 --> 00:18:50,760
As you know, my complaint is that somehow introduction

265
00:18:50,760 --> 00:18:53,960
to the, I'm not saying any of the present AI

266
00:18:53,960 --> 00:18:57,520
theories have been confirmed to tell you

267
00:18:57,520 --> 00:19:01,600
that the brain works as such and such a rule-based system

268
00:19:01,600 --> 00:19:07,200
or such and such a, or use Winston type representations

269
00:19:07,200 --> 00:19:09,800
or Roger Shank type representations

270
00:19:09,800 --> 00:19:12,920
or scripts or frames or whatever.

271
00:19:12,920 --> 00:19:18,560
And the next to last chapter of the emotion machine

272
00:19:18,560 --> 00:19:22,600
sort of summarizes, I think, almost a dozen

273
00:19:22,600 --> 00:19:29,200
different AI theories of ways to represent knowledge.

274
00:19:29,200 --> 00:19:35,560
Nobody has confirmed that any of those particular ideas

275
00:19:35,560 --> 00:19:41,400
represent what happens in a mammalian brain.

276
00:19:41,400 --> 00:19:47,520
And the problem is that the, to me,

277
00:19:47,520 --> 00:19:49,880
is that the neuroscience community just

278
00:19:49,880 --> 00:19:53,320
doesn't read that stuff and doesn't design experiments

279
00:19:53,320 --> 00:19:55,080
to look for them.

280
00:19:58,080 --> 00:20:03,200
David has been moving from computer science and AI

281
00:20:03,200 --> 00:20:05,160
into that.

282
00:20:05,160 --> 00:20:08,480
So he's my current source of knowledge

283
00:20:08,480 --> 00:20:09,800
about what's happening there.

284
00:20:12,640 --> 00:20:16,480
Have any of you been following contemporary neuroscience?

285
00:20:20,600 --> 00:20:22,680
That's strange.

286
00:20:22,680 --> 00:20:23,180
Yeah.

287
00:20:23,180 --> 00:20:31,940
So you were talking about software a little bit.

288
00:20:31,940 --> 00:20:37,460
So I think they analyzed Einstein's brain

289
00:20:37,460 --> 00:20:40,700
and realized that's why I talked about the glia cells.

290
00:20:40,700 --> 00:20:49,820
And maybe he had a lot more glia cells than normal neurons.

291
00:20:49,820 --> 00:20:55,620
And so do you believe that the intelligence of humans

292
00:20:55,620 --> 00:21:01,020
is more on the software side or on the hardware side?

293
00:21:01,020 --> 00:21:03,500
We have computers that are very, very powerful.

294
00:21:03,500 --> 00:21:08,620
Could we create software that we can run these machines that

295
00:21:08,620 --> 00:21:12,220
were produced in humans?

296
00:21:13,220 --> 00:21:19,220
I don't see any reason to doubt it.

297
00:21:25,220 --> 00:21:29,220
As far as we know, computers can simulate anything.

298
00:21:29,220 --> 00:21:33,700
What they can't do yet, I suppose,

299
00:21:33,700 --> 00:21:37,060
is simulate large-scale quantum phenomenon

300
00:21:37,060 --> 00:21:45,980
because if you know the Feynman theory of quantum mechanics,

301
00:21:45,980 --> 00:21:53,300
it's that if you have a network of physical systems that

302
00:21:53,300 --> 00:21:58,660
are connected, then it's in the nature of physics

303
00:21:58,660 --> 00:22:06,980
that whatever happens from one state to another

304
00:22:06,980 --> 00:22:12,940
in the real universe, whatever happens actually

305
00:22:12,940 --> 00:22:17,380
happens by the wave function.

306
00:22:17,380 --> 00:22:19,580
The wave function represents the sum

307
00:22:19,580 --> 00:22:27,700
of the activities propagating through all possible paths.

308
00:22:27,700 --> 00:22:33,380
So in some sense, that's too exponential to simulate

309
00:22:33,380 --> 00:22:35,380
on a computer.

310
00:22:35,380 --> 00:22:41,380
In other words, I believe the biggest supercomputers

311
00:22:41,380 --> 00:22:46,380
can simulate a helium atom today fairly well.

312
00:22:46,380 --> 00:22:50,060
But they can't simulate a lithium atom

313
00:22:50,060 --> 00:22:55,860
because it's sort of four or five layers of exclamation.

314
00:22:56,020 --> 00:22:59,340
It's between 2 to the 2 to the 2 to the 2

315
00:22:59,340 --> 00:23:03,340
and 4 to the 4 to the 4 to the 4.

316
00:23:03,340 --> 00:23:06,340
Those numbers get out of hand.

317
00:23:06,340 --> 00:23:09,340
But I suspect that the reason the brain works

318
00:23:09,340 --> 00:23:14,340
is that it's evolved to prevent quantum effects from making

319
00:23:14,340 --> 00:23:17,340
things complicated.

320
00:23:17,340 --> 00:23:22,340
The great thing about a neuron is that generally speaking,

321
00:23:22,340 --> 00:23:24,820
a neuron fires all or none.

322
00:23:25,780 --> 00:23:28,780
And at this point, you have to get

323
00:23:28,780 --> 00:23:37,780
a full half-volt of potential between the neuron's fibers

324
00:23:37,780 --> 00:23:39,780
surrounding fluid.

325
00:23:39,780 --> 00:23:44,780
And that's a half a volt is a big difference.

326
00:23:48,260 --> 00:23:52,860
So Blake, do you believe that the software that we have right

327
00:23:52,900 --> 00:23:56,900
now is equivalent to, for example, the intelligence

328
00:23:56,900 --> 00:24:01,900
that we have in dogs or, for example, simple animals?

329
00:24:01,900 --> 00:24:09,900
Is the difference that we just need to implement the software,

330
00:24:09,900 --> 00:24:11,900
like modify the software?

331
00:24:11,900 --> 00:24:17,900
Or somehow we need to create a whole different software that

332
00:24:17,940 --> 00:24:20,940
knows there doesn't seem to be much difference

333
00:24:20,940 --> 00:24:25,940
in the architecture of the local architecture.

334
00:24:25,940 --> 00:24:27,940
I'm turning the microphone off.

335
00:24:27,940 --> 00:24:28,940
I want to be quiet.

336
00:24:28,940 --> 00:24:29,940
Oh, can I turn it off?

337
00:24:29,940 --> 00:24:31,940
Yes.

338
00:24:31,940 --> 00:24:33,940
It's not green.

339
00:24:33,940 --> 00:24:35,940
Yeah, so throw the switch.

340
00:24:35,940 --> 00:24:36,940
Is it green now?

341
00:24:36,940 --> 00:24:37,940
Now it's green.

342
00:24:41,940 --> 00:24:43,940
The difference between the dog and a person

343
00:24:43,980 --> 00:24:47,980
is the huge frontal cortex.

344
00:24:47,980 --> 00:24:50,980
I think the rest of it is fairly similar.

345
00:24:50,980 --> 00:24:53,980
And I presume the hippocampus and the amygdala

346
00:24:53,980 --> 00:24:57,980
and the structures that control which parts of the cortex

347
00:24:57,980 --> 00:25:00,980
are used for what are somewhat different.

348
00:25:00,980 --> 00:25:06,980
But the small details of the all mammalian brains

349
00:25:06,980 --> 00:25:08,980
are practically the same.

350
00:25:09,020 --> 00:25:14,020
I mean, basically you can't make an early genetic change

351
00:25:14,020 --> 00:25:18,020
in how neurons work, or all the brain cells of the offspring

352
00:25:18,020 --> 00:25:22,020
would be somewhat different and the thing would be dead.

353
00:25:22,020 --> 00:25:27,020
So evolution has this property that generally there

354
00:25:27,020 --> 00:25:31,020
are only two places in the development of an embryo

355
00:25:31,020 --> 00:25:36,020
that evolution can operate, namely

356
00:25:36,060 --> 00:25:39,060
in the pre-placental stage.

357
00:25:39,060 --> 00:25:44,060
You can change the way the egg breaks up and evolves

358
00:25:44,060 --> 00:25:48,060
and you can have amazing things like identical twins happen

359
00:25:48,060 --> 00:25:53,060
without any effect on the nature of the adult offspring.

360
00:25:53,060 --> 00:25:57,060
Or you can change the things that happen most recently

361
00:25:57,060 --> 00:26:03,060
in evolution, like little tweaks in how some part of the nervous

362
00:26:03,100 --> 00:26:10,100
system works if it doesn't change earlier stages.

363
00:26:10,100 --> 00:26:13,100
However, mutations that operate in the middle of all that

364
00:26:13,100 --> 00:26:17,100
and change the number of segments in the embryo,

365
00:26:17,100 --> 00:26:21,100
I guess you could have a longer tail or a shorter tail

366
00:26:21,100 --> 00:26:22,100
and that won't affect much.

367
00:26:22,100 --> 00:26:27,100
But if you change the 12 segments of the spine

368
00:26:27,100 --> 00:26:29,100
that the brain develops from, you'd

369
00:26:29,140 --> 00:26:35,140
get a huge alteration in how that animal will think.

370
00:26:35,140 --> 00:26:39,140
So we're sort of, in other words,

371
00:26:39,140 --> 00:26:44,140
evolution cannot change intermediate structures

372
00:26:44,140 --> 00:26:49,140
very much or the animal won't live.

373
00:26:49,140 --> 00:26:50,140
Bob Lohler.

374
00:26:50,140 --> 00:26:54,140
If one thinks of comparing a person to a dog,

375
00:26:54,140 --> 00:26:58,140
would it not be most appropriate to think of those persons

376
00:26:58,180 --> 00:27:02,180
who are like the wild boy of southern France

377
00:27:02,180 --> 00:27:06,180
who grew up in the woods without any language

378
00:27:06,180 --> 00:27:10,180
and say that if you're going to look at an individual's

379
00:27:10,180 --> 00:27:14,180
intelligence, that would be a fair comparison with a dog.

380
00:27:14,180 --> 00:27:18,180
Whereas what we have when we think of people today

381
00:27:18,180 --> 00:27:22,180
is people who have learned so much through interaction

382
00:27:22,180 --> 00:27:25,180
with other people that the transmission of culture

383
00:27:26,180 --> 00:27:28,180
is it not essentially ways of thinking

384
00:27:28,180 --> 00:27:31,180
that have been learned throughout the history

385
00:27:31,180 --> 00:27:33,180
of civilization and some of us are

386
00:27:33,180 --> 00:27:36,180
able to pass on to others?

387
00:27:36,180 --> 00:27:37,180
Sure.

388
00:27:37,180 --> 00:27:42,180
Although if you expose a dog to humans,

389
00:27:42,180 --> 00:27:45,180
it doesn't learn language.

390
00:27:45,180 --> 00:27:48,180
It may or may not come.

391
00:27:48,180 --> 00:27:51,180
Right.

392
00:27:51,180 --> 00:27:58,820
But presumably, language is fairly recent.

393
00:27:58,820 --> 00:28:05,180
So you could have mutations in the structure of the language

394
00:28:05,180 --> 00:28:09,700
centers and still have a human that's alive.

395
00:28:09,700 --> 00:28:11,660
And it might be better at language

396
00:28:11,660 --> 00:28:14,780
than most other people or somewhat worse.

397
00:28:14,780 --> 00:28:18,420
So we could have lots of small mutations in anything

398
00:28:18,420 --> 00:28:20,140
that's been recently evolved.

399
00:28:26,340 --> 00:28:32,580
But the frontal cortexes and the human cortex

400
00:28:32,580 --> 00:28:38,220
is really very large compared to the rest of the brain.

401
00:28:38,220 --> 00:28:42,100
Same in dolphins and a couple of other animals.

402
00:28:42,100 --> 00:28:43,860
I forget.

403
00:28:43,860 --> 00:28:45,540
Whales.

404
00:28:45,540 --> 00:28:46,220
Yeah.

405
00:28:46,220 --> 00:28:50,100
So the reason why I ask that is that it seems to me

406
00:28:50,100 --> 00:28:54,580
that we have some quality, like some kind of we

407
00:28:54,580 --> 00:28:59,300
can see the world, like add some qualities to the world.

408
00:28:59,300 --> 00:29:03,580
And this is what I would call consciousness.

409
00:29:03,580 --> 00:29:07,220
And for me, it seems that dogs also

410
00:29:07,220 --> 00:29:11,220
have this quality of seeing the world

411
00:29:11,220 --> 00:29:14,140
and adding qualities to the world.

412
00:29:14,140 --> 00:29:18,420
So maybe this is good, this is bad.

413
00:29:18,420 --> 00:29:22,540
There are different qualities for different beings.

414
00:29:22,540 --> 00:29:26,660
And the software that we produce right now

415
00:29:26,660 --> 00:29:33,060
seems to be maybe faster and maybe do more tests than what

416
00:29:33,060 --> 00:29:35,340
maybe a dog does.

417
00:29:35,340 --> 00:29:41,660
But for me, it doesn't seem that it has essentially

418
00:29:41,660 --> 00:29:46,460
these quality things.

419
00:29:46,460 --> 00:29:48,820
It doesn't have consciousness in this sense.

420
00:29:48,820 --> 00:29:54,940
It doesn't aggregate quality to the things in the world.

421
00:29:54,940 --> 00:29:59,820
Well, I think I know what you're getting at.

422
00:29:59,820 --> 00:30:05,860
But you're using that word consciousness,

423
00:30:05,860 --> 00:30:12,260
which I've decided to abandon because it's

424
00:30:12,260 --> 00:30:14,620
36 different things.

425
00:30:14,620 --> 00:30:20,800
And probably a dog has five or six of them or 31.

426
00:30:20,800 --> 00:30:22,620
I don't know.

427
00:30:22,620 --> 00:30:33,540
But one question is, do you think

428
00:30:33,540 --> 00:30:37,540
a dog can think several steps ahead

429
00:30:37,540 --> 00:30:40,420
and consider two alternative?

430
00:30:45,980 --> 00:30:46,580
That's funny.

431
00:30:52,660 --> 00:30:54,740
Oh, let's make this abstract.

432
00:30:54,740 --> 00:30:58,780
So here's a world, and the dog is here,

433
00:30:58,780 --> 00:31:00,940
and it wants to get here.

434
00:31:00,980 --> 00:31:03,940
And there are all sorts of obstacles in it.

435
00:31:03,940 --> 00:31:08,460
So can the dog say, well, if I went this way,

436
00:31:08,460 --> 00:31:10,740
I'd have such and such difficulty,

437
00:31:10,740 --> 00:31:15,500
whereas if I went this way, I'd have this difficulty.

438
00:31:15,500 --> 00:31:19,220
Well, I think this one looks better.

439
00:31:19,220 --> 00:31:23,060
Do you think your dog considers two or three alternatives

440
00:31:23,060 --> 00:31:25,100
and makes plans?

441
00:31:25,100 --> 00:31:31,540
I have no idea, but the curious thing about a person

442
00:31:31,540 --> 00:31:38,900
is you can decide that you're going to not act

443
00:31:38,900 --> 00:31:43,100
in the situation until you've considered 16 plans.

444
00:31:43,100 --> 00:31:45,100
And then one part of your brain is

445
00:31:45,100 --> 00:31:50,460
making these different approaches to the problems.

446
00:31:50,460 --> 00:31:53,580
And another part of your brain is saying, well, now

447
00:31:53,580 --> 00:31:56,380
I've made five plans, and I'm beginning

448
00:31:56,380 --> 00:32:01,180
to forget the first one, so I better reformulate it.

449
00:32:01,180 --> 00:32:07,980
And you're doing all of these self-conscious in the sense

450
00:32:07,980 --> 00:32:13,780
that you're making plans that involve predicting

451
00:32:13,780 --> 00:32:17,140
what decisions you will make.

452
00:32:17,140 --> 00:32:21,260
And instead of making them, you make the decision to say,

453
00:32:21,260 --> 00:32:23,900
I'm going to follow out these two plans

454
00:32:23,900 --> 00:32:29,340
and use the result of that to decide which one to do.

455
00:32:29,340 --> 00:32:31,820
Do you think a dog does any of that?

456
00:32:31,820 --> 00:32:34,420
Does it look around and say, well, I could go that way

457
00:32:34,420 --> 00:32:35,180
or this way?

458
00:32:39,540 --> 00:32:45,300
I remember our dog was good at if you throw a ball,

459
00:32:45,300 --> 00:32:46,900
it would go and get it.

460
00:32:46,900 --> 00:32:50,580
And if you threw two balls, it would go and get both of them.

461
00:32:51,340 --> 00:32:53,560
And sometimes, if you threw three balls,

462
00:32:53,560 --> 00:32:57,540
it would go and get them all.

463
00:32:57,540 --> 00:33:00,580
And sometimes, if a ball would roll under a couch

464
00:33:00,580 --> 00:33:04,740
that it couldn't reach, it would get the other two,

465
00:33:04,740 --> 00:33:07,980
and it would think, and then it would run back to the kitchen

466
00:33:07,980 --> 00:33:10,580
where that ball is usually found.

467
00:33:10,580 --> 00:33:12,940
And then it would come back disappointed.

468
00:33:12,940 --> 00:33:15,580
So what does that mean?

469
00:33:15,580 --> 00:33:17,140
Did it have parallel plans?

470
00:33:17,140 --> 00:33:23,340
Or does it make a new one when the previous one fails,

471
00:33:23,340 --> 00:33:25,660
and they're not actually parallel?

472
00:33:29,740 --> 00:33:31,860
What's your guess?

473
00:33:31,860 --> 00:33:33,620
How far ahead does a dog think?

474
00:33:33,620 --> 00:33:34,420
Do you have a dog?

475
00:33:34,420 --> 00:33:39,620
Yeah, I do have a dog, but I don't

476
00:33:39,620 --> 00:33:43,300
believe that's the essential part of beings

477
00:33:43,300 --> 00:33:47,940
that have some kind of advanced brain.

478
00:33:47,940 --> 00:33:49,340
We can plan ahead.

479
00:33:49,340 --> 00:33:51,060
Humans can plan ahead.

480
00:33:51,060 --> 00:34:03,220
I don't think the fundamental part of intelligence.

481
00:34:03,220 --> 00:34:07,700
Humans, I think Winston says that humans

482
00:34:07,700 --> 00:34:12,220
are better than primates in they can understand stories,

483
00:34:12,220 --> 00:34:15,060
and they can join together stories.

484
00:34:15,060 --> 00:34:22,060
But somehow, I don't buy the story

485
00:34:22,060 --> 00:34:29,420
that primates are just like rule planners.

486
00:34:29,420 --> 00:34:38,020
I think somehow we have some quality machine of the world,

487
00:34:38,020 --> 00:34:43,100
and somehow we're not writing a software.

488
00:34:43,100 --> 00:34:44,820
But you know, it's funny.

489
00:34:44,820 --> 00:34:47,460
Computer science teaches us things

490
00:34:47,460 --> 00:34:51,260
that weren't obvious before.

491
00:34:51,260 --> 00:34:55,580
It might turn out that if you're a computer

492
00:34:55,580 --> 00:35:00,900
and you only have two registers, then, well, in principle,

493
00:35:00,900 --> 00:35:04,780
you could do anything, but that's another matter.

494
00:35:04,780 --> 00:35:07,580
But it might turn out that maybe a dog has only

495
00:35:07,580 --> 00:35:11,300
two registers and a person has four.

496
00:35:11,300 --> 00:35:14,060
And a trivial thing like that makes

497
00:35:14,060 --> 00:35:19,020
it possible to have two plans and put them in suspense

498
00:35:19,020 --> 00:35:23,700
and think about the strategy and come back and change one.

499
00:35:23,700 --> 00:35:28,140
Whereas if you only had two registers,

500
00:35:28,140 --> 00:35:30,460
your mind would be much lower order,

501
00:35:30,460 --> 00:35:32,780
and there's no big difference.

502
00:35:32,860 --> 00:35:38,580
Computer science tells us that the usual way

503
00:35:38,580 --> 00:35:44,140
of thinking about abilities might be wrong.

504
00:35:44,140 --> 00:35:47,580
Before computer science, people didn't really

505
00:35:47,580 --> 00:35:52,220
have that kind of idea.

506
00:35:52,220 --> 00:35:54,700
Many years ago, I was in a contest.

507
00:35:54,700 --> 00:36:02,100
I mean, you know, a science, because some of our friends

508
00:36:02,100 --> 00:36:04,460
showed that you could make a universal computer

509
00:36:04,460 --> 00:36:06,580
with four registers.

510
00:36:06,580 --> 00:36:11,500
And I had discovered some other things,

511
00:36:11,500 --> 00:36:17,380
and I managed to show that you could make a universal computer

512
00:36:17,380 --> 00:36:18,820
with just two registers.

513
00:36:18,820 --> 00:36:27,140
And that was a big surprise to a lot of people.

514
00:36:27,140 --> 00:36:32,860
But there never was anything in the history

515
00:36:32,860 --> 00:36:36,300
of psychology of that nature.

516
00:36:36,300 --> 00:36:45,980
So there never were really technical theories of it's

517
00:36:45,980 --> 00:36:48,260
really computational complexity.

518
00:36:48,260 --> 00:36:51,660
What does it take to solve certain kinds of problems?

519
00:36:51,660 --> 00:36:56,180
And until the 1960s, there weren't any theories of that.

520
00:36:56,180 --> 00:37:02,300
And I'm not sure that that aspect of computer science

521
00:37:02,300 --> 00:37:08,420
has actually reached many psychologists or neuroscientists.

522
00:37:08,420 --> 00:37:10,060
I'm not even sure that it's relevant,

523
00:37:10,060 --> 00:37:14,380
but it's really interesting that the difference between two

524
00:37:14,380 --> 00:37:18,100
and three registers could make an exponential difference

525
00:37:18,100 --> 00:37:22,100
in how fast you could solve certain kinds of problems

526
00:37:22,100 --> 00:37:22,960
and not others.

527
00:37:26,300 --> 00:37:30,060
So maybe there'll be a little more mathematical psychology

528
00:37:30,060 --> 00:37:34,420
in the next couple of decades.

529
00:37:34,420 --> 00:37:37,580
Yeah.

530
00:37:37,580 --> 00:37:43,740
So in artificial intelligence, how much of our efforts

531
00:37:43,740 --> 00:37:48,340
should be devoted to reflecting on our own thinking as humans

532
00:37:48,340 --> 00:37:49,900
and trying to figure out what's really

533
00:37:49,900 --> 00:37:52,460
going on inside our brains and then trying

534
00:37:52,460 --> 00:37:57,500
to implement that versus observing and identifying

535
00:37:57,500 --> 00:38:00,540
what kinds of problems we as humans can solve

536
00:38:00,540 --> 00:38:03,580
and then come up with any sort of way for a computer

537
00:38:03,580 --> 00:38:07,900
to, in a human-like way, solve these problems?

538
00:38:07,900 --> 00:38:09,260
There are a lot of nice questions.

539
00:38:09,260 --> 00:38:13,300
I don't think it doesn't make any sense

540
00:38:13,300 --> 00:38:16,820
to suggest that we think about what's

541
00:38:16,820 --> 00:38:20,460
happening in our brains, because that

542
00:38:20,460 --> 00:38:21,960
takes scientific instruments.

543
00:38:21,960 --> 00:38:28,440
But it certainly makes sense to go over

544
00:38:28,440 --> 00:38:38,360
older theories of psychology and ask what kinds of procedures

545
00:38:38,360 --> 00:38:41,000
to solve a certain kind of problem, what kind of procedures

546
00:38:41,000 --> 00:38:43,520
are absolutely necessary.

547
00:38:43,520 --> 00:38:46,080
And you could find some things like that,

548
00:38:46,080 --> 00:38:48,960
like how many registers would you need

549
00:38:48,960 --> 00:38:54,800
and what kinds of conditionals and what kind of addressing.

550
00:39:00,640 --> 00:39:02,960
So I think a lot of cognitive psychology,

551
00:39:02,960 --> 00:39:07,720
modern cognitive psychology, is of that character.

552
00:39:07,720 --> 00:39:17,600
But I don't see any way to introspect and well

553
00:39:17,600 --> 00:39:20,440
enough to guess how your brain does something,

554
00:39:20,440 --> 00:39:25,040
because we're just not that conscious.

555
00:39:25,040 --> 00:39:28,080
We don't have access to.

556
00:39:28,080 --> 00:39:30,360
You could think for 10 years about,

557
00:39:30,360 --> 00:39:33,520
how do I think of the next word to speak?

558
00:39:33,520 --> 00:39:37,760
And unlikely that you would.

559
00:39:37,760 --> 00:39:39,720
You might get some new ideas about how

560
00:39:39,720 --> 00:39:43,920
this might have happened, in fact.

561
00:39:43,920 --> 00:39:47,800
But you couldn't be sure.

562
00:39:47,800 --> 00:39:49,560
Well, I take it back.

563
00:39:49,560 --> 00:39:56,960
Probably people, you could probably

564
00:39:56,960 --> 00:40:02,280
get some correct theories by being lucky and clever.

565
00:40:02,280 --> 00:40:04,920
And then you'd have to find a neuroscientist

566
00:40:04,920 --> 00:40:10,600
to design an experiment to see if there's any evidence for that.

567
00:40:10,600 --> 00:40:14,520
In particular, I'd like to convince some neurologists

568
00:40:14,520 --> 00:40:18,880
to consider the idea of K lines.

569
00:40:18,880 --> 00:40:23,920
That's described, I think, in both of my books.

570
00:40:23,920 --> 00:40:27,960
And think of experiments to see if you

571
00:40:27,960 --> 00:40:33,280
could get them to light up or otherwise localize.

572
00:40:33,280 --> 00:40:39,200
And once you have in your mind the idea

573
00:40:39,200 --> 00:40:43,680
that maybe the way one brain connects,

574
00:40:43,680 --> 00:40:47,680
sends information to another is over something like K lines,

575
00:40:47,680 --> 00:40:52,680
which I think I talked about that the other day,

576
00:40:52,680 --> 00:40:57,160
random superimposed coding on parallel wires,

577
00:40:57,160 --> 00:40:59,680
then maybe you could think of experiments

578
00:40:59,680 --> 00:41:03,480
that even present brain scanning techniques

579
00:41:03,480 --> 00:41:08,800
could use to localize these.

580
00:41:08,800 --> 00:41:14,440
My main concern is that the way they do brain scanning now

581
00:41:14,440 --> 00:41:20,960
is to set thresholds to see which brain centers light up

582
00:41:20,960 --> 00:41:23,200
and which go turn off.

583
00:41:23,200 --> 00:41:24,840
And then they say, oh, I see.

584
00:41:24,840 --> 00:41:27,400
This activity looks like it happens

585
00:41:27,400 --> 00:41:32,960
in the lateral hippocampus, because you see that light up.

586
00:41:32,960 --> 00:41:36,600
I think that there should be at least

587
00:41:36,600 --> 00:41:40,920
a couple of neuroscientist groups

588
00:41:40,920 --> 00:41:45,240
who do the opposite, which is to reduce the contrast.

589
00:41:45,240 --> 00:41:48,360
And when there are several brain centers that

590
00:41:48,360 --> 00:41:51,480
seem to be involved in an activity,

591
00:41:51,480 --> 00:41:53,920
then say something to the patient

592
00:41:53,920 --> 00:41:58,960
and look for one area to get 2% dimmer and another

593
00:41:58,960 --> 00:42:03,080
to look 4% brighter and say, that

594
00:42:03,080 --> 00:42:06,520
might mean that there's a K line going from this one

595
00:42:06,520 --> 00:42:12,000
to that one with an inhibitory effect on this or that.

596
00:42:12,000 --> 00:42:15,080
But as far as I know right now, every paper

597
00:42:15,080 --> 00:42:19,840
I've ever seen published showing brain centers lighting up

598
00:42:19,840 --> 00:42:21,720
has high contrast.

599
00:42:21,720 --> 00:42:24,280
And so they're missing all the small things.

600
00:42:24,280 --> 00:42:29,480
And maybe they're only seeing the end result of the process

601
00:42:29,480 --> 00:42:32,000
where a little thinking has gone on

602
00:42:32,000 --> 00:42:37,800
with all these intricate, low-intensity interactions.

603
00:42:37,800 --> 00:42:41,760
And then the thing decides, oh, OK, I'm going to do this.

604
00:42:41,760 --> 00:42:45,440
And you conclude that that brain center which lit up

605
00:42:45,440 --> 00:42:48,280
is the one that decided to do this,

606
00:42:48,280 --> 00:42:54,840
whereas it's the result of a very small, fast avalanche.

607
00:42:54,840 --> 00:42:57,340
OK.

608
00:42:57,340 --> 00:42:59,600
Have you seen the one a couple weeks ago

609
00:42:59,600 --> 00:43:03,640
about reading out the visual field in real time?

610
00:43:03,640 --> 00:43:04,920
From the visual cortex?

611
00:43:04,920 --> 00:43:05,680
Yes.

612
00:43:05,680 --> 00:43:06,920
Quite a nice hack.

613
00:43:06,920 --> 00:43:09,120
They aren't actually reading out the visual field.

614
00:43:09,120 --> 00:43:12,720
They, for each subject, they do a massive amount of training

615
00:43:12,720 --> 00:43:17,520
where they flash thousands of one-second video clips

616
00:43:17,520 --> 00:43:21,960
and assemble a database of very small perturbations

617
00:43:21,960 --> 00:43:24,320
in different parts of the visual cortex lighting up.

618
00:43:24,320 --> 00:43:28,240
And then they show a novel video to each of the subjects

619
00:43:28,240 --> 00:43:34,160
and basically just do a linear combination of all

620
00:43:34,160 --> 00:43:36,920
of the videos that they've done in the training phase

621
00:43:36,920 --> 00:43:41,080
weighted by how closely things line up in the brain.

622
00:43:41,080 --> 00:43:44,520
And you can sort of see what's going on.

623
00:43:44,520 --> 00:43:46,600
It's quite striking.

624
00:43:46,600 --> 00:43:48,520
Can you tell what they're thinking?

625
00:43:48,520 --> 00:43:51,120
You can only tell what they're seeing.

626
00:43:51,120 --> 00:43:51,760
But I think.

627
00:43:51,760 --> 00:43:55,720
Yeah, if your eyes are closed, your primary visual cortex

628
00:43:55,720 --> 00:43:57,240
probably doesn't do anything, does it?

629
00:43:57,240 --> 00:44:00,320
I think it's just, yeah.

630
00:44:00,320 --> 00:44:05,560
But the secondary one might be representing things

631
00:44:05,560 --> 00:44:06,840
that might be there.

632
00:44:06,840 --> 00:44:07,600
Yes.

633
00:44:07,600 --> 00:44:10,400
So the goal of the authors of this paper

634
00:44:10,400 --> 00:44:17,000
is eventually to literally make movies out of dreams.

635
00:44:17,000 --> 00:44:19,920
But that's a long way off.

636
00:44:19,920 --> 00:44:21,960
It's an old idea in science fiction.

637
00:44:26,840 --> 00:44:28,560
How many of you read science fiction?

638
00:44:31,240 --> 00:44:33,320
Wow.

639
00:44:33,320 --> 00:44:34,360
That's a majority.

640
00:44:38,000 --> 00:44:39,480
Who's the best new writer?

641
00:44:44,080 --> 00:44:45,520
He's been writing a long time.

642
00:44:49,920 --> 00:44:50,420
Yeah.

643
00:45:00,080 --> 00:45:03,680
As I had dinner with Stevenson at the Hillis's

644
00:45:03,680 --> 00:45:04,920
a couple of years ago.

645
00:45:12,440 --> 00:45:14,280
Yeah.

646
00:45:14,280 --> 00:45:16,880
So from what I understood, it seems

647
00:45:16,880 --> 00:45:21,520
that you're saying that the difference between us

648
00:45:21,520 --> 00:45:28,880
and, for example, dogs is just a confrontation of power.

649
00:45:28,880 --> 00:45:33,120
So do you believe that the difference between dogs

650
00:45:33,120 --> 00:45:38,640
and computers is also just a confrontation?

651
00:45:38,640 --> 00:45:43,480
What is the difference between dogs and a Turing machine?

652
00:45:43,480 --> 00:45:55,960
Well, it might be that only humans and maybe some

653
00:45:55,960 --> 00:46:00,040
of their closest relatives can imagine a sequence.

654
00:46:04,000 --> 00:46:09,400
In other words, the simplest, oldest theories in psychology

655
00:46:09,400 --> 00:46:14,560
were the theories like David Hume

656
00:46:14,560 --> 00:46:16,960
had the idea of association.

657
00:46:16,960 --> 00:46:24,680
One idea in the mind or brain causes another idea

658
00:46:24,680 --> 00:46:26,200
to appear in another.

659
00:46:26,200 --> 00:46:32,720
So that means that a brain that's learned associations

660
00:46:32,720 --> 00:46:36,640
or learn if-then rule-based systems

661
00:46:36,640 --> 00:46:38,880
can make chains of things.

662
00:46:38,880 --> 00:46:45,160
But the question is, can any animal other than humans

663
00:46:45,160 --> 00:46:50,600
imagine two different situations and then compare them and say,

664
00:46:50,600 --> 00:46:54,660
if I did this and then that, how would the result

665
00:46:54,660 --> 00:46:57,480
differ from doing that and then this?

666
00:46:57,480 --> 00:47:01,720
If you look at Jerry Sussman's thesis,

667
00:47:01,720 --> 00:47:07,400
if you're at MIT, a good thing to do is to read the,

668
00:47:07,400 --> 00:47:09,360
when you're taking your course, you

669
00:47:09,360 --> 00:47:11,880
should read the PhD thesis of your professor.

670
00:47:17,040 --> 00:47:20,520
It not only will help you understand better

671
00:47:20,520 --> 00:47:25,680
what the professor said, you'll get a higher grade if you care

672
00:47:25,680 --> 00:47:29,620
and many other advantages, like you'll actually

673
00:47:29,620 --> 00:47:32,760
be able to talk to him and his mind won't throw up.

674
00:47:37,400 --> 00:47:47,440
So I don't know if a dog can recapitulate a,

675
00:47:47,440 --> 00:47:52,120
can the dog think, I think I'll go around this fence

676
00:47:52,120 --> 00:47:56,560
and when I get to this tree, I'll do this, I'll pee on it.

677
00:47:56,560 --> 00:47:58,200
That's what dogs do.

678
00:47:58,200 --> 00:48:03,120
And then I'll, whereas if I go this way,

679
00:48:03,120 --> 00:48:05,120
something else will happen.

680
00:48:05,120 --> 00:48:14,680
It might be that pre-primates can't do much of that.

681
00:48:14,680 --> 00:48:23,040
On the other hand, if you ask, what is the song of the whale?

682
00:48:23,040 --> 00:48:25,320
What's the whale that has this 20-minute song?

683
00:48:26,320 --> 00:48:36,640
It's the, I think my conjecture is that a whale has

684
00:48:36,640 --> 00:48:41,400
to swim 1,000 miles or several hundred miles sometimes

685
00:48:41,400 --> 00:48:46,440
to get the food it wants because things change.

686
00:48:46,440 --> 00:48:52,680
And each group of whales, humpback whales,

687
00:48:52,680 --> 00:48:57,160
I guess, sing this song that's about 20 minutes long.

688
00:48:57,160 --> 00:49:01,720
And nobody has made a good conjecture

689
00:49:01,720 --> 00:49:06,080
about what's the content of that song.

690
00:49:06,080 --> 00:49:08,320
But it's shared among the animals.

691
00:49:08,320 --> 00:49:12,800
And it goes, they can hear it 20 or 50 miles away

692
00:49:12,800 --> 00:49:14,200
and repeat it.

693
00:49:14,200 --> 00:49:16,400
And it changes every season.

694
00:49:16,400 --> 00:49:20,040
So I suspect that the obvious thing that it should be about

695
00:49:20,200 --> 00:49:22,920
is, where's the food these days?

696
00:49:22,920 --> 00:49:26,120
Where are the best flocks of fish to eat?

697
00:49:26,120 --> 00:49:33,320
Because a whale can't afford to swim 200 miles to the place

698
00:49:33,320 --> 00:49:39,720
where its favorite fish were last year and find it empty.

699
00:49:39,720 --> 00:49:46,440
It takes a lot of energy to cross the ocean.

700
00:49:46,440 --> 00:49:51,360
So maybe those animals have the ability

701
00:49:51,360 --> 00:49:55,800
to remember very long sequences and even some semantics

702
00:49:55,800 --> 00:49:57,440
connected with it.

703
00:49:57,440 --> 00:50:01,360
I don't know if dogs have anything like that.

704
00:50:01,360 --> 00:50:05,320
Do dogs ever seem to be talking to each other?

705
00:50:05,320 --> 00:50:07,200
Or are they just?

706
00:50:07,200 --> 00:50:09,640
So I have a story about dogs.

707
00:50:09,640 --> 00:50:13,640
So apparently, in Moscow, not all dogs,

708
00:50:13,640 --> 00:50:16,240
but a very small fraction of the stray dogs

709
00:50:16,720 --> 00:50:20,200
in the city have learned how to ride the metro.

710
00:50:20,200 --> 00:50:22,920
They live out in the suburbs because, I guess, people

711
00:50:22,920 --> 00:50:25,280
give them less trouble when they're out in the suburbs.

712
00:50:25,280 --> 00:50:27,400
And then they take the subway each day

713
00:50:27,400 --> 00:50:30,280
into the city center, where there are more people.

714
00:50:30,280 --> 00:50:33,800
And they have various strategies for begging in the city center.

715
00:50:33,800 --> 00:50:36,960
So for instance, they find some guy with a sandwich.

716
00:50:36,960 --> 00:50:39,040
And they bark really loudly behind the guy.

717
00:50:39,040 --> 00:50:40,480
And the guy would drop the sandwich.

718
00:50:40,480 --> 00:50:43,000
And then they'd steal it.

719
00:50:43,000 --> 00:50:45,920
Or they have a pack of them, and they all know each other.

720
00:50:45,920 --> 00:50:49,280
And they send out a really cute one to beg for food.

721
00:50:49,280 --> 00:50:51,000
And so they'll give the cute one food.

722
00:50:51,000 --> 00:50:53,640
And the cute one brings it back to everyone else.

723
00:50:53,640 --> 00:50:55,640
And simply navigating the subway

724
00:50:55,640 --> 00:50:59,280
is actually a bit complicated for a dog.

725
00:50:59,280 --> 00:51:02,280
But somehow, a very small group of dogs in Moscow

726
00:51:02,280 --> 00:51:05,000
have learned how to do it, like figure out where their stop is,

727
00:51:05,000 --> 00:51:08,240
get on, get off.

728
00:51:08,240 --> 00:51:14,280
Our dog once hopped on the green line and got off at Park Street.

729
00:51:14,320 --> 00:51:16,240
So she was missing for a while.

730
00:51:16,240 --> 00:51:20,240
And somebody at Park Street called up and said,

731
00:51:20,240 --> 00:51:22,280
your dog is here.

732
00:51:22,280 --> 00:51:25,200
So I went down and got her.

733
00:51:25,200 --> 00:51:31,680
And the agent said, you know, we had a dog that

734
00:51:31,680 --> 00:51:37,240
came to Park Street every day and changed trains

735
00:51:37,240 --> 00:51:41,840
and took the red line to somewhere.

736
00:51:41,880 --> 00:51:47,840
And finally, we found out that it used

737
00:51:47,840 --> 00:51:51,840
to go to work with its owner every day.

738
00:51:51,840 --> 00:51:53,720
And he died.

739
00:51:53,720 --> 00:51:58,520
And the dog took the same trip every day.

740
00:51:58,520 --> 00:52:06,120
The tea people understood that he shouldn't be bothered with it.

741
00:52:06,120 --> 00:52:08,440
Our dog chased cars.

742
00:52:08,440 --> 00:52:10,000
Was it Jenny?

743
00:52:10,000 --> 00:52:14,120
And it was terrible because we knew

744
00:52:14,120 --> 00:52:15,200
she was going to get hurt.

745
00:52:15,200 --> 00:52:20,400
And finally, a car squashed her leg.

746
00:52:20,400 --> 00:52:25,920
And she was laid up for a while with a somewhat broken leg.

747
00:52:25,920 --> 00:52:29,240
And I thought, well, she won't chase cars anymore.

748
00:52:29,240 --> 00:52:31,000
But she did.

749
00:52:31,000 --> 00:52:36,040
But what she wouldn't do is go to the intersection of Carlton

750
00:52:36,040 --> 00:52:40,880
and Ivy Street anymore, which is.

751
00:52:40,880 --> 00:52:46,560
So she had learned something, but it wasn't the right thing.

752
00:52:57,280 --> 00:52:58,760
I'm not sure I answered her.

753
00:52:58,760 --> 00:52:59,760
Go ahead.

754
00:53:05,760 --> 00:53:06,240
Thank you.

755
00:53:06,240 --> 00:53:08,000
Well, actually, according to Iso,

756
00:53:08,000 --> 00:53:10,800
there's this story that you gave in Chapter 2

757
00:53:10,800 --> 00:53:15,520
about the girl who's digging dirt.

758
00:53:15,520 --> 00:53:21,040
So in the case where she learns whether digging dirt

759
00:53:21,040 --> 00:53:23,600
is a good or bad activity is when

760
00:53:23,600 --> 00:53:27,000
there is somebody with whom she had an attachment

761
00:53:27,000 --> 00:53:28,920
bond with president who's telling her

762
00:53:28,920 --> 00:53:30,280
whether it's good or bad.

763
00:53:30,280 --> 00:53:32,840
And in the case where she learns to avoid that spot

764
00:53:32,840 --> 00:53:35,880
is when something bad happens to her in the spot.

765
00:53:35,880 --> 00:53:40,200
So in a sense, the dog is behaving just like that logic.

766
00:53:40,200 --> 00:53:46,480
Yes, except that the dog is oriented toward location

767
00:53:46,480 --> 00:53:48,560
rather than something else.

768
00:53:57,440 --> 00:54:04,040
It's about a possible hierarchy of higher order representation

769
00:54:04,040 --> 00:54:05,520
schemes of knowledge.

770
00:54:05,520 --> 00:54:11,600
There's like semantic nets on top and at the bottom.

771
00:54:11,600 --> 00:54:14,600
There's like neural network in the middle of KLIs

772
00:54:14,600 --> 00:54:17,320
and on the bottom there's something such a.

773
00:54:17,320 --> 00:54:19,200
So the way I thought about the questions

774
00:54:19,200 --> 00:54:22,680
that were just asked that humans, it's just natural to me

775
00:54:22,680 --> 00:54:27,080
that we need all the intermediate representations

776
00:54:27,080 --> 00:54:29,840
in order to support something like semantic nets.

777
00:54:29,840 --> 00:54:33,600
And it seems natural to me to think that humans of all these,

778
00:54:33,600 --> 00:54:35,880
the whole hierarchy of representations by dogs

779
00:54:35,880 --> 00:54:39,360
might have something only in the middle,

780
00:54:39,360 --> 00:54:44,640
like they might have something like neural nets or something.

781
00:54:44,640 --> 00:54:47,120
So my question is, what behaviors

782
00:54:47,120 --> 00:54:51,240
that you could observe in real life

783
00:54:51,240 --> 00:54:54,720
could only be done with one of these intermediate

784
00:54:54,720 --> 00:54:57,840
recommendations of knowledge that

785
00:54:57,840 --> 00:55:00,720
can't be done with something like machine learning?

786
00:55:00,720 --> 00:55:01,200
Hmm.

787
00:55:03,880 --> 00:55:06,360
You mean machine learning of what,

788
00:55:06,360 --> 00:55:08,840
of some particular kind?

789
00:55:08,840 --> 00:55:12,960
The kind that's currently fashionable, I think.

790
00:55:12,960 --> 00:55:16,040
Kind of like with like Qt data set before sort

791
00:55:17,040 --> 00:55:20,000
of calibration of some grammar.

792
00:55:25,960 --> 00:55:29,360
It seems to me that if you recognize a behavior like that,

793
00:55:29,360 --> 00:55:31,560
it might be a worthy intermediate goal

794
00:55:31,560 --> 00:55:33,840
to be able to model that instead of trying

795
00:55:33,840 --> 00:55:35,920
to model something like natural language, which

796
00:55:35,920 --> 00:55:40,880
is you might need the first part to get the second part.

797
00:55:40,880 --> 00:55:42,320
Well, it would be nice to know.

798
00:55:46,880 --> 00:55:55,400
I wonder how much is known about elephants, which are awfully

799
00:55:55,400 --> 00:56:03,640
smart compared to, I suspect that they are very good

800
00:56:03,640 --> 00:56:07,960
at making plans because it's so easy for an elephant

801
00:56:07,960 --> 00:56:11,400
to make a fatal mistake.

802
00:56:11,400 --> 00:56:17,320
So unfortunately, probably nobody, no research group

803
00:56:17,320 --> 00:56:23,320
has enough budget to study that kind of animal

804
00:56:23,320 --> 00:56:27,520
because it's just too expensive.

805
00:56:27,520 --> 00:56:28,960
How smart are elephants?

806
00:56:28,960 --> 00:56:29,440
Anybody?

807
00:56:33,760 --> 00:56:36,000
I've never interacted with one.

808
00:56:37,000 --> 00:56:41,000
I'm not sure if you have a question.

809
00:56:41,000 --> 00:56:44,000
The question is, are there behaviors

810
00:56:44,000 --> 00:56:49,120
that you need an intermediate level of the repetition

811
00:56:49,120 --> 00:56:53,520
of knowledge in order to perform that you don't need

812
00:56:53,520 --> 00:56:55,560
like the highest level, like the semantic,

813
00:56:55,560 --> 00:56:58,040
like basically natural language in order to do?

814
00:56:58,040 --> 00:57:00,760
So you could say that if I saw an animal doing this behavior,

815
00:57:00,760 --> 00:57:02,800
I know that it has some intermediate level

816
00:57:02,800 --> 00:57:03,800
of representation of knowledge.

817
00:57:03,800 --> 00:57:06,800
That's more than kind of a brute force machine learning

818
00:57:06,800 --> 00:57:07,800
approach.

819
00:57:07,800 --> 00:57:09,800
Because what's discussed before, a computer

820
00:57:09,800 --> 00:57:12,800
can do pathfinding with just like a brute force approach.

821
00:57:12,800 --> 00:57:14,800
But I don't think that's how humans do it or animals do it.

822
00:57:18,800 --> 00:57:20,800
I can't think of a good example.

823
00:57:23,800 --> 00:57:27,800
It's just hard to think of any animals besides us that

824
00:57:28,360 --> 00:57:36,080
it's just hard to think of any animals besides us that have

825
00:57:36,080 --> 00:57:40,360
really elaborate semantic networks.

826
00:57:40,360 --> 00:57:46,840
There's Coco, who's a gorilla that apparently

827
00:57:46,840 --> 00:57:47,920
had hundreds of words.

828
00:57:52,800 --> 00:57:56,800
I think the question is to find something lower than words.

829
00:57:56,800 --> 00:57:58,800
Like maybe Betty the crow with her.

830
00:57:58,800 --> 00:57:59,800
With that stick?

831
00:57:59,800 --> 00:58:00,300
Yeah.

832
00:58:05,300 --> 00:58:06,800
How many of you have seen the crow movie?

833
00:58:10,800 --> 00:58:14,800
She bends the, she has a wire that she bends

834
00:58:14,800 --> 00:58:16,300
and pulls something out of a tube.

835
00:58:23,800 --> 00:58:25,800
I don't think machine learning can do that.

836
00:58:25,800 --> 00:58:28,800
So I don't think you need semantics.

837
00:58:28,800 --> 00:58:32,800
I have a parrot who lives in a three dimensional cage.

838
00:58:32,800 --> 00:58:37,800
And she knows how to get from any place to another.

839
00:58:37,800 --> 00:58:42,800
And if she's in a hurry, she'll find a new way

840
00:58:42,800 --> 00:58:45,800
at the risk of injuring a wing because there

841
00:58:45,800 --> 00:58:47,800
are a lot of sticks in the way.

842
00:58:47,800 --> 00:58:50,800
So flying is risky.

843
00:58:56,800 --> 00:59:00,800
Our daughter Julie once visited Coco, the gorilla.

844
00:59:00,800 --> 00:59:07,800
And she was introduced Coco's in a cage.

845
00:59:07,800 --> 00:59:18,800
And Penny, who is Coco's owner, introduces Julie

846
00:59:18,800 --> 00:59:19,800
in sign language.

847
00:59:19,800 --> 00:59:22,800
Coco, it's not words.

848
00:59:22,800 --> 00:59:24,800
It's not spoken.

849
00:59:24,800 --> 00:59:25,800
It's sign language.

850
00:59:28,800 --> 00:59:31,800
So Julie gets some name.

851
00:59:31,800 --> 00:59:35,800
And she's introduced to Coco.

852
00:59:35,800 --> 00:59:38,800
And Coco likes Julie.

853
00:59:38,800 --> 00:59:42,800
So Coco says, let me out.

854
00:59:42,800 --> 00:59:47,800
And Penny says, no, you can't get out.

855
00:59:47,800 --> 00:59:52,800
Coco says, then let Julie in.

856
00:59:52,800 --> 00:59:57,800
And I thought that showed some fairly abstract reasoning

857
00:59:57,800 --> 00:59:58,800
or representation.

858
01:00:01,800 --> 01:00:03,800
And Penny didn't let Julie in.

859
01:00:04,800 --> 01:00:23,800
But Coco seemed to have a fair amount of declarative syntax.

860
01:00:23,800 --> 01:00:27,800
I don't know if she could do passives or anything like that.

861
01:00:27,800 --> 01:00:29,800
If you're interested, you probably

862
01:00:29,800 --> 01:00:31,800
can look it up on the web.

863
01:00:33,800 --> 01:00:40,800
Penny thought that Coco knew 600 or 700 words.

864
01:00:40,800 --> 01:00:44,800
And a friend of ours was a teenager who worked for her.

865
01:00:44,800 --> 01:00:49,800
And what's his name?

866
01:00:49,800 --> 01:00:56,800
And he was convinced that Coco knew more than 1,000 words.

867
01:00:56,800 --> 01:00:58,800
But he said, you see, I'm a teenager.

868
01:00:58,800 --> 01:01:04,800
And I'm still good at picking up gestures and clues better

869
01:01:04,800 --> 01:01:05,800
than the adults here.

870
01:01:09,800 --> 01:01:11,800
Anyway, I gather Coco is still there.

871
01:01:11,800 --> 01:01:16,800
And I don't know if she's still learning more words.

872
01:01:16,800 --> 01:01:19,800
But every now and then, we get a letter

873
01:01:19,800 --> 01:01:20,800
asking to send more money.

874
01:01:21,800 --> 01:01:29,800
Oh, in the last lecture, I couldn't

875
01:01:29,800 --> 01:01:36,800
think of the right crypt arithmetic example.

876
01:01:50,800 --> 01:01:59,800
I think that's the one that the new old Simon book starts out

877
01:01:59,800 --> 01:02:01,800
with.

878
01:02:01,800 --> 01:02:03,800
So obviously, m is 1.

879
01:02:03,800 --> 01:02:10,800
And then I bet some of you could figure that out

880
01:02:10,800 --> 01:02:11,800
in four or five minutes.

881
01:02:14,800 --> 01:02:16,800
Has anybody figured it out yet?

882
01:02:21,800 --> 01:02:37,800
Help send more questions.

883
01:02:40,800 --> 01:02:41,800
Yeah.

884
01:02:41,800 --> 01:02:44,800
I have an example of where, since I go out

885
01:02:44,800 --> 01:02:48,800
to a restaurant of this type of exotic food

886
01:02:48,800 --> 01:02:50,800
that I've never, ever had before,

887
01:02:50,800 --> 01:02:52,800
and I end up getting sick from it.

888
01:02:52,800 --> 01:02:56,800
So what determines what I learn from this?

889
01:02:56,800 --> 01:02:59,800
Because there are many different possibilities here.

890
01:02:59,800 --> 01:03:01,800
There's the one possibility of I learn

891
01:03:01,800 --> 01:03:03,800
to avoid the specific food I ate.

892
01:03:03,800 --> 01:03:08,800
Another possibility is I learn to avoid that type of food,

893
01:03:08,800 --> 01:03:10,800
because it might contain some sort of spice

894
01:03:10,800 --> 01:03:11,800
that I react to badly.

895
01:03:11,800 --> 01:03:13,800
And the third possibility, there might be more,

896
01:03:13,800 --> 01:03:15,800
is I learn to avoid that restaurant,

897
01:03:16,800 --> 01:03:18,800
because that just might be a bad restaurant.

898
01:03:18,800 --> 01:03:21,800
So in this case, it's not entirely clear

899
01:03:21,800 --> 01:03:24,800
which one I ought to pick.

900
01:03:24,800 --> 01:03:26,800
And of course, maybe in real life,

901
01:03:26,800 --> 01:03:29,800
I might go there again and empirically try another food

902
01:03:29,800 --> 01:03:31,800
or try the same food at different restaurants.

903
01:03:31,800 --> 01:03:34,800
But what do you think about this in that scenario?

904
01:03:34,800 --> 01:03:37,800
What causes people to pick which one?

905
01:03:37,800 --> 01:03:41,800
The trouble is we keep thinking of ourselves as people.

906
01:03:41,800 --> 01:03:48,800
And you should think of yourself as a sort of petri dish

907
01:03:48,800 --> 01:03:52,800
with a trillion bacteria in it.

908
01:03:52,800 --> 01:03:56,800
And it's really not important to you what you eat,

909
01:03:56,800 --> 01:04:00,800
but your intestinal bacteria are the ones who are really

910
01:04:00,800 --> 01:04:05,800
going to suffer, because they're not used to anything new.

911
01:04:05,800 --> 01:04:10,800
So I don't know what conclusion to draw from that.

912
01:04:11,800 --> 01:04:15,800
But...

913
01:04:15,800 --> 01:04:18,800
Seriously, you mentioned David Hume,

914
01:04:18,800 --> 01:04:22,800
not that not represented as associations.

915
01:04:22,800 --> 01:04:26,800
And that occurs to me as being sort of like a wiki structure

916
01:04:26,800 --> 01:04:28,800
where entries have tags.

917
01:04:28,800 --> 01:04:32,800
So an entry might be defined by what tags it has

918
01:04:32,800 --> 01:04:34,800
and what associations it has.

919
01:04:34,800 --> 01:04:37,800
I'm wondering if that structure has been,

920
01:04:37,800 --> 01:04:39,800
if somebody has attempted to code that

921
01:04:39,800 --> 01:04:41,800
into some kind of virtual structure,

922
01:04:41,800 --> 01:04:47,800
has there been any success with that idea into potential AI?

923
01:04:54,800 --> 01:04:57,800
I don't know how to answer that.

924
01:05:08,800 --> 01:05:11,800
Do any psychologists use semantic networks

925
01:05:11,800 --> 01:05:13,800
as representations?

926
01:05:13,800 --> 01:05:19,800
Pat, do you know, has anybody, I know,

927
01:05:19,800 --> 01:05:25,800
is anyone building an AI system with semantic representations

928
01:05:25,800 --> 01:05:28,800
or semantic networks anymore?

929
01:05:28,800 --> 01:05:30,800
Or is it all, everything I've seen

930
01:05:30,800 --> 01:05:35,800
has gone probabilistic in the last few years?

931
01:05:36,800 --> 01:05:40,800
Your project, do you have any competitors?

932
01:05:40,800 --> 01:05:41,800
No.

933
01:05:41,800 --> 01:05:49,800
Any idea what the IBM people are using?

934
01:05:49,800 --> 01:05:52,800
I saw a long article that I didn't read yet.

935
01:05:52,800 --> 01:05:55,800
My impression is traditional information retrieval

936
01:05:55,800 --> 01:06:00,800
plus 100 tags plus machine learning.

937
01:06:00,800 --> 01:06:03,800
They seem to have a whole lot of different,

938
01:06:03,800 --> 01:06:05,800
slightly different representations

939
01:06:05,800 --> 01:06:06,800
that they switch among.

940
01:06:09,800 --> 01:06:13,800
But none of them are very semantic.

941
01:06:13,800 --> 01:06:15,800
Well, they probably have, I don't know,

942
01:06:15,800 --> 01:06:17,800
does anybody know what the answer is?

943
01:06:17,800 --> 01:06:20,800
But they must have little frame-like things

944
01:06:20,800 --> 01:06:22,800
for the standard questions.

945
01:06:22,800 --> 01:06:26,800
Of course, the thing doesn't answer any,

946
01:06:26,800 --> 01:06:29,800
it doesn't do any reasoning, as far as you can tell.

947
01:06:29,800 --> 01:06:30,800
Right.

948
01:06:30,800 --> 01:06:35,800
So it's trying to match sentences in the database

949
01:06:35,800 --> 01:06:36,800
with the question.

950
01:06:45,800 --> 01:06:49,800
Well, what's your theory of why there aren't other groups

951
01:06:49,800 --> 01:06:54,800
working on what we used to and you are?

952
01:06:54,800 --> 01:06:58,800
Well, when those are computing is a fact.

953
01:06:58,800 --> 01:07:04,800
And if you can do better in less time that way than figuring

954
01:07:04,800 --> 01:07:08,800
out how it really works, then that's what you do.

955
01:07:08,800 --> 01:07:13,800
No one does research on chess.

956
01:07:13,800 --> 01:07:17,800
No one does any research on how humans might play chess

957
01:07:17,800 --> 01:07:20,800
because the build-over computers have won.

958
01:07:20,800 --> 01:07:21,300
Right.

959
01:07:23,800 --> 01:07:26,800
There were some articles on chess and checkers

960
01:07:26,800 --> 01:07:27,800
early in the game.

961
01:07:27,800 --> 01:07:31,800
But nothing recent, as far as I know.

962
01:07:35,800 --> 01:07:40,800
So it's a local, in many ways, it's a local maximum phenomenon.

963
01:07:40,800 --> 01:07:43,800
The build-over computing stuff has got up to a certain local

964
01:07:43,800 --> 01:07:44,800
maximum.

965
01:07:44,800 --> 01:07:47,800
And so you can do better than that some other way

966
01:07:47,800 --> 01:07:50,800
but in too fast a time.

967
01:07:50,800 --> 01:07:54,800
Well, I wonder if we could invent a new TV show where

968
01:07:54,800 --> 01:07:56,800
the questions are interesting.

969
01:07:57,800 --> 01:08:07,800
Like I'm obsessed with the question of why you can pull

970
01:08:07,800 --> 01:08:11,800
something with a string but you can't push it.

971
01:08:11,800 --> 01:08:15,800
And in fact, what was this?

972
01:08:15,800 --> 01:08:17,800
We had a student who actually did something

973
01:08:17,800 --> 01:08:19,800
with that a long time ago.

974
01:08:19,800 --> 01:08:23,800
I've lost track of him.

975
01:08:23,800 --> 01:08:28,800
But how could you make a TV show that had common sense

976
01:08:28,800 --> 01:08:33,800
questions rather than ones about sports and actors?

977
01:08:39,800 --> 01:08:42,800
Well, if you imagine what happens when you push with a string,

978
01:08:42,800 --> 01:08:44,800
it's hard to explain that in words.

979
01:08:44,800 --> 01:08:45,300
Hypocles.

980
01:08:45,300 --> 01:08:46,800
It's easy to imagine.

981
01:08:46,800 --> 01:08:47,800
Yeah.

982
01:08:47,800 --> 01:08:48,800
So you can simulate it.

983
01:08:48,800 --> 01:08:49,300
Yeah.

984
01:08:49,300 --> 01:08:51,300
So I have a question.

985
01:08:51,300 --> 01:08:53,300
So suppose in the future we can create our own,

986
01:08:53,300 --> 01:08:55,300
which is as intelligent as human, as smart.

987
01:08:55,300 --> 01:08:57,300
And how we should evaluate it?

988
01:08:57,300 --> 01:09:00,300
When do we know that we reach, like, certainly, like,

989
01:09:00,300 --> 01:09:04,300
which test should pass or which should, I don't know.

990
01:09:04,300 --> 01:09:06,300
So for example, Watson, right?

991
01:09:06,300 --> 01:09:09,300
It can answer pretty hard questions and seem

992
01:09:09,300 --> 01:09:10,300
to be intelligent.

993
01:09:10,300 --> 01:09:14,300
But what all it is about is intelligence.

994
01:09:14,300 --> 01:09:18,300
Pretty hard questions and seem to be intelligent.

995
01:09:18,300 --> 01:09:21,300
But what all it is doing is doing some algorithms

996
01:09:21,300 --> 01:09:23,300
and then calculating some probabilities and stuff.

997
01:09:23,300 --> 01:09:24,300
Humans don't do that.

998
01:09:24,300 --> 01:09:26,300
They try to understand the questions,

999
01:09:26,300 --> 01:09:28,300
and they assume for the answer, right?

1000
01:09:28,300 --> 01:09:32,300
But then suppose you can create a robot that can behave as it is.

1001
01:09:32,300 --> 01:09:33,300
Like, I don't know.

1002
01:09:33,300 --> 01:09:34,300
How would you evaluate?

1003
01:09:34,300 --> 01:09:36,300
Like, when do you know that you reach something?

1004
01:09:36,300 --> 01:09:53,300
That's sort of funny because if it's any good,

1005
01:09:53,300 --> 01:09:55,300
you wouldn't have that question.

1006
01:09:55,300 --> 01:09:59,300
You'd say, well, what can't it do?

1007
01:09:59,300 --> 01:10:00,300
And why not?

1008
01:10:00,300 --> 01:10:02,300
And you'd argue with it.

1009
01:10:06,300 --> 01:10:11,300
In other words, people talk about passing the Turing test

1010
01:10:11,300 --> 01:10:12,300
or whatever.

1011
01:10:12,300 --> 01:10:23,300
And it's hard to imagine a machine that you converse with

1012
01:10:23,300 --> 01:10:24,300
for a while.

1013
01:10:24,300 --> 01:10:31,300
And then when you're told it's a machine, you're surprised.

1014
01:10:36,300 --> 01:10:46,300
I think that, for example, we can make a machine

1015
01:10:46,300 --> 01:10:48,300
to say some very intelligent and smart things

1016
01:10:48,300 --> 01:10:51,300
because it may know all this text information

1017
01:10:51,300 --> 01:10:53,300
from different books and all this information

1018
01:10:53,300 --> 01:10:55,300
it has somewhere in the database, right?

1019
01:10:55,300 --> 01:10:57,300
But then when people speak,

1020
01:10:57,300 --> 01:10:59,300
they kind of understand what they're speaking.

1021
01:10:59,300 --> 01:11:02,300
How do you know if someone doesn't understand something,

1022
01:11:02,300 --> 01:11:05,300
doesn't understand, or does it have to understand at all?

1023
01:11:05,300 --> 01:11:09,300
Well, I would ask you questions like, why can't you

1024
01:11:09,300 --> 01:11:11,300
push something with a string?

1025
01:11:17,300 --> 01:11:21,300
Well, anyone have a Google working?

1026
01:11:21,300 --> 01:11:25,300
What does Google say if you ask it that?

1027
01:11:25,300 --> 01:11:31,300
Maybe it'll quote me or someone who's, yeah?

1028
01:11:31,300 --> 01:11:31,800
Yeah?

1029
01:11:31,800 --> 01:11:34,300
How would you answer that question?

1030
01:11:34,300 --> 01:11:37,300
Why can't you pull but not push?

1031
01:11:45,300 --> 01:11:47,300
I'd say, well, it would buckle.

1032
01:11:47,300 --> 01:11:50,300
And then they would say, what do you mean by buckle?

1033
01:11:50,300 --> 01:11:55,300
And then I'd say, oh, it would fold up so that it got shorter

1034
01:11:55,300 --> 01:11:59,300
without exerting any force at the end or blah, blah.

1035
01:11:59,300 --> 01:12:01,300
I don't know.

1036
01:12:01,300 --> 01:12:03,300
There are lots of answers.

1037
01:12:03,300 --> 01:12:04,300
How would you answer it?

1038
01:12:10,300 --> 01:12:13,300
A physicist might say, if you got it really very, very,

1039
01:12:13,300 --> 01:12:16,300
very straight, you could push it with a string.

1040
01:12:22,300 --> 01:12:24,300
But quantum mechanics would say you can't.

1041
01:12:24,300 --> 01:12:26,300
Yeah?

1042
01:12:27,300 --> 01:12:32,300
I feel like if you like the typical or an interesting show

1043
01:12:32,300 --> 01:12:36,300
would be have an alternate cookie show or something,

1044
01:12:36,300 --> 01:12:41,300
where you have to use objects that's not normally found

1045
01:12:41,300 --> 01:12:42,300
to have that used.

1046
01:12:42,300 --> 01:12:45,300
So you want to paint a room, but you're not given a brush.

1047
01:12:45,300 --> 01:12:53,300
You're given, say, a sponge, a piece of cloth, and eggplants.

1048
01:12:53,300 --> 01:12:55,300
You want to paint it purple.

1049
01:12:55,300 --> 01:13:00,300
So it has to represent the thing in a different way

1050
01:13:00,300 --> 01:13:03,300
other than words.

1051
01:13:03,300 --> 01:13:05,300
Words.

1052
01:13:05,300 --> 01:13:08,300
That's interesting.

1053
01:13:08,300 --> 01:13:12,300
When I was in graduate school, I took a course in knot theory.

1054
01:13:12,300 --> 01:13:15,300
And in fact, you couldn't talk about them.

1055
01:13:15,300 --> 01:13:17,300
If anybody had a question, they'd

1056
01:13:17,300 --> 01:13:23,300
have to run up to the board and they'd

1057
01:13:23,300 --> 01:13:25,300
have to do something like this.

1058
01:13:32,300 --> 01:13:33,300
Is that a knot?

1059
01:13:36,300 --> 01:13:39,300
No.

1060
01:13:39,300 --> 01:13:40,300
No, that's just a loop.

1061
01:13:45,300 --> 01:13:50,300
But if you were restricted to words,

1062
01:13:50,300 --> 01:13:51,300
it would take a half hour.

1063
01:13:55,300 --> 01:13:56,300
That's interesting.

1064
01:14:00,300 --> 01:14:03,300
Yeah?

1065
01:14:03,300 --> 01:14:05,300
You mentioned solving the string puzzle

1066
01:14:05,300 --> 01:14:08,300
by imagining the result. And I think I heard someone else say,

1067
01:14:08,300 --> 01:14:09,300
computers can do that in some way.

1068
01:14:09,300 --> 01:14:10,300
You can simulate a string.

1069
01:14:10,300 --> 01:14:12,300
And we know in the physics that you

1070
01:14:12,300 --> 01:14:15,300
could give a reasonable approximation of the string.

1071
01:14:15,300 --> 01:14:21,300
But I find that the question that is often not asked in AI

1072
01:14:21,300 --> 01:14:25,300
by computers is, how does one choose the correct model

1073
01:14:25,300 --> 01:14:26,300
in which to answer questions?

1074
01:14:26,300 --> 01:14:28,300
There's a lot of questions we're really

1075
01:14:28,300 --> 01:14:29,300
good at answering with computers.

1076
01:14:29,300 --> 01:14:32,300
And some of them genetic algorithms are good for.

1077
01:14:32,300 --> 01:14:33,300
Some of them basic statistics.

1078
01:14:33,300 --> 01:14:34,300
Some of them formal logic.

1079
01:14:34,300 --> 01:14:37,300
Some of them physics simulation.

1080
01:14:37,300 --> 01:14:40,300
But to me, this is the core question,

1081
01:14:40,300 --> 01:14:42,300
because this is what people decide and no one

1082
01:14:42,300 --> 01:14:44,300
seems to have ever tackled in AI.

1083
01:14:44,300 --> 01:14:48,300
Well, for instance, you have to, if somebody asks the question,

1084
01:14:48,300 --> 01:14:53,300
you have to make up a biography of that person

1085
01:14:53,300 --> 01:14:56,300
because the same question from different people

1086
01:14:56,300 --> 01:14:58,300
would get really different answers.

1087
01:15:04,300 --> 01:15:07,300
Why does a kettle make a noise when the water boils?

1088
01:15:07,300 --> 01:15:08,300
The water boils.

1089
01:15:11,300 --> 01:15:15,300
If you know that the other person is a physicist,

1090
01:15:15,300 --> 01:15:17,300
and it's easy to think of things to say,

1091
01:15:17,300 --> 01:15:24,300
but it's not a very good example.

1092
01:15:27,300 --> 01:15:29,300
What's the context of that?

1093
01:15:29,300 --> 01:15:36,300
How do you, in a human conversation,

1094
01:15:36,300 --> 01:15:39,300
how does each person know what to say next?

1095
01:15:39,300 --> 01:15:43,300
I guess one question is, how do people

1096
01:15:43,300 --> 01:15:46,300
decide what AI methods to use to tackle a problem?

1097
01:15:46,300 --> 01:15:48,300
And I guess the more fundamental question

1098
01:15:48,300 --> 01:15:50,300
is, when people are solving problems,

1099
01:15:50,300 --> 01:15:53,300
how do they decide how they're going to think about the problem?

1100
01:15:53,300 --> 01:15:56,300
Are they going to think about it by visualizing it,

1101
01:15:56,300 --> 01:15:58,300
think about it by trying a different thing,

1102
01:15:58,300 --> 01:16:01,300
think about it by analogy or morphology?

1103
01:16:01,300 --> 01:16:05,300
Of all the tools we have, why do we get the ones we do?

1104
01:16:06,300 --> 01:16:15,300
Yeah, well, that goes back to the if you make a list of the 50

1105
01:16:15,300 --> 01:16:22,300
most common ways to think, and somebody asks you a question

1106
01:16:22,300 --> 01:16:27,300
or ask why does such and such happen,

1107
01:16:27,300 --> 01:16:30,300
how do you decide which of your ways to think about it?

1108
01:16:31,300 --> 01:16:38,300
And I suspect that that's another knowledge base.

1109
01:16:38,300 --> 01:16:42,300
So we have common sense knowledge about,

1110
01:16:42,300 --> 01:16:46,300
if you let go of an object, it will fall.

1111
01:16:46,300 --> 01:16:55,300
And then we have more general knowledge

1112
01:16:55,300 --> 01:16:58,300
about what happens when an object falls.

1113
01:16:58,300 --> 01:17:00,300
Why didn't it break?

1114
01:17:00,300 --> 01:17:04,300
Well, it actually did, because here's

1115
01:17:04,300 --> 01:17:10,300
a little white thing which turned into dust.

1116
01:17:10,300 --> 01:17:13,300
And so that's why I think you need

1117
01:17:13,300 --> 01:17:18,300
to have five or six or how many different levels

1118
01:17:18,300 --> 01:17:20,300
of representation.

1119
01:17:20,300 --> 01:17:24,300
So as soon as somebody asks a question,

1120
01:17:24,300 --> 01:17:31,300
one part of your brain is coming up with your first idea.

1121
01:17:31,300 --> 01:17:33,300
Another part of your brain is saying,

1122
01:17:33,300 --> 01:17:36,300
is this a question about physics or philosophy

1123
01:17:36,300 --> 01:17:41,300
or is it a social question?

1124
01:17:41,300 --> 01:17:44,300
Did this person ask it because they actually want to know

1125
01:17:44,300 --> 01:17:46,300
or they want to trap me?

1126
01:17:46,300 --> 01:17:59,460
Or so I think generally this idea of there

1127
01:17:59,460 --> 01:18:03,700
must be many kinds of society of mind models that people have.

1128
01:18:03,700 --> 01:18:11,540
And each person, whenever you're talking to somebody,

1129
01:18:11,540 --> 01:18:15,380
you choose some model of what is this conversation about?

1130
01:18:15,380 --> 01:18:20,780
Am I trying to accomplish something by this discussion?

1131
01:18:20,780 --> 01:18:24,540
Is it really an interesting question?

1132
01:18:24,540 --> 01:18:27,740
Do I not want to offend with the person

1133
01:18:27,740 --> 01:18:31,940
or do I want to make him go away forever?

1134
01:18:31,940 --> 01:18:35,020
And little parts of your brain are making all these decisions

1135
01:18:35,020 --> 01:18:37,100
for you.

1136
01:18:37,100 --> 01:18:41,660
I'd like to introduce Bob Lawler, who's visiting.

1137
01:18:41,660 --> 01:18:47,100
One of my favorite stories about Feynman

1138
01:18:47,100 --> 01:18:51,780
comes from asking him at dinner one night.

1139
01:18:51,780 --> 01:18:56,260
I asked him how he got to be so smart.

1140
01:18:56,260 --> 01:19:01,460
And he said that when he was an undergraduate here,

1141
01:19:01,460 --> 01:19:05,220
he would consider every time he was able to solve a problem

1142
01:19:05,220 --> 01:19:09,100
just the beginning step of how to exploit that.

1143
01:19:09,100 --> 01:19:10,620
And what he would then do would be

1144
01:19:10,620 --> 01:19:15,620
to try to reformulate the problem in as many

1145
01:19:15,620 --> 01:19:18,460
different representations as he could

1146
01:19:18,460 --> 01:19:21,820
and then use his solution of the first problem

1147
01:19:21,820 --> 01:19:25,900
as a guide in working out alternate representations

1148
01:19:25,900 --> 01:19:28,380
and procedures in that.

1149
01:19:28,380 --> 01:19:30,220
The consequence, according to him,

1150
01:19:30,220 --> 01:19:32,980
was that he became very good at knowing

1151
01:19:32,980 --> 01:19:37,100
which was the most fit representation to use

1152
01:19:37,100 --> 01:19:40,860
in solving any particular problem that he encountered.

1153
01:19:40,860 --> 01:19:45,060
And he said that that's where his legendary capability

1154
01:19:45,060 --> 01:19:48,940
of being so quick with good solutions and good methods

1155
01:19:48,940 --> 01:19:50,820
the solutions came from.

1156
01:19:50,820 --> 01:19:55,620
So maybe a criteria for an intelligent machine

1157
01:19:55,620 --> 01:19:59,940
would be one that had a number of 15 different ways

1158
01:19:59,940 --> 01:20:04,060
of thinking and applied them regularly

1159
01:20:04,060 --> 01:20:08,140
to develop alternative information

1160
01:20:08,140 --> 01:20:11,740
about different methods of problem solving.

1161
01:20:11,740 --> 01:20:16,060
He would expect that to have some facility at choosing

1162
01:20:16,060 --> 01:20:17,900
based on its experience.

1163
01:20:17,900 --> 01:20:25,760
Yeah, he wrote something about because the other physicists

1164
01:20:25,760 --> 01:20:29,180
would argue about whether to use Heisenberg matrices

1165
01:20:29,180 --> 01:20:32,260
or Schrodinger's equation.

1166
01:20:32,300 --> 01:20:35,220
And he thought he was the only one who

1167
01:20:35,220 --> 01:20:38,340
knew how to solve each problem both ways

1168
01:20:38,340 --> 01:20:42,260
because most of the other physicists

1169
01:20:42,260 --> 01:20:44,100
would get very good at one or the other.

1170
01:20:47,860 --> 01:20:56,980
He had another feature, which was that if you argued with him,

1171
01:20:56,980 --> 01:20:59,060
sometimes he would say, oh, you're right.

1172
01:20:59,060 --> 01:20:59,900
I was wrong.

1173
01:21:03,140 --> 01:21:08,700
Like he was once arguing with Fredkin about,

1174
01:21:08,700 --> 01:21:11,460
could you have clocks all over the universe

1175
01:21:11,460 --> 01:21:13,500
that were synchronized?

1176
01:21:13,500 --> 01:21:21,780
And the standard idea is you couldn't because of relativity.

1177
01:21:21,780 --> 01:21:25,540
And Fredkin said, well, suppose you start out on Earth

1178
01:21:25,540 --> 01:21:33,020
and you send a huge army of little bacteria-sized clocks

1179
01:21:33,020 --> 01:21:37,660
and send them through all possible routes to every place

1180
01:21:37,660 --> 01:21:46,840
and figure out and compensate for all the accelerations

1181
01:21:46,840 --> 01:21:49,900
they had experienced on the path,

1182
01:21:49,900 --> 01:21:53,660
then wouldn't you get a synchronous time everywhere?

1183
01:21:53,660 --> 01:21:56,140
And Feynman said, you're right.

1184
01:21:56,140 --> 01:21:59,380
I was wrong without blinking.

1185
01:22:05,820 --> 01:22:06,820
He may have been wrong.

1186
01:22:23,660 --> 01:22:24,660
More questions?

1187
01:22:38,140 --> 01:22:40,140
And then I want to stay in line with this question

1188
01:22:40,140 --> 01:22:44,140
about how do we know what method to use for solving problems.

1189
01:22:44,140 --> 01:22:47,140
I'm kind of curious how we know what data set

1190
01:22:47,140 --> 01:22:49,620
and what data to use when solving problems.

1191
01:22:49,620 --> 01:22:52,620
Because we have so much sensory input in the moment

1192
01:22:53,580 --> 01:22:55,580
and so much data that we have from experience.

1193
01:22:55,580 --> 01:22:57,580
But when you get a problem, you instantly

1194
01:22:57,580 --> 01:23:00,580
I guess can be a sort of a solution for that.

1195
01:23:00,580 --> 01:23:03,580
But if it turns to how you could possibly represent

1196
01:23:03,580 --> 01:23:07,580
good data relationships in a way that a computer might be able

1197
01:23:07,580 --> 01:23:09,580
to do, because right now the problem is that we always

1198
01:23:09,580 --> 01:23:13,580
have to very narrowly define a problem for a machine

1199
01:23:13,580 --> 01:23:14,580
to be able to solve it.

1200
01:23:14,580 --> 01:23:16,580
But I feel like we could come up with good methods

1201
01:23:16,580 --> 01:23:21,580
for filtering massive data sets to just what might be relevant.

1202
01:23:22,020 --> 01:23:25,020
It doesn't involve trial and error.

1203
01:23:25,020 --> 01:23:25,520
Yes.

1204
01:23:29,740 --> 01:23:38,700
So the thing must be that if you have a problem,

1205
01:23:38,700 --> 01:23:40,220
how do you characterize it?

1206
01:23:42,820 --> 01:23:46,380
How do you think, what kind of problem is this?

1207
01:23:46,380 --> 01:23:49,460
And what method is good for that kind of problem?

1208
01:23:49,460 --> 01:24:03,500
So I suppose that people vary a lot, and it's a great question.

1209
01:24:06,500 --> 01:24:07,760
That's what the critics do.

1210
01:24:07,760 --> 01:24:10,420
They say, what kind of problem is this?

1211
01:24:10,420 --> 01:24:14,540
How do I recognize this particular predicament?

1212
01:24:14,540 --> 01:24:22,380
And I wish there were some psychologists

1213
01:24:22,380 --> 01:24:27,580
who thought about that the way Newell and Simon did.

1214
01:24:27,580 --> 01:24:32,060
God, in the 1960s, it's 50 years ago.

1215
01:24:32,060 --> 01:24:34,660
How many of you have seen that book called

1216
01:24:34,660 --> 01:24:37,860
Human Problem Solving?

1217
01:24:37,860 --> 01:24:43,020
It's a big, thick book, and it's got all sorts of chapters.

1218
01:24:43,020 --> 01:24:46,820
That's the one I mentioned the other day, where they actually

1219
01:24:46,820 --> 01:24:49,660
had some theories of human problem solving

1220
01:24:49,660 --> 01:24:54,420
and simulated this.

1221
01:25:00,020 --> 01:25:06,620
They gave subjects problems like this and said,

1222
01:25:06,620 --> 01:25:09,740
we want you to figure out what numbers those are.

1223
01:25:09,740 --> 01:25:12,480
And they lied to the subjects and said,

1224
01:25:12,480 --> 01:25:15,920
this is an important kind of problem in cryptography.

1225
01:25:15,920 --> 01:25:19,080
The secret agents need to know how

1226
01:25:19,080 --> 01:25:25,160
to decode cryptograms of this sort, where usually it's

1227
01:25:25,160 --> 01:25:26,080
the other way around.

1228
01:25:26,080 --> 01:25:28,480
The numbers stand for letters, and there's

1229
01:25:28,480 --> 01:25:29,760
some complicated coding.

1230
01:25:29,760 --> 01:25:32,880
But these are simple cases, so you have

1231
01:25:32,880 --> 01:25:36,320
to figure out that sort of thing.

1232
01:25:36,320 --> 01:25:39,220
And then the book has various chapters

1233
01:25:39,220 --> 01:25:42,980
on theories of how you recognize different kinds of problems

1234
01:25:42,980 --> 01:25:46,900
and select strategies.

1235
01:25:46,900 --> 01:25:49,940
And of course, some people are better than others.

1236
01:25:49,940 --> 01:25:54,140
And believe it or not, at MIT, there

1237
01:25:54,140 --> 01:25:57,740
was almost a whole decade of psychologists

1238
01:25:57,740 --> 01:26:02,740
here who were studying the psychology of five-person

1239
01:26:02,740 --> 01:26:05,180
groups.

1240
01:26:05,180 --> 01:26:12,220
It became, suppose you take five people and put them in a room

1241
01:26:12,220 --> 01:26:14,900
and give them problems like this.

1242
01:26:14,900 --> 01:26:19,700
Or not the same cryptography, but little puzzles

1243
01:26:19,700 --> 01:26:23,140
that require some cleverness to solve.

1244
01:26:23,140 --> 01:26:26,980
And you record a video.

1245
01:26:26,980 --> 01:26:30,620
They didn't have video in those days, so it was actual film.

1246
01:26:36,140 --> 01:26:38,980
There's a whole generation of publications

1247
01:26:38,980 --> 01:26:44,380
about the social and cognitive behavior

1248
01:26:44,380 --> 01:26:46,300
of these little groups of people.

1249
01:26:46,300 --> 01:26:48,900
They zeroed in on five-person groups

1250
01:26:48,900 --> 01:26:51,940
for reasons I don't remember, but it turned out

1251
01:26:51,940 --> 01:26:55,900
that almost always when you had that,

1252
01:26:55,900 --> 01:26:59,220
the group divided into two competitive groups

1253
01:26:59,220 --> 01:27:02,440
with two and three, and every now and then

1254
01:27:02,440 --> 01:27:03,900
they would reorganize.

1255
01:27:03,900 --> 01:27:07,500
But it was more a study in social relations

1256
01:27:07,500 --> 01:27:10,100
than in cognitive psychology.

1257
01:27:10,100 --> 01:27:16,020
But it's an interesting book, and there

1258
01:27:16,020 --> 01:27:19,620
must be contemporary studies like that

1259
01:27:19,620 --> 01:27:21,620
of how people cooperate.

1260
01:27:21,620 --> 01:27:25,940
But I just haven't been in that environment.

1261
01:27:25,940 --> 01:27:31,060
Any of you taken a psychology course recently?

1262
01:27:31,060 --> 01:27:31,700
Not a one?

1263
01:27:34,220 --> 01:27:39,420
Just wonder what's happened to general psychology.

1264
01:27:39,420 --> 01:27:43,100
I used to sit in on Teuber and a couple of other lecturers

1265
01:27:43,100 --> 01:27:46,660
here, and the psychology course was

1266
01:27:46,660 --> 01:27:54,380
sort of like 20% optical illusions and stuff like that.

1267
01:27:54,380 --> 01:27:58,740
They also concentrate a lot on developmental psychology.

1268
01:27:58,780 --> 01:28:04,300
Well, that's nice to hear, because I

1269
01:28:04,300 --> 01:28:07,940
don't believe there was any of that in Teuber's class.

1270
01:28:07,940 --> 01:28:11,780
I think Professor Gabrielli now teaches the introductory

1271
01:28:11,780 --> 01:28:14,540
psychology, and he still wants to talk about it.

1272
01:28:14,540 --> 01:28:21,220
Do they still believe Piaget, or do they think that he was wrong?

1273
01:28:21,220 --> 01:28:25,780
I think they probably take the same approach as with Freud.

1274
01:28:25,780 --> 01:28:29,100
They would say, great ideas and a revolution,

1275
01:28:29,100 --> 01:28:36,500
but they also don't think he's the end of the world.

1276
01:28:36,500 --> 01:28:39,580
Well, he got a

1277
01:28:39,580 --> 01:28:43,660
I know plenty of the childhood development class.

1278
01:28:43,660 --> 01:28:46,820
You read Piaget, but you read the books.

1279
01:28:46,820 --> 01:28:52,980
Yeah, in Piaget's later years, he got algebra envy,

1280
01:28:52,980 --> 01:28:57,340
and he wanted to be more scientific

1281
01:28:57,340 --> 01:29:03,420
and studied logic and a few things like that

1282
01:29:03,420 --> 01:29:05,580
and became less scientific.

1283
01:29:05,580 --> 01:29:11,380
It was sort of sad to, I can imagine,

1284
01:29:11,380 --> 01:29:13,740
being browbeaten by mathematicians,

1285
01:29:13,740 --> 01:29:16,300
because they're the ones who were getting published.

1286
01:29:19,500 --> 01:29:21,700
How many books did Piaget?

1287
01:29:21,740 --> 01:29:23,340
Incredible numbers.

1288
01:29:23,340 --> 01:29:26,180
If I may add a comment about Piaget here,

1289
01:29:26,180 --> 01:29:31,940
that really comes from an old friend of many of us, Seymour.

1290
01:29:31,940 --> 01:29:34,900
As you know, he was, of course, Piaget's mathematician

1291
01:29:34,900 --> 01:29:35,700
for many years.

1292
01:29:35,700 --> 01:29:39,260
We got back from Piaget's lab.

1293
01:29:39,260 --> 01:29:43,620
But Seymour said that he felt that Piaget's best work was

1294
01:29:43,620 --> 01:29:47,140
his early work, especially building his case studies.

1295
01:29:47,140 --> 01:29:49,780
And one time when we were talking

1296
01:29:49,820 --> 01:29:54,060
about the issue of focusing from the AI lab,

1297
01:29:54,060 --> 01:29:56,580
the work done in psychology here,

1298
01:29:56,580 --> 01:30:00,780
Seymour said he felt that was less necessary than more

1299
01:30:00,780 --> 01:30:04,380
of a concentration on AI, because he expected

1300
01:30:04,380 --> 01:30:07,860
in the future the world of study of the mind

1301
01:30:07,860 --> 01:30:11,780
would separate into two individual studies, one much

1302
01:30:11,780 --> 01:30:16,020
more biological, like the neurosciences of today,

1303
01:30:16,020 --> 01:30:18,780
and the other focused more on the structure of knowledge

1304
01:30:18,780 --> 01:30:21,580
and on representations and, in effect,

1305
01:30:21,580 --> 01:30:24,260
the genetic epistemology of Piaget.

1306
01:30:24,260 --> 01:30:27,780
And he added that something that was a quote later in the end

1307
01:30:27,780 --> 01:30:31,820
was that even if Piaget's marvelous theory today

1308
01:30:31,820 --> 01:30:34,140
proved to be wrong, he would ensure

1309
01:30:34,140 --> 01:30:36,180
that whatever replaced it would be

1310
01:30:36,180 --> 01:30:39,460
of theory of the same sort, one of the development

1311
01:30:39,460 --> 01:30:42,740
of knowledge in all its changes.

1312
01:30:42,740 --> 01:30:46,700
So I don't think people will get away from Piaget however much

1313
01:30:46,700 --> 01:30:47,780
they want to.

1314
01:30:47,860 --> 01:30:51,340
I don't think so either.

1315
01:30:51,340 --> 01:30:57,340
I meant to introduce our visitor here,

1316
01:30:57,340 --> 01:31:07,620
because Bob Lowler here has reproduced a good many

1317
01:31:07,620 --> 01:31:13,260
of the kinds of studies that Piaget did in the 1930s

1318
01:31:13,260 --> 01:31:14,780
and 40s.

1319
01:31:14,780 --> 01:31:16,740
And if you look him up on the web,

1320
01:31:16,740 --> 01:31:19,820
you'll must have a few papers.

1321
01:31:19,820 --> 01:31:22,420
Actually, I'd better tell you what the website is,

1322
01:31:22,420 --> 01:31:25,700
because it's still hidden from web probes.

1323
01:31:25,700 --> 01:31:29,340
It's nlcsa.net.

1324
01:31:29,340 --> 01:31:31,220
That would be hard to.

1325
01:31:31,220 --> 01:31:37,260
Natural Learning Case Study Archive.net.

1326
01:31:44,060 --> 01:31:46,580
It's still in process, still in development,

1327
01:31:46,580 --> 01:31:48,060
but it's worth looking at that.

1328
01:31:48,060 --> 01:31:50,340
How many children did Piaget have?

1329
01:31:50,340 --> 01:31:54,140
Well, Piaget had three children whose development he studied.

1330
01:31:54,140 --> 01:31:58,100
But what he did was to mix together

1331
01:31:58,100 --> 01:32:00,780
the information from all three studies

1332
01:32:00,780 --> 01:32:03,860
and supported the ideas with which he began.

1333
01:32:03,860 --> 01:32:09,180
So it was illustrations of his theories.

1334
01:32:09,180 --> 01:32:11,620
Anyway, Bob has quite a lot of studies

1335
01:32:11,620 --> 01:32:16,980
about how his children developed concepts of number

1336
01:32:16,980 --> 01:32:20,500
and geometry and things like that.

1337
01:32:20,500 --> 01:32:24,540
And I don't know of anyone else since Piaget

1338
01:32:24,540 --> 01:32:29,260
who's continued to do those sorts of experiments.

1339
01:32:29,260 --> 01:32:32,780
There were quite a lot at Piaget's Institute

1340
01:32:32,780 --> 01:32:37,980
in Geneva for some years after Piaget was gone.

1341
01:32:37,980 --> 01:32:40,700
But I think it's pretty much closed now, hasn't it?

1342
01:32:40,700 --> 01:32:44,420
Well, the last psychologist that Piaget hired was Jacques

1343
01:32:44,420 --> 01:32:50,500
Vanech, who was no longer at the university.

1344
01:32:50,500 --> 01:32:53,420
He retired, and it has been taken over

1345
01:32:53,420 --> 01:32:58,460
by the neo-Piagetians who are doing something different.

1346
01:32:58,460 --> 01:33:00,940
Is there any other place?

1347
01:33:00,940 --> 01:33:07,900
Well, there was Yoichi's lab on children in Japan.

1348
01:33:07,900 --> 01:33:13,900
There aren't many people who take Piaget seriously

1349
01:33:13,900 --> 01:33:15,380
in this country as well as others.

1350
01:33:21,300 --> 01:33:26,780
So Robert mentioned that fire had

1351
01:33:26,780 --> 01:33:29,660
more representations of the world

1352
01:33:29,660 --> 01:33:34,820
than usual people.

1353
01:33:34,820 --> 01:33:38,540
When I talked about Einstein and the glia cells,

1354
01:33:38,540 --> 01:33:41,740
I referred to that because I believe

1355
01:33:41,740 --> 01:33:45,380
k-lines is a way of representing the world.

1356
01:33:45,380 --> 01:33:51,700
And maybe Einstein had better ways of representing the world.

1357
01:33:51,700 --> 01:33:58,060
And I believe that, for example, agents and resources

1358
01:33:58,060 --> 01:34:00,660
are not different from the Turing machines.

1359
01:34:00,660 --> 01:34:04,460
You can create a very simple Turing machine

1360
01:34:04,580 --> 01:34:09,100
that acts like agents, and you have some mental states.

1361
01:34:09,100 --> 01:34:17,540
But there is no, I believe, good way right now

1362
01:34:17,540 --> 01:34:21,060
of representing the world and updating

1363
01:34:21,060 --> 01:34:23,780
this representation of the world.

1364
01:34:23,780 --> 01:34:29,420
It seems to me that when you grow up,

1365
01:34:29,420 --> 01:34:34,380
you're learning how to represent the world better and better.

1366
01:34:34,380 --> 01:34:39,540
And you have some layers, and that's all k-lines.

1367
01:34:39,540 --> 01:34:47,300
And if glia cells are actually related to k-lines,

1368
01:34:47,300 --> 01:34:52,940
so it means that Einstein had a better

1369
01:34:52,940 --> 01:34:55,980
hardware in representing the world.

1370
01:34:55,980 --> 01:35:01,380
That's why he would be smarter than other people.

1371
01:35:01,380 --> 01:35:10,140
Well, it's hard to I'm sure that that's right,

1372
01:35:10,140 --> 01:35:15,940
that you have a certain amount of hardware,

1373
01:35:15,940 --> 01:35:19,780
but you can reconfigure some of it.

1374
01:35:23,660 --> 01:35:26,940
Nobody really knows, but some brain centers

1375
01:35:26,940 --> 01:35:29,300
may have only a few neurons.

1376
01:35:29,340 --> 01:35:36,060
And maybe there's some retrograde signals

1377
01:35:36,060 --> 01:35:42,220
so that if two brain centers are simultaneously activated,

1378
01:35:42,220 --> 01:35:50,460
then usually the signals only go one way from one to the other.

1379
01:35:50,460 --> 01:35:53,260
Have to go through a third one to get back.

1380
01:35:53,260 --> 01:36:00,020
But it could be that the neurons have a property

1381
01:36:00,020 --> 01:36:02,180
that if two centers are activated,

1382
01:36:02,180 --> 01:36:04,940
maybe that causes more connections

1383
01:36:04,940 --> 01:36:08,660
to be made between them that can then be programmed more.

1384
01:36:08,660 --> 01:36:15,500
I don't think anybody really has a clear idea of whether you

1385
01:36:15,500 --> 01:36:19,020
can grow new connections between brain centers that

1386
01:36:19,020 --> 01:36:20,180
are far apart.

1387
01:36:20,180 --> 01:36:21,700
Does anybody know?

1388
01:36:21,700 --> 01:36:23,900
Is there anything?

1389
01:36:23,900 --> 01:36:24,400
Yeah?

1390
01:36:24,400 --> 01:36:24,900
Yeah?

1391
01:36:24,900 --> 01:36:26,980
One of the knowledge that there was no such thing

1392
01:36:26,980 --> 01:36:28,860
as adult neurogenesis, and now it

1393
01:36:28,860 --> 01:36:31,540
is known that it exists in certain limited regions

1394
01:36:31,540 --> 01:36:32,420
of the brain.

1395
01:36:32,420 --> 01:36:33,780
So in the future, it may be known

1396
01:36:33,780 --> 01:36:35,700
that it exists everywhere.

1397
01:36:35,700 --> 01:
[01:44:29.700 --> 01:44:35.380]  to make it into a question, except for the WH questions,
[01:44:35.380 --> 01:44:39.780]  which are standalone words and you don't need them.
[01:44:39.780 --> 01:44:42.980]  Yes, you'd say, this is expensive.
[01:44:42.980 --> 01:44:47.580]  And they don't need the WH if you do enough of that.
[01:44:54.140 --> 01:44:56.700]  So the question, is that in the brain at birth?
[01:45:00.700 --> 01:45:04.620]  Is that pattern married in English where you can say,
[01:45:04.620 --> 01:45:05.620]  is this expensive?
[01:45:05.620 --> 01:45:08.100]  And you can say, how expensive is this
[01:45:08.100 --> 01:45:10.580]  without that rising intonation?
[01:45:10.580 --> 01:45:14.180]  It's not a marriage thing using a separate word,
[01:45:14.180 --> 01:45:17.180]  but you don't need that separate word if it's an N word.
[01:45:17.180 --> 01:45:20.660]  But if you're saying, how expensive is this
[01:45:20.660 --> 01:45:22.220]  without the question of inflection,
[01:45:22.220 --> 01:45:25.660]  it almost sounds like you're making a statement about just
[01:45:25.660 --> 01:45:28.300]  how ridiculously expensive it is, you know?
[01:45:28.300 --> 01:45:31.220]  Like, you're going, how expensive is this?
[01:45:31.220 --> 01:45:33.180]  Versus, how expensive is this?
[01:45:55.540 --> 01:45:56.900]  Well, I should let you go.
ecent ones,

1416
01:37:52,540 --> 01:38:02,540
but he had an army of students, and he was extremely funny.

1417
01:38:02,540 --> 01:38:03,540
What else?

1418
01:38:07,540 --> 01:38:11,540
Continuing on the idea of hardware versus software,

1419
01:38:11,540 --> 01:38:17,940
what do you think about the idea that intelligences or humans

1420
01:38:17,940 --> 01:38:22,540
may need strong instincts when they're born?

1421
01:38:22,540 --> 01:38:24,940
And the interplay between their instincts,

1422
01:38:24,940 --> 01:38:27,340
like they know to cry when they're hungry

1423
01:38:27,340 --> 01:38:28,740
or to look for their mother.

1424
01:38:28,740 --> 01:38:30,740
And they need these instincts because they

1425
01:38:31,740 --> 01:38:34,580
need these instincts in order to develop

1426
01:38:34,580 --> 01:38:37,900
higher orders of knowledge.

1427
01:38:37,900 --> 01:38:39,860
You'd have to ask L. Ron Hubbard for it.

1428
01:38:39,860 --> 01:38:40,360
Thank you.

1429
01:38:50,060 --> 01:38:55,380
I don't recall any real attempts to see.

1430
01:39:03,780 --> 01:39:08,780
I don't think I've ever run across anybody

1431
01:39:08,780 --> 01:39:13,500
claiming to have correlations between prenatal experience

1432
01:39:13,500 --> 01:39:18,500
and the development of intelligence.

1433
01:39:18,500 --> 01:39:23,020
I was talking about before intelligence

1434
01:39:23,020 --> 01:39:26,540
has really developed, before you learn language,

1435
01:39:26,540 --> 01:39:28,980
you need to have a motivation to do some things.

1436
01:39:28,980 --> 01:39:31,660
So you need to have instinctual reactions to things.

1437
01:39:31,660 --> 01:39:34,940
And by accumulation of experiential knowledge,

1438
01:39:34,940 --> 01:39:36,620
after you're born, you need to have that.

1439
01:39:36,620 --> 01:39:42,180
Children learn language 12 to 18 months.

1440
01:39:42,180 --> 01:39:43,620
What are you saying?

1441
01:39:43,620 --> 01:39:45,140
That they need some preparation?

1442
01:39:49,220 --> 01:39:51,740
I'm not sure what you're asking.

1443
01:39:51,740 --> 01:39:53,700
So thinking from an engineering point of view,

1444
01:39:53,700 --> 01:39:58,260
if you were to build a robot, would

1445
01:39:58,260 --> 01:40:00,780
you need to program it with some instincts,

1446
01:40:00,780 --> 01:40:05,140
some real thumb algorithms in order

1447
01:40:05,140 --> 01:40:06,900
to get it started in the world in order

1448
01:40:06,900 --> 01:40:09,260
to build experiential knowledge?

1449
01:40:09,260 --> 01:40:10,740
You might want to build something

1450
01:40:10,740 --> 01:40:14,420
like a difference engine so that you can represent a goal

1451
01:40:14,420 --> 01:40:16,660
and it will try to achieve it.

1452
01:40:16,660 --> 01:40:21,700
So you need some engine for producing any behavior at all.

1453
01:40:24,140 --> 01:40:26,540
If you take the approach that maybe to build an AI,

1454
01:40:26,540 --> 01:40:30,060
you should build an infant robot and then you

1455
01:40:30,060 --> 01:40:33,660
teach it as you were like a human child,

1456
01:40:33,660 --> 01:40:35,620
then would it be useful to make it

1457
01:40:35,620 --> 01:40:38,100
dependent on some other figure in order

1458
01:40:38,100 --> 01:40:41,580
to help it learn how to do things like a human child would?

1459
01:40:47,020 --> 01:40:50,620
Well, in order to learn, you have to learn from something.

1460
01:40:50,620 --> 01:40:57,460
And one way to learn is in isolation, just to have some,

1461
01:40:57,460 --> 01:41:01,980
you could build a goal to predict what will happen.

1462
01:41:01,980 --> 01:41:06,620
And the best way to predict, as Alan Kay put it once,

1463
01:41:06,620 --> 01:41:10,700
the best way to predict the future is to invent it.

1464
01:41:10,700 --> 01:41:19,740
So you could make a, or you could put a model of an adult

1465
01:41:19,740 --> 01:41:27,100
in it to start with so that, in other words,

1466
01:41:27,100 --> 01:41:29,820
one way to make a very smart child

1467
01:41:29,820 --> 01:41:34,340
is to copy its mother's brain into a little subbrain

1468
01:41:34,340 --> 01:41:35,540
when it's born.

1469
01:41:35,540 --> 01:41:36,940
And then it could learn from that

1470
01:41:36,940 --> 01:41:39,860
instead of depending on anybody else.

1471
01:41:39,860 --> 01:41:42,340
I'm not sure.

1472
01:41:42,340 --> 01:41:43,700
You have to start with something.

1473
01:41:43,700 --> 01:42:02,660
Of course, humans, as Bob mentioned or someone mentioned,

1474
01:42:02,660 --> 01:42:06,900
if you take a human baby and isolate it,

1475
01:42:06,900 --> 01:42:13,540
it looks like it won't develop language by itself

1476
01:42:13,540 --> 01:42:15,580
because I don't know what because.

1477
01:42:22,900 --> 01:42:25,100
In fact, I remember one of our children

1478
01:42:25,100 --> 01:42:27,820
who was just learning to talk.

1479
01:42:27,820 --> 01:42:30,820
And she said, something came up.

1480
01:42:30,820 --> 01:42:32,660
And she said, what because is that?

1481
01:42:36,900 --> 01:42:37,900
Do you remember?

1482
01:42:37,900 --> 01:42:42,060
It took a while to get her to say why.

1483
01:42:42,660 --> 01:42:46,820
So she would come up and say, what because?

1484
01:42:46,820 --> 01:42:52,180
And I would say, you're asking, why did this?

1485
01:42:52,180 --> 01:42:54,140
After a long time, she got the hint.

1486
01:42:58,980 --> 01:43:03,100
Why do all WH words start with WH?

1487
01:43:03,100 --> 01:43:04,020
One of them doesn't.

1488
01:43:04,020 --> 01:43:04,660
How?

1489
01:43:04,660 --> 01:43:07,100
How?

1490
01:43:07,100 --> 01:43:08,060
Could you say, wow?

1491
01:43:12,060 --> 01:43:12,560
How?

1492
01:43:15,340 --> 01:43:16,180
Is there a theory?

1493
01:43:19,020 --> 01:43:20,780
It's a basic sound telling that you're

1494
01:43:20,780 --> 01:43:26,500
making a query before you can do the rising inflection.

1495
01:43:26,500 --> 01:43:27,860
It's interesting.

1496
01:43:27,860 --> 01:43:30,260
Is it true in French?

1497
01:43:30,260 --> 01:43:31,820
Quoi?

1498
01:43:31,820 --> 01:43:34,180
Well, the land of the silent letter.

1499
01:43:35,180 --> 01:43:35,680
That's it.

1500
01:43:40,500 --> 01:43:45,300
Anybody know what's the equivalent of WH words

1501
01:43:45,300 --> 01:43:48,460
in your native language?

1502
01:43:48,460 --> 01:43:49,700
What?

1503
01:43:49,700 --> 01:43:51,180
N?

1504
01:43:51,180 --> 01:43:52,900
In what?

1505
01:43:52,900 --> 01:43:54,100
Really?

1506
01:43:54,100 --> 01:43:56,020
They all start with N?

1507
01:43:56,020 --> 01:43:56,520
Wow.

1508
01:44:00,060 --> 01:44:03,260
Interesting.

1509
01:44:03,300 --> 01:44:08,340
Maybe the infants have an effect on something.

1510
01:44:08,340 --> 01:44:13,220
Do questions in Turkish end with a rise?

1511
01:44:13,220 --> 01:44:14,460
Is it rising?

1512
01:44:14,460 --> 01:44:20,460
So only the WH questions, OK, all questions

1513
01:44:20,460 --> 01:44:22,500
end in a kind of an inflection.

1514
01:44:22,500 --> 01:44:26,820
But normally, you have a little word

1515
01:44:26,820 --> 01:44:29,700
that you would put at the end of any sentence

1516
01:44:29,700 --> 01:44:35,380
to make it into a question, except for the WH questions,

1517
01:44:35,380 --> 01:44:39,780
which are standalone words and you don't need them.

1518
01:44:39,780 --> 01:44:42,980
Yes, you'd say, this is expensive.

1519
01:44:42,980 --> 01:44:47,580
And they don't need the WH if you do enough of that.

1520
01:44:54,140 --> 01:44:56,700
So the question, is that in the brain at birth?

1521
01:45:00,700 --> 01:45:04,620
Is that pattern married in English where you can say,

1522
01:45:04,620 --> 01:45:05,620
is this expensive?

1523
01:45:05,620 --> 01:45:08,100
And you can say, how expensive is this

1524
01:45:08,100 --> 01:45:10,580
without that rising intonation?

1525
01:45:10,580 --> 01:45:14,180
It's not a marriage thing using a separate word,

1526
01:45:14,180 --> 01:45:17,180
but you don't need that separate word if it's an N word.

1527
01:45:17,180 --> 01:45:20,660
But if you're saying, how expensive is this

1528
01:45:20,660 --> 01:45:22,220
without the question of inflection,

1529
01:45:22,220 --> 01:45:25,660
it almost sounds like you're making a statement about just

1530
01:45:25,660 --> 01:45:28,300
how ridiculously expensive it is, you know?

1531
01:45:28,300 --> 01:45:31,220
Like, you're going, how expensive is this?

1532
01:45:31,220 --> 01:45:33,180
Versus, how expensive is this?

1533
01:45:55,540 --> 01:45:56,900
Well, I should let you go.

