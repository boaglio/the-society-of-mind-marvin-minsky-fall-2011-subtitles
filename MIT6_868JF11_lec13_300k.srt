1
00:00:00,000 --> 00:00:02,400
The following content is provided under a Creative

2
00:00:02,400 --> 00:00:03,760
Commons license.

3
00:00:03,760 --> 00:00:06,000
Your support will help MIT OpenCourseWare

4
00:00:06,000 --> 00:00:10,080
continue to offer high quality educational resources for free.

5
00:00:10,080 --> 00:00:12,660
To make a donation or to view additional materials

6
00:00:12,660 --> 00:00:16,560
from hundreds of MIT courses, visit MIT OpenCourseWare

7
00:00:16,560 --> 00:00:17,600
at ocw.mit.edu.

8
00:00:17,600 --> 00:00:30,800
So everyone's working on papers, I suppose.

9
00:00:30,800 --> 00:00:32,160
When are they due in?

10
00:00:32,160 --> 00:00:34,000
They're due on Sunday.

11
00:00:34,000 --> 00:00:36,760
Horrors.

12
00:00:36,760 --> 00:00:40,840
Well, when I went to graduate school,

13
00:00:40,840 --> 00:00:46,160
it was at Princeton in the math department.

14
00:00:46,160 --> 00:00:49,760
And there were all sorts of courses

15
00:00:49,760 --> 00:00:54,800
given by very good professors, because I

16
00:00:54,800 --> 00:00:56,800
think I've mentioned this.

17
00:00:56,800 --> 00:01:07,600
This is sort of 1950, and the universities in the United

18
00:01:07,600 --> 00:01:11,160
States were, at least the science departments,

19
00:01:11,160 --> 00:01:14,900
were filled with great mathematicians and physicists

20
00:01:15,780 --> 00:01:23,540
and people like that who had been extracted from Europe

21
00:01:23,540 --> 00:01:25,940
because of World War II.

22
00:01:25,940 --> 00:01:32,100
And the math department at Princeton

23
00:01:32,100 --> 00:01:42,180
was very, very, very exciting because of the,

24
00:01:42,180 --> 00:01:47,180
if you had a question, you had a very good chance

25
00:01:47,180 --> 00:01:53,700
that the world's leading authority on that subject

26
00:01:53,700 --> 00:01:57,260
would be in the next room or two.

27
00:01:57,260 --> 00:01:59,300
I was interested in the theory of knots,

28
00:01:59,300 --> 00:02:02,260
and there was a Professor Ralph Fox, who

29
00:02:02,260 --> 00:02:07,100
was one of the world's experts on knot theory.

30
00:02:07,100 --> 00:02:09,580
And in fact, he was so good at it that I gave up.

31
00:02:13,180 --> 00:02:17,940
This is a nice, important thing to keep in mind.

32
00:02:17,940 --> 00:02:20,740
Always be ready to give up if there's someone

33
00:02:20,740 --> 00:02:25,540
doing something better than you and find the right niche.

34
00:02:25,540 --> 00:02:32,060
But anyway, they had courses, and you

35
00:02:32,060 --> 00:02:34,820
would register for courses, and everyone

36
00:02:34,820 --> 00:02:37,740
got an A in all courses.

37
00:02:37,740 --> 00:02:38,460
I'm sorry.

38
00:02:38,460 --> 00:02:43,180
There were no grades when I got there

39
00:02:43,180 --> 00:02:46,780
because Professor Lefcets didn't like the idea of grading

40
00:02:46,780 --> 00:02:52,060
people, and he was head of the department.

41
00:02:52,060 --> 00:02:59,300
And his feeling was that if somebody wasn't doing good work,

42
00:02:59,300 --> 00:03:01,180
then it wasn't that person's fault.

43
00:03:01,180 --> 00:03:05,060
It was the fault of the admissions committee.

44
00:03:05,060 --> 00:03:10,380
And if anybody got through the hurdles to get there,

45
00:03:10,380 --> 00:03:14,540
then they were assumed to be mathematicians,

46
00:03:14,540 --> 00:03:17,860
and that was that.

47
00:03:17,860 --> 00:03:23,580
But after a couple of years, the provost of the university

48
00:03:23,580 --> 00:03:25,500
got annoyed with the math department,

49
00:03:25,500 --> 00:03:28,780
and he sent down a message that there would have

50
00:03:28,780 --> 00:03:31,460
to be grades in every course.

51
00:03:31,460 --> 00:03:35,620
And Lefcets said, OK.

52
00:03:35,620 --> 00:03:37,700
He didn't believe in fighting when

53
00:03:37,700 --> 00:03:40,220
there was a good alternative.

54
00:03:40,220 --> 00:03:44,700
The alternative was everybody got A's in every course,

55
00:03:44,700 --> 00:03:46,980
whether they took the course or not.

56
00:03:46,980 --> 00:03:59,220
And I think the end of the story is

57
00:03:59,220 --> 00:04:03,180
that when he retired, the new chairman

58
00:04:03,180 --> 00:04:08,820
wasn't as good at evading the authorities.

59
00:04:08,820 --> 00:04:13,540
So I have a little discomfort at grades myself.

60
00:04:13,540 --> 00:04:22,900
And when I gave this course in the 60s and in the 70s,

61
00:04:22,900 --> 00:04:27,340
everybody got an A. And it took several years

62
00:04:27,340 --> 00:04:28,940
for the departments to notice.

63
00:04:32,420 --> 00:04:40,340
Anyway, so we have to give grades at MIT.

64
00:04:44,940 --> 00:04:47,420
But all that's great fun.

65
00:04:47,420 --> 00:04:50,020
Anybody?

66
00:04:50,020 --> 00:04:53,580
What didn't we cover in this course that you wish we had?

67
00:04:53,580 --> 00:04:56,620
And I'd like to know what some of you

68
00:04:56,620 --> 00:04:59,940
are doing also.

69
00:04:59,940 --> 00:05:03,580
What's your plan to make machines smarter

70
00:05:03,580 --> 00:05:05,780
in the future?

71
00:05:05,780 --> 00:05:15,220
And how hard is it to find a job where you can do research?

72
00:05:15,220 --> 00:05:19,180
I haven't tried getting a new job for a long time.

73
00:05:19,180 --> 00:05:27,980
I just tried to hire a former student,

74
00:05:27,980 --> 00:05:30,100
but he's starting a huge startup.

75
00:05:30,100 --> 00:05:36,980
And if you can't find a job, I guess you can make one

76
00:05:36,980 --> 00:05:42,860
if you can fool enough investors into supporting a new operation.

77
00:05:49,180 --> 00:05:53,380
The most of the work you're going to do

78
00:05:53,380 --> 00:05:56,100
is designed around a very specific problem.

79
00:05:56,100 --> 00:05:56,980
Design what?

80
00:05:56,980 --> 00:05:58,860
Around a very specific problem that you're

81
00:05:58,860 --> 00:06:03,220
addressing, getting a machine to solve some particular problem.

82
00:06:03,220 --> 00:06:05,100
There's always a fast path to that

83
00:06:05,100 --> 00:06:07,420
by finding the right set of data and really

84
00:06:07,420 --> 00:06:12,980
finding the right model that will work reasonably well.

85
00:06:12,980 --> 00:06:17,580
It's not really like a business case

86
00:06:17,580 --> 00:06:22,700
that you're made for making a general intelligence in it

87
00:06:22,700 --> 00:06:28,620
when there's motivation for some specific application.

88
00:06:28,620 --> 00:06:29,340
That's right.

89
00:06:41,220 --> 00:06:43,660
Goes back to World War II again.

90
00:06:43,660 --> 00:06:49,020
The reason we could do so much in those early years

91
00:06:49,020 --> 00:06:59,380
about general theories was that the Defense Department had

92
00:06:59,380 --> 00:07:07,460
created this new wing called ARPA, Advanced Research

93
00:07:07,460 --> 00:07:09,740
Projects Agency.

94
00:07:09,740 --> 00:07:14,100
And I actually don't know what caused it to begin,

95
00:07:14,100 --> 00:07:20,340
but a lot of it was involved with trying

96
00:07:20,340 --> 00:07:23,700
to make new kinds of instruments to detect

97
00:07:23,700 --> 00:07:29,740
intercontinental missiles and heaven knows what else.

98
00:07:29,740 --> 00:07:37,860
But they decided to have a basic research branch

99
00:07:37,860 --> 00:07:45,060
to because if you're in the military, one of your concerns

100
00:07:45,060 --> 00:07:46,740
is defense.

101
00:07:46,740 --> 00:07:55,900
And for some reason that I don't know how to explain,

102
00:07:55,900 --> 00:08:00,860
but a group of people in the Office of Naval Research

103
00:08:00,860 --> 00:08:05,220
concluded that the best defense is

104
00:08:05,220 --> 00:08:09,960
to be way ahead in technology of everyone else.

105
00:08:09,960 --> 00:08:14,180
And since there were no particular constraints on them,

106
00:08:14,180 --> 00:08:19,260
they didn't have to say you have to write a progress

107
00:08:19,260 --> 00:08:23,260
report every three months and stuff like that.

108
00:08:35,540 --> 00:08:42,340
Anyway, another unique thing about the situation

109
00:08:42,340 --> 00:08:46,020
when I started was that the students knew more

110
00:08:46,020 --> 00:08:52,580
than the professors about computer science.

111
00:08:52,580 --> 00:09:01,260
Now, there were people around MIT who knew about computers,

112
00:09:01,260 --> 00:09:06,580
but we had the bad luck of having the best analog

113
00:09:06,580 --> 00:09:13,020
computers in the world, Vannevar Bush and people like that.

114
00:09:16,180 --> 00:09:20,860
So when things change suddenly enough,

115
00:09:20,860 --> 00:09:26,120
your best people might turn out to be the worst for a few years.

116
00:09:26,120 --> 00:09:30,320
And the leaders in innovation came from the Tech Model

117
00:09:30,320 --> 00:09:36,640
Railroad Club and a few institutions like that.

118
00:09:36,640 --> 00:09:43,160
I certainly knew something about computers, at least

119
00:09:43,160 --> 00:09:47,440
the theory of them, but I didn't know anything

120
00:09:47,440 --> 00:09:49,840
about modern programming.

121
00:09:49,840 --> 00:09:53,160
And either did anyone else, except Newell and Simon

122
00:09:53,160 --> 00:09:59,440
and Carnegie Mellon and John McCarthy, who

123
00:09:59,460 --> 00:10:02,580
they invented Lisp processing languages,

124
00:10:02,580 --> 00:10:07,900
and John McCarthy refined them and basically combined

125
00:10:07,900 --> 00:10:14,340
a very clumsy logo-like Lisp processing language

126
00:10:14,340 --> 00:10:21,700
with algebraic language.

127
00:10:21,700 --> 00:10:28,480
And so Lisp came after.

128
00:10:28,480 --> 00:10:32,000
There had been primitive Lisp processors.

129
00:10:32,000 --> 00:10:36,580
And all of Newell and Simon's work, including a chess program,

130
00:10:36,580 --> 00:10:41,580
had been done with just a programming language that

131
00:10:41,580 --> 00:10:43,700
had something like car, clutter, and cons.

132
00:10:44,700 --> 00:10:56,100
So that was about 10 years after the beginning.

133
00:10:56,100 --> 00:11:00,020
The first computers at MIT came mainly

134
00:11:00,020 --> 00:11:05,700
from Lincoln Lab, who had the job of building supercomputers

135
00:11:05,700 --> 00:11:12,940
to analyze radar data and stuff like that, which were indeed

136
00:11:12,940 --> 00:11:17,020
connected with national defense and trying

137
00:11:17,020 --> 00:11:21,100
to combine radar reports from all sorts of places

138
00:11:21,100 --> 00:11:27,540
to get early warnings of activities in space

139
00:11:27,540 --> 00:11:30,220
and so forth.

140
00:11:30,220 --> 00:11:35,060
When the first Sputnik was launched by the Russians,

141
00:11:35,060 --> 00:11:38,020
there was just a little thing going beep, beep,

142
00:11:38,020 --> 00:11:40,980
beep in low Earth orbit.

143
00:11:40,980 --> 00:11:52,540
And within a couple of hours after the Russians announced

144
00:11:52,540 --> 00:12:01,620
that Sputnik was there and they announced its orbit,

145
00:12:01,620 --> 00:12:05,660
a couple of hackers at Lincoln Lab, surely,

146
00:12:05,660 --> 00:12:13,260
managed to track the thing by hooking a couple of radio.

147
00:12:13,260 --> 00:12:15,380
It was going beep, beep, beep.

148
00:12:15,380 --> 00:12:20,060
And a guy named Brad Howland, who was a teenager,

149
00:12:20,060 --> 00:12:23,600
I think, then hooked a couple of radio receivers

150
00:12:23,600 --> 00:12:25,940
up with some flip flops and things

151
00:12:25,940 --> 00:12:31,020
and was able to get its exact trajectory from the Doppler

152
00:12:31,020 --> 00:12:31,520
shifts.

153
00:12:36,940 --> 00:12:45,180
Anyway, then I think I've talked about that history.

154
00:12:45,180 --> 00:12:50,980
At some point about early 1970 or so,

155
00:12:50,980 --> 00:12:57,460
it's 10 or 15 years after ARPA started.

156
00:12:57,460 --> 00:13:03,940
Well, the big ARPA project was called Project MAC in 1963.

157
00:13:03,940 --> 00:13:13,480
And MIT got $3 million a year for designing and building

158
00:13:13,480 --> 00:13:15,220
a time sharing system.

159
00:13:15,220 --> 00:13:17,820
The idea for a time sharing system

160
00:13:17,820 --> 00:13:20,300
had bubbled up in a few places.

161
00:13:20,300 --> 00:13:23,500
But it was mainly a couple of other youngsters, John

162
00:13:23,500 --> 00:13:30,620
McCarthy and Edward Fredkin, who recognized

163
00:13:30,620 --> 00:13:34,260
that if a computer could compute thousands of operations

164
00:13:34,260 --> 00:13:38,540
per second, then it should be able to compute

165
00:13:38,540 --> 00:13:43,020
hundreds of operations per second for tens of people.

166
00:13:43,020 --> 00:13:47,100
And when it gets to a billion operations per second,

167
00:13:47,100 --> 00:13:49,760
then it should be able to compute

168
00:13:49,760 --> 00:13:52,820
a million operations per second for 1,000 people

169
00:13:52,820 --> 00:13:54,220
and so forth.

170
00:13:54,220 --> 00:13:59,860
And MIT was the first place to develop time sharing system.

171
00:13:59,860 --> 00:14:03,940
And as I said, they sent this $3 million a year

172
00:14:03,940 --> 00:14:07,820
to get that project to work.

173
00:14:07,820 --> 00:14:11,360
And because the guy in charge of that

174
00:14:11,360 --> 00:14:18,060
was a former professor of psychology named Joe Licklider,

175
00:14:18,060 --> 00:14:22,780
had been a friend of mine when I was an undergraduate,

176
00:14:22,780 --> 00:14:25,980
he added another million dollars a year

177
00:14:25,980 --> 00:14:30,700
and gave it to us to start the AI lab.

178
00:14:30,700 --> 00:14:37,580
And if that ever happens to you, I hope it will.

179
00:14:37,580 --> 00:14:50,340
Anyway, as I complained a lot during the course,

180
00:14:50,340 --> 00:14:56,060
we were able to sort of do anything

181
00:14:56,060 --> 00:14:58,740
we thought was possible.

182
00:14:58,740 --> 00:15:01,220
And we didn't worry about whether it could be done

183
00:15:01,220 --> 00:15:04,300
in a year or two or 20.

184
00:15:04,340 --> 00:15:08,700
We just had enough money and enough adventurous people

185
00:15:08,700 --> 00:15:13,100
to just start doing those things.

186
00:15:13,100 --> 00:15:17,100
So in the very first year, we had

187
00:15:17,100 --> 00:15:22,180
Jim Slagle making the program that

188
00:15:22,180 --> 00:15:31,500
could do integrals at the level that students in 1801

189
00:15:31,500 --> 00:15:32,380
were learning to do.

190
00:15:34,700 --> 00:15:37,960
I think I mentioned I made the first program that

191
00:15:37,960 --> 00:15:39,700
computed derivatives.

192
00:15:39,700 --> 00:15:42,460
And it was just five or 10 lines of code

193
00:15:42,460 --> 00:15:46,300
because it worked lexically.

194
00:15:46,300 --> 00:15:53,380
It said, if you see xy, replace it by the symbols x dy plus y

195
00:15:53,380 --> 00:15:59,020
dx, and so forth.

196
00:15:59,020 --> 00:16:02,960
That's how you do derivatives, where d means derivative.

197
00:16:02,960 --> 00:16:06,360
And since it's operating on the names of functions,

198
00:16:06,360 --> 00:16:09,560
then you just recurse until all the function names

199
00:16:09,560 --> 00:16:11,360
have disappeared.

200
00:16:11,360 --> 00:16:16,760
And so that was just a page of code.

201
00:16:16,760 --> 00:16:23,800
And then Slagle, who was blind, wrote 20 pages of code.

202
00:16:23,800 --> 00:16:27,920
And Joel Moses, who was what's he now?

203
00:16:27,920 --> 00:16:29,720
Provost or?

204
00:16:29,720 --> 00:16:32,640
He used to be.

205
00:16:32,640 --> 00:16:36,800
Did he retire or go back to work?

206
00:16:36,800 --> 00:16:37,680
Probably both.

207
00:16:41,160 --> 00:16:42,120
Yep.

208
00:16:42,120 --> 00:16:48,880
And he mentioned that it took him a very long time

209
00:16:48,880 --> 00:16:51,920
to decode Slagle's 20 pages.

210
00:16:51,920 --> 00:16:56,200
Because since Slagle is writing in Braille,

211
00:16:56,200 --> 00:16:58,600
poor Slagle has to understand his code, too.

212
00:16:58,600 --> 00:17:01,840
And you can't just glance at a page.

213
00:17:01,840 --> 00:17:07,080
So he wrote this incredibly intricate compressed stuff.

214
00:17:07,080 --> 00:17:15,560
And it had the weird property that it solved integrals.

215
00:17:15,560 --> 00:17:19,000
It did integrals at just about the same speed

216
00:17:19,000 --> 00:17:21,080
as good math students.

217
00:17:21,080 --> 00:17:26,120
So if you gave it an expression with a couple of sines

218
00:17:26,120 --> 00:17:31,880
and cosines and things, it might take 10 or 15 minutes.

219
00:17:31,880 --> 00:17:36,600
And everybody was very impressed that this computer could keep

220
00:17:36,600 --> 00:17:38,360
up with a human.

221
00:17:38,360 --> 00:17:47,600
But of course, that fact had no meaning whatever.

222
00:17:47,600 --> 00:17:53,600
That happened to be how slow computers were at that time.

223
00:17:53,600 --> 00:17:59,920
The IBM 704, which was I guess we had a 701 and then a 704.

224
00:17:59,920 --> 00:18:05,560
And I think that machine was doing about 20,000 operations

225
00:18:05,560 --> 00:18:10,120
per second, which is probably more than a person can do.

226
00:18:10,120 --> 00:18:11,200
But no one knows.

227
00:18:11,200 --> 00:18:25,200
Well, so who has an idea on what should be done next?

228
00:18:29,360 --> 00:18:32,640
Yeah, what would you like to do?

229
00:18:32,640 --> 00:18:36,960
A robot that would integrate, like, I don't know,

230
00:18:36,960 --> 00:18:43,280
maybe an AI system that would interact more with the world

231
00:18:43,280 --> 00:18:46,320
if I do.

232
00:18:46,320 --> 00:18:48,680
Well, we're going to need robots pretty soon.

233
00:18:52,680 --> 00:18:55,480
Just for example, this aging problem.

234
00:18:55,480 --> 00:18:57,240
We're getting more and more people

235
00:18:57,240 --> 00:19:04,200
who are crippled in one way or another,

236
00:19:04,200 --> 00:19:10,920
handicapped, a huge population of people

237
00:19:10,920 --> 00:19:16,840
with mild brain disorders, of course, serious ones also.

238
00:19:16,840 --> 00:19:20,960
And there are not enough nurses to take care

239
00:19:20,960 --> 00:19:26,600
of the disabled, aged people.

240
00:19:26,600 --> 00:19:30,680
And I think that's probably going

241
00:19:30,680 --> 00:19:34,080
to be a major need for robotics.

242
00:19:36,520 --> 00:19:39,320
As I see it, the problem with robotics

243
00:19:39,320 --> 00:19:42,760
is that people build robots.

244
00:19:42,760 --> 00:19:44,960
And you look at any laboratory that

245
00:19:44,960 --> 00:19:48,600
has built a physical robot, and Media Lab

246
00:19:48,600 --> 00:19:52,320
is a perfect example of it.

247
00:19:52,320 --> 00:19:57,360
Or Rod Brooks had a wonderful robot called

248
00:19:57,360 --> 00:20:01,480
Cog with a head and two hands.

249
00:20:01,480 --> 00:20:03,480
Took a lot of engineering.

250
00:20:03,480 --> 00:20:05,560
And there was only one of it.

251
00:20:05,560 --> 00:20:07,880
And it only worked for a few hours a week

252
00:20:07,880 --> 00:20:11,840
because it's very complicated and it breaks.

253
00:20:11,840 --> 00:20:20,960
And so the joke is that if I were starting a robotics lab,

254
00:20:21,000 --> 00:20:22,280
I would not permit robots.

255
00:20:25,800 --> 00:20:31,040
Once you simulate the robots and get them to do what you want,

256
00:20:31,040 --> 00:20:34,920
then you take it out of the university

257
00:20:34,920 --> 00:20:38,920
and hire some good mechanical engineers

258
00:20:38,920 --> 00:20:41,240
to make the robot you want.

259
00:20:41,240 --> 00:20:42,600
But you don't want to.

260
00:20:42,600 --> 00:20:45,200
Robotics research should not involve robots.

261
00:20:45,200 --> 00:20:54,040
And I think I have sort of mixed feelings about the, what's

262
00:20:54,040 --> 00:20:56,600
the great basketball robot event called?

263
00:21:02,000 --> 00:21:05,720
Well, whatever you call it, they kick a ball and try to get.

264
00:21:05,720 --> 00:21:11,960
It's actually, they try to get it through to a goal.

265
00:21:11,960 --> 00:21:13,440
So it is soccer.

266
00:21:15,440 --> 00:21:18,960
Yeah, whatever.

267
00:21:18,960 --> 00:21:22,680
That's probably a plus for the high school students

268
00:21:22,680 --> 00:21:25,080
who are involved in it.

269
00:21:25,080 --> 00:21:26,760
But yes?

270
00:21:26,760 --> 00:21:34,920
Why do you have robots that all universities use as a base?

271
00:21:34,920 --> 00:21:39,520
Why do we keep building hardware?

272
00:21:39,520 --> 00:21:44,520
Is it necessary to keep building hardware back again?

273
00:21:44,520 --> 00:21:47,160
I'm not sure what's your question.

274
00:21:47,160 --> 00:21:51,160
When we use computers, we don't build the computers back again.

275
00:21:51,160 --> 00:21:55,160
We use the, like we just create software.

276
00:21:55,160 --> 00:21:55,660
Yes.

277
00:21:55,660 --> 00:21:58,920
Aren't we doing the same for robots?

278
00:21:58,920 --> 00:22:03,760
And if not, why aren't we just programming the robot

279
00:22:03,760 --> 00:22:06,240
to do stuff that we want?

280
00:22:06,240 --> 00:22:09,680
What I'm saying is if people program the robot,

281
00:22:09,680 --> 00:22:14,080
then it takes them two weeks to do the simplest thing.

282
00:22:14,080 --> 00:22:17,240
If you're programming a simulated robot,

283
00:22:17,240 --> 00:22:19,360
then it takes two minutes.

284
00:22:19,360 --> 00:22:23,000
Why does it take more to program the robot?

285
00:22:23,000 --> 00:22:26,400
Because the robot's broken, and there's only one in the lab.

286
00:22:26,400 --> 00:22:28,920
And there are eight people who want to use it.

287
00:22:28,920 --> 00:22:33,600
And 3 quarters of the time, it's down because some motor,

288
00:22:33,600 --> 00:22:36,440
because it's not in mass production.

289
00:22:36,440 --> 00:22:40,620
If you have a machine that somebody made,

290
00:22:40,620 --> 00:22:43,500
it's probably the first machine that person ever made.

291
00:22:46,100 --> 00:22:48,380
So it's no good.

292
00:22:48,380 --> 00:22:51,220
Look at the Honda robot called ASIMO.

293
00:22:54,900 --> 00:22:56,860
You've all seen it.

294
00:22:56,860 --> 00:23:00,980
But you all saw it 10 years ago.

295
00:23:00,980 --> 00:23:02,940
It has not changed.

296
00:23:02,940 --> 00:23:06,660
But they say it doesn't fall over so often anymore.

297
00:23:10,980 --> 00:23:12,660
There is a company, a French company,

298
00:23:12,660 --> 00:23:16,820
that does a robot called Now Robot that works pretty good.

299
00:23:16,820 --> 00:23:17,580
I don't know.

300
00:23:17,580 --> 00:23:22,860
Why don't we buy some robots from some company?

301
00:23:22,860 --> 00:23:25,500
Because they're no good.

302
00:23:25,500 --> 00:23:26,780
They don't do any.

303
00:23:26,780 --> 00:23:27,940
They've only got one elbow.

304
00:23:30,580 --> 00:23:32,220
They can't reach around.

305
00:23:32,220 --> 00:23:35,740
They don't have, they don't.

306
00:23:35,740 --> 00:23:40,740
You can buy a hand for $30,000 or $40,000

307
00:23:40,740 --> 00:23:42,580
that has three or four fingers.

308
00:23:42,580 --> 00:23:43,080
But

309
00:23:43,080 --> 00:23:45,100
There's probably a trade-off, though,

310
00:23:45,100 --> 00:23:49,780
between making a more complex robot that has more interaction

311
00:23:49,780 --> 00:23:51,260
you can make.

312
00:23:51,260 --> 00:23:53,620
That gets more difficult when you're building an actual robot.

313
00:23:53,620 --> 00:23:57,140
But for a simulator, simulating the real environment and gravity

314
00:23:57,140 --> 00:24:02,100
in those sort of systems can also be difficult to create.

315
00:24:02,100 --> 00:24:05,260
So that you're sure that whatever their reaction

316
00:24:05,260 --> 00:24:07,300
So your program, if they robot, you

317
00:24:07,300 --> 00:24:10,060
need to make sure the environment that it's in

318
00:24:10,060 --> 00:24:12,620
is somewhat real, right?

319
00:24:12,620 --> 00:24:16,260
Well, it depends what you mean by real.

320
00:24:16,260 --> 00:24:19,900
It seems to me if the robot is smart and can learn

321
00:24:19,900 --> 00:24:25,900
and is adaptable and when a piece of its software

322
00:24:25,900 --> 00:24:30,020
doesn't work, then it modifies it.

323
00:24:30,020 --> 00:24:33,860
In other words, if you had a higher level learning machine

324
00:24:33,860 --> 00:24:39,340
in the processor of the robot, then

325
00:24:39,340 --> 00:24:41,140
it could work in a real environment

326
00:24:41,140 --> 00:24:45,420
by learning that if you do this on a slippery floor,

327
00:24:45,420 --> 00:24:49,500
you might, maybe you should lean the other way before you.

328
00:24:52,620 --> 00:24:56,460
Patty Mays did a little bit of that 20 years ago.

329
00:24:57,260 --> 00:24:59,740
With what?

330
00:24:59,740 --> 00:25:02,380
Rule system that defines that moving

331
00:25:02,380 --> 00:25:05,100
and attacks on threshold energy.

332
00:25:05,100 --> 00:25:07,740
Yeah, but you see, if you have a real robot,

333
00:25:07,740 --> 00:25:12,740
you've only got an hour a week to debug this thing.

334
00:25:12,740 --> 00:25:17,220
And the program is working in real time

335
00:25:17,220 --> 00:25:19,980
if you look at real robots.

336
00:25:19,980 --> 00:25:23,580
Whereas if you're simulating a stick figure robot,

337
00:25:23,580 --> 00:25:26,420
then you can simulate friction and slipping.

338
00:25:26,420 --> 00:25:33,500
But you can move it at 100,000 operations per second, maybe.

339
00:25:33,500 --> 00:25:35,620
Another thing is there's something

340
00:25:35,620 --> 00:25:37,580
weird about the robot people.

341
00:25:37,580 --> 00:25:41,420
They tell me there's so much feedback needed

342
00:25:41,420 --> 00:25:43,740
that they need a separate computer for each joint.

343
00:25:47,420 --> 00:25:49,020
I hear this everywhere.

344
00:25:49,020 --> 00:25:51,680
They say, it's distributed.

345
00:25:51,680 --> 00:25:52,740
It's really good.

346
00:25:52,740 --> 00:25:56,780
Of course, it's still falling over.

347
00:25:56,780 --> 00:25:59,140
I agree that there are a lot of things about robots

348
00:25:59,140 --> 00:26:01,460
that can be simulated on a computer.

349
00:26:01,460 --> 00:26:03,180
And I agree that a lot of the things

350
00:26:03,180 --> 00:26:06,180
that people are currently working on with real robots

351
00:26:06,180 --> 00:26:09,220
would better be first done simulated.

352
00:26:09,220 --> 00:26:12,100
But I think that when you have a robot, especially when

353
00:26:12,100 --> 00:26:14,980
you have situations where it interacts with a human being,

354
00:26:14,980 --> 00:26:19,380
the reaction of the human being is sort of really different,

355
00:26:19,380 --> 00:26:22,060
depending on whether it's a simulation or a physical being.

356
00:26:22,060 --> 00:26:24,940
Because I guess human beings inherently

357
00:26:24,940 --> 00:26:27,580
have different reactions to objects

358
00:26:27,580 --> 00:26:29,300
with physical embodiments.

359
00:26:29,300 --> 00:26:31,100
So I feel like at a certain point,

360
00:26:31,100 --> 00:26:33,980
especially when it interfaces with humans, such as caretaker

361
00:26:33,980 --> 00:26:37,700
robots, you do have to switch to the real robots.

362
00:26:37,700 --> 00:26:42,360
Well, the caretaker robot has to be a smart program that's

363
00:26:42,360 --> 00:26:45,820
running a physical robot eventually.

364
00:26:45,820 --> 00:26:49,380
It's just that we don't have the smart programs.

365
00:26:49,380 --> 00:26:52,340
And we could have had them 25 years ago

366
00:26:52,340 --> 00:26:55,340
if the people wanting to make a helper robot

367
00:26:55,340 --> 00:26:58,740
didn't have a physical robot in their lab.

368
00:26:58,740 --> 00:27:01,380
Look at COG, which COG is proudly

369
00:27:01,380 --> 00:27:05,060
displayed in the MIT Museum.

370
00:27:05,060 --> 00:27:12,580
I don't know of a single fact or technique or idea

371
00:27:12,580 --> 00:27:15,980
that came out of that project in the decade

372
00:27:15,980 --> 00:27:20,420
when graduate students wrote bad theses about it.

373
00:27:20,420 --> 00:27:22,260
So I guess then ultimately, it's sort

374
00:27:22,260 --> 00:27:26,620
of like a division of labor problem, where it's like,

375
00:27:26,620 --> 00:27:28,380
because it seems like when people are testing

376
00:27:28,380 --> 00:27:29,980
their code on actual robots, it's

377
00:27:29,980 --> 00:27:32,180
like they have too many problems at the same time.

378
00:27:32,180 --> 00:27:34,540
Whereas you're saying they should focus on the software.

379
00:27:34,540 --> 00:27:36,740
And after the software is all good,

380
00:27:36,740 --> 00:27:39,140
then they can move on to integrating the software

381
00:27:39,140 --> 00:27:41,260
with the hardware.

382
00:27:41,260 --> 00:27:44,500
Yeah, well, the robots in Second Life.

383
00:27:46,380 --> 00:27:48,900
We're pretty good.

384
00:27:48,900 --> 00:27:50,860
I don't know of any people who complained

385
00:27:50,860 --> 00:27:53,900
that they wanted real ones.

386
00:27:56,380 --> 00:27:58,860
I'm not familiar with the robots in Second Life.

387
00:27:58,860 --> 00:28:00,580
What did they do?

388
00:28:00,580 --> 00:28:03,980
Well, you could make one, because there was a language

389
00:28:03,980 --> 00:28:05,140
for it.

390
00:28:05,140 --> 00:28:06,740
I don't think they did anything much.

391
00:28:06,740 --> 00:28:11,180
But they don't fall over.

392
00:28:16,380 --> 00:28:24,540
But I guess a good example might be, well,

393
00:28:24,540 --> 00:28:31,940
Dean Kamen's robot soccer project.

394
00:28:31,940 --> 00:28:39,620
And Carnegie Mellon had a robot soccer project for many years.

395
00:28:39,620 --> 00:28:42,900
And I don't know that any.

396
00:28:42,900 --> 00:28:46,100
I've never heard of any interesting discovery

397
00:28:46,100 --> 00:28:47,220
that came from that.

398
00:28:50,860 --> 00:28:55,900
The Media Lab also, they're the nice looking robots that smile.

399
00:28:59,540 --> 00:29:05,260
And people are impressed by the way

400
00:29:05,260 --> 00:29:14,260
that humans are attracted to interact with these robots.

401
00:29:14,260 --> 00:29:18,780
But I think once we know that, it's

402
00:29:18,780 --> 00:29:22,220
something to avoid rather than to exploit.

403
00:29:22,220 --> 00:29:26,820
Until when the robot says, pleased to meet you,

404
00:29:26,820 --> 00:29:29,540
wouldn't it be nice if it were actually pleased to meet you?

405
00:29:30,500 --> 00:29:33,180
The Intelligent Welcome match would be our target.

406
00:29:36,260 --> 00:29:39,340
Even if it doesn't move, if it just sat there and felt

407
00:29:39,340 --> 00:29:44,380
good about seeing you, that would be a great achievement.

408
00:29:44,380 --> 00:29:45,980
I don't know how you would test for it.

409
00:29:51,900 --> 00:29:53,380
Have you ever played with a Leo dinosaur?

410
00:29:54,380 --> 00:29:56,820
Have you ever played with a Leo dinosaur?

411
00:29:59,940 --> 00:30:02,820
Yes.

412
00:30:02,820 --> 00:30:07,100
And there was a cat that Itachi made.

413
00:30:11,660 --> 00:30:15,420
In fact, we ordered a PLEO, but we never got it.

414
00:30:15,420 --> 00:30:17,260
Are they still in production?

415
00:30:17,260 --> 00:30:18,260
I believe so.

416
00:30:18,260 --> 00:30:20,260
I don't know.

417
00:30:20,260 --> 00:30:22,740
I don't think so.

418
00:30:22,900 --> 00:30:25,100
They seem like they appear to be, yes.

419
00:30:25,100 --> 00:30:25,780
What?

420
00:30:25,780 --> 00:30:27,060
They appear to be.

421
00:30:27,060 --> 00:30:28,860
Yeah, it's a beautiful job.

422
00:30:32,700 --> 00:30:33,940
Somebody made a cat.

423
00:30:33,940 --> 00:30:35,300
I'm trying to remember.

424
00:30:35,300 --> 00:30:37,980
Another Japanese company.

425
00:30:37,980 --> 00:30:41,580
It was an awfully good cat.

426
00:30:41,580 --> 00:30:44,220
It wouldn't do much, but every now and then it would mew

427
00:30:44,220 --> 00:30:49,340
and sort of look at you and beg for something.

428
00:30:49,340 --> 00:30:51,820
But you couldn't resist it.

429
00:30:55,620 --> 00:30:56,500
Yeah?

430
00:30:56,500 --> 00:30:59,740
So from your experience, do you know how long it would take

431
00:30:59,740 --> 00:31:07,140
to simulate a real world, to create a simulation,

432
00:31:07,140 --> 00:31:09,260
a simple simulation of the real world?

433
00:31:09,260 --> 00:31:09,740
Well, what?

434
00:31:09,740 --> 00:31:11,380
And how many people?

435
00:31:11,380 --> 00:31:15,340
Because you're saying that we should simulate robots.

436
00:31:15,340 --> 00:31:17,500
You need a computer to create software for that,

437
00:31:17,500 --> 00:31:18,980
to set up real robots.

438
00:31:18,980 --> 00:31:20,420
And so we need to create the software

439
00:31:20,420 --> 00:31:23,940
that simulates the real world.

440
00:31:23,940 --> 00:31:25,940
But that's complicated for me.

441
00:31:25,940 --> 00:31:28,940
You know, like, you have some ideas,

442
00:31:28,940 --> 00:31:34,420
like how long would it take, how many people,

443
00:31:34,420 --> 00:31:37,900
and why aren't we doing that?

444
00:31:37,900 --> 00:31:40,380
Or why isn't there somebody doing that?

445
00:31:40,380 --> 00:31:52,220
I wish somebody would explain to me why the majority of,

446
00:31:52,220 --> 00:32:02,220
you know, there are epidemics of ideologies.

447
00:32:02,220 --> 00:32:11,100
So that around 1980, the dominant idea

448
00:32:11,100 --> 00:32:16,660
of making machines better was to build rule-based systems.

449
00:32:16,660 --> 00:32:25,940
And that's probably still the most popular way

450
00:32:25,940 --> 00:32:30,060
to program a computer for a complicated commercial

451
00:32:30,060 --> 00:32:33,820
application, like making airplane reservations

452
00:32:33,820 --> 00:32:45,500
and keeping stock of parts in the factory and on and on.

453
00:32:45,500 --> 00:32:48,260
Now, when the first rule-based systems appeared,

454
00:32:48,260 --> 00:32:51,460
they were called production-based languages.

455
00:32:51,460 --> 00:32:56,340
Production is just, I think, the word

456
00:32:56,340 --> 00:33:00,460
comes from Emil Post in the 1920s.

457
00:33:00,460 --> 00:33:14,580
And the idea is you have a set of symbols, x, a, y, b, z,

458
00:33:14,580 --> 00:33:25,460
where x, y, and z are variables, and a and b are constants.

459
00:33:25,460 --> 00:33:29,260
In other words, an algebraic expression, basically.

460
00:33:29,260 --> 00:33:34,020
And if the state of the world or the state of some process

461
00:33:34,020 --> 00:33:40,100
is described by this expression, then you change it

462
00:33:40,100 --> 00:33:45,740
into something else like ax.

463
00:33:48,340 --> 00:33:52,620
So one operation might be, if there's an ace anywhere,

464
00:33:52,620 --> 00:33:55,300
let's move it to the left.

465
00:33:56,020 --> 00:33:59,340
That's just a simple production.

466
00:33:59,340 --> 00:34:03,060
Anyway, if you read my old book on computation,

467
00:34:03,060 --> 00:34:07,340
there's a couple of chapters about production systems.

468
00:34:07,340 --> 00:34:15,900
And those were the first universal logical systems

469
00:34:15,900 --> 00:34:19,220
that could be applied to computers.

470
00:34:19,220 --> 00:34:23,060
So although there were no computers in the 1920s

471
00:34:23,060 --> 00:34:26,780
when Emil Post started working on that,

472
00:34:26,780 --> 00:34:32,980
there had been some discussion back in 1900 or so

473
00:34:32,980 --> 00:34:35,940
when mathematicians like Hilbert, who

474
00:34:35,940 --> 00:34:38,220
were interested both in applied mathematics

475
00:34:38,220 --> 00:34:45,860
and basic logic theories, were trying

476
00:34:45,860 --> 00:34:50,960
to say what kinds of processes can be represented

477
00:34:50,960 --> 00:34:57,640
by what kinds of algebraic or production-like systems.

478
00:34:57,640 --> 00:34:59,920
So there was a lot of research before.

479
00:34:59,920 --> 00:35:03,280
Well, not a lot, but quite a bit of research

480
00:35:03,280 --> 00:35:05,760
before computers even appeared.

481
00:35:05,760 --> 00:35:13,880
And in the 1950s, making rule-based systems

482
00:35:13,880 --> 00:35:18,560
just out of transforming sequences of symbols

483
00:35:18,560 --> 00:35:19,760
became popular.

484
00:35:19,760 --> 00:35:26,960
And that was fine, because those systems are sort of universal.

485
00:35:26,960 --> 00:35:32,140
However, to make a production system

486
00:35:32,140 --> 00:35:34,800
to play a good game of chess, there

487
00:35:34,800 --> 00:35:37,840
might be some point at which just making rules,

488
00:35:37,840 --> 00:35:40,920
saying if there's a pawn here and a queen here

489
00:35:40,920 --> 00:35:44,040
and a bishop here, do this.

490
00:35:44,040 --> 00:35:46,840
Maybe you can't express things that way.

491
00:35:46,840 --> 00:35:50,140
And you might have to have a higher level expression,

492
00:35:50,140 --> 00:35:53,880
such as if two pieces are under attack

493
00:35:53,880 --> 00:35:57,880
at distant parts of the board, then

494
00:35:57,880 --> 00:36:04,560
you should decide to make a move that improves your situation

495
00:36:04,560 --> 00:36:15,480
in one of these situations and blocks

496
00:36:15,480 --> 00:36:19,480
a way of the opponent improving his situation

497
00:36:19,480 --> 00:36:21,400
on the other one.

498
00:36:21,400 --> 00:36:26,380
So you can't express that in terms

499
00:36:26,380 --> 00:36:29,160
of the positions of the pieces.

500
00:36:29,160 --> 00:36:32,000
You have to have a higher level abstraction.

501
00:36:32,000 --> 00:36:34,640
And what happened in the development

502
00:36:34,640 --> 00:36:38,580
of rule-based systems is that you could make rules

503
00:36:38,580 --> 00:36:42,720
about rules, but nobody ever got very good at it.

504
00:36:42,720 --> 00:36:49,880
And you still don't find six-level systems

505
00:36:49,880 --> 00:36:54,840
like the kind proposed in the Society of Mind Book, which

506
00:36:54,840 --> 00:36:59,960
says whatever you do, you want to have multiple levels.

507
00:36:59,960 --> 00:37:02,900
And at the third or fourth level,

508
00:37:02,900 --> 00:37:05,880
the system should have some reflective ability

509
00:37:05,880 --> 00:37:09,560
so that it's keeping track of how successful it's

510
00:37:09,560 --> 00:37:10,840
been at something.

511
00:37:10,840 --> 00:37:13,480
That's a simple kind of learning.

512
00:37:13,480 --> 00:37:18,440
Or it's noticing that abstractions of a certain sort

513
00:37:18,440 --> 00:37:22,200
have been very productive in a certain kind of situation

514
00:37:22,200 --> 00:37:25,040
described by some other abstraction.

515
00:37:25,040 --> 00:37:31,480
Nobody's ever built any programs that go up four or five

516
00:37:31,480 --> 00:37:33,640
levels of that sort.

517
00:37:33,640 --> 00:37:40,560
And I suppose the reason is that nobody, virtually no one,

518
00:37:40,560 --> 00:37:44,640
has a job where they could definitely

519
00:37:44,640 --> 00:37:47,440
plan to spend four or five years.

520
00:37:47,440 --> 00:37:50,720
I think David here said, did you say three or four years

521
00:37:50,720 --> 00:37:53,640
to map out this animal?

522
00:37:53,640 --> 00:37:55,040
That's my hope.

523
00:37:55,040 --> 00:37:57,680
Right, and then you should have a meta-hope

524
00:37:57,680 --> 00:38:00,680
that you'll have such a good student or two

525
00:38:00,680 --> 00:38:02,960
that you'll only have to spend a year of it

526
00:38:02,960 --> 00:38:09,240
in this young Richard Greenblatt or Bill Gosper

527
00:38:10,240 --> 00:38:12,560
or Peter Sampson will come up.

528
00:38:12,560 --> 00:38:15,920
And you'll notice that he's better than you at this.

529
00:38:15,920 --> 00:38:17,880
And that's the best thing.

530
00:38:17,880 --> 00:38:21,360
And then you can quit and go to a higher level.

531
00:38:21,360 --> 00:38:25,280
But there aren't any jobs like that.

532
00:38:28,040 --> 00:38:32,480
We had a lab, sub lab, that split off from the media lab

533
00:38:32,480 --> 00:38:36,840
that I won't mention its name.

534
00:38:36,840 --> 00:38:38,680
But its director got into the habit

535
00:38:38,680 --> 00:38:41,040
of warning a progress report each month.

536
00:38:43,960 --> 00:38:52,840
Well, when we were funded by ARPA,

537
00:38:52,840 --> 00:38:57,000
there was something about a proposal each year.

538
00:38:57,000 --> 00:38:58,840
And guess what our proposal was?

539
00:39:02,960 --> 00:39:06,360
Anybody want to guess?

540
00:39:06,360 --> 00:39:07,560
Intelligent machines?

541
00:39:07,560 --> 00:39:09,120
It was what we had done.

542
00:39:09,120 --> 00:39:11,800
We proposed to do what we had done last year.

543
00:39:16,360 --> 00:39:20,760
Of course, it's nice when the head of the agency

544
00:39:20,760 --> 00:39:24,840
is an old buddy of yours from undergraduate years.

545
00:39:24,840 --> 00:39:26,320
That was Licklider.

546
00:39:26,320 --> 00:39:29,480
But that worked out fine.

547
00:39:29,480 --> 00:39:34,880
And so our proposals were, in fact, progress reports.

548
00:39:34,880 --> 00:39:41,880
And that way, you could do anything you want.

549
00:39:41,880 --> 00:39:44,600
Now, if you had to do that every month,

550
00:39:44,600 --> 00:39:50,120
then those proposals would look so flimsy and sick

551
00:39:50,120 --> 00:39:54,400
that any good executive would say,

552
00:39:54,400 --> 00:39:55,920
you should squash that project.

553
00:39:55,920 --> 00:39:57,440
It's not getting anything done.

554
00:40:01,320 --> 00:40:02,720
Well, I don't know how to fix that.

555
00:40:04,880 --> 00:40:13,360
If we try to deal with the economic collapse

556
00:40:13,360 --> 00:40:20,360
of the civilization that we're in,

557
00:40:20,360 --> 00:40:24,120
it may be politically hard to have five- or 10-year research

558
00:40:24,120 --> 00:40:25,520
projects.

559
00:40:25,520 --> 00:40:32,520
So I have no idea what's going to happen in the next decade.

560
00:40:32,520 --> 00:40:33,020
Yes?

561
00:40:36,520 --> 00:40:38,120
Two questions.

562
00:40:38,120 --> 00:40:40,920
What do you think is the most effective format for people

563
00:40:40,920 --> 00:40:43,120
to make progress in the AI research?

564
00:40:43,120 --> 00:40:45,560
In universities, would it be within corporations,

565
00:40:45,560 --> 00:40:48,160
like what IBM is doing, or what?

566
00:40:48,160 --> 00:40:53,720
And also, who you think should be working on these problems?

567
00:40:53,720 --> 00:41:01,480
Well, the problem, it looks like this,

568
00:41:01,480 --> 00:41:05,000
what do you call the IBM thing?

569
00:41:05,000 --> 00:41:05,800
Watson.

570
00:41:05,800 --> 00:41:07,360
Watson.

571
00:41:07,360 --> 00:41:10,200
That might be a good thing.

572
00:41:10,200 --> 00:41:17,840
It's the first substantial AI project

573
00:41:17,840 --> 00:41:21,720
that is trying to combine more than one method.

574
00:41:21,720 --> 00:41:23,240
But I don't know.

575
00:41:23,240 --> 00:41:28,080
I actually haven't read even what's available about it.

576
00:41:28,080 --> 00:41:32,000
But IBM did have AI researchers for the last.

577
00:41:37,160 --> 00:41:44,360
There was a group in the middle 1950s.

578
00:41:44,360 --> 00:41:48,700
You've all read about this Dartmouth AI conference

579
00:41:48,700 --> 00:41:50,880
in 1954.

580
00:41:50,880 --> 00:41:58,960
And that had McCarthy and me from around here,

581
00:41:58,960 --> 00:42:02,000
and Newell and Simon, who were at Carnegie Mellon.

582
00:42:07,260 --> 00:42:10,760
I think it was called Carnegie Institute of Technology then,

583
00:42:10,760 --> 00:42:12,560
but whatever.

584
00:42:12,560 --> 00:42:17,000
And it had a couple of people from IBM,

585
00:42:17,000 --> 00:42:22,600
and it had about six or seven assorted individuals

586
00:42:22,600 --> 00:42:23,960
from here and there.

587
00:42:23,960 --> 00:42:27,640
So IBM had a guy named Nat Rochester,

588
00:42:27,640 --> 00:42:32,960
who had been involved in designing computers.

589
00:42:32,960 --> 00:42:35,720
And he had a lot of good ideas.

590
00:42:35,720 --> 00:42:41,080
And I had written on paper a geometry theorem proving

591
00:42:41,080 --> 00:42:42,400
program.

592
00:42:42,400 --> 00:42:45,320
And he started a project to actually put that

593
00:42:45,320 --> 00:42:47,600
on a computer.

594
00:42:47,600 --> 00:42:51,760
So for a few years, IBM had a few people working

595
00:42:51,760 --> 00:42:58,760
on different small AI projects.

596
00:42:58,760 --> 00:43:04,440
Then that disappeared after about 10 or 15 years.

597
00:43:04,440 --> 00:43:09,780
And there was essentially no artificial and very little

598
00:43:09,780 --> 00:43:11,640
programming research at IBM.

599
00:43:11,640 --> 00:43:15,120
And they were mostly into hardware and commercial

600
00:43:15,120 --> 00:43:18,240
software and so forth.

601
00:43:18,240 --> 00:43:22,760
This Watson thing is apparently three or four years old,

602
00:43:22,760 --> 00:43:26,080
and it looks like a substantial project.

603
00:43:26,080 --> 00:43:30,360
And I'm a little worried that since they're not

604
00:43:30,360 --> 00:43:34,880
very academic, they might not have the idea that if you've

605
00:43:34,880 --> 00:43:37,520
discovered something, the first thing you should do

606
00:43:37,520 --> 00:43:42,200
is publish all its details so that you get credit for it.

607
00:43:42,200 --> 00:43:47,760
Well, it's hard.

608
00:43:47,760 --> 00:43:50,640
Not very many companies encourage

609
00:43:50,640 --> 00:43:52,440
the individuals getting credit.

610
00:43:52,440 --> 00:43:56,200
And it looks like the guys in that project

611
00:43:56,200 --> 00:44:01,200
are able to publish a substantial amount.

612
00:44:01,200 --> 00:44:10,320
But I think the first good sign will be if they just dump

613
00:44:10,320 --> 00:44:15,040
their code out, too, and let people go over it and copy it

614
00:44:15,040 --> 00:44:18,200
and see if there's anything there they can use.

615
00:44:22,200 --> 00:44:24,800
It's very rare for any AI researcher

616
00:44:24,800 --> 00:44:28,080
to use anyone else's code, as far as I know.

617
00:44:28,080 --> 00:44:29,880
It's almost never happened.

618
00:44:32,200 --> 00:44:33,480
I don't know.

619
00:44:37,080 --> 00:44:42,360
The best example, almost, was Winograd's Shrutlu program,

620
00:44:42,360 --> 00:44:49,040
which was exported to Carnegie Mellon and to Stanford.

621
00:44:49,040 --> 00:44:54,280
And both of them ran that program quite a bit.

622
00:44:54,280 --> 00:44:55,920
But it wasn't.

623
00:44:55,920 --> 00:44:58,280
I don't know why it died.

624
00:44:58,280 --> 00:45:02,800
But apparently, Winograd himself hadn't commented enough

625
00:45:02,800 --> 00:45:04,360
to remember how it worked.

626
00:45:04,360 --> 00:45:11,560
And the beginners never got very far in improving

627
00:45:11,560 --> 00:45:17,000
either its robotic ability, which was non-trivial,

628
00:45:17,000 --> 00:45:20,920
or its linguistic ability, which was unique.

629
00:45:20,920 --> 00:45:27,040
It had better parsers and better semantic operations,

630
00:45:27,040 --> 00:45:30,480
if only for a limited blocks world,

631
00:45:30,480 --> 00:45:34,720
put the big red block on top of the little green block.

632
00:45:34,720 --> 00:45:38,080
And it would do things like that.

633
00:45:41,160 --> 00:45:43,200
And you could say, put the block which

634
00:45:43,200 --> 00:45:49,520
is on top of the big red block and do such and such with it.

635
00:45:49,520 --> 00:45:53,200
So it had fairly complicated ways

636
00:45:53,200 --> 00:45:56,800
to refer to and describe things.

637
00:45:59,760 --> 00:46:05,240
But it became a showpiece where you could demonstrate it.

638
00:46:05,240 --> 00:46:09,200
And after a few years, you couldn't even.

639
00:46:09,200 --> 00:46:10,120
Has anyone tried it?

640
00:46:14,280 --> 00:46:15,400
I haven't.

641
00:46:15,400 --> 00:46:18,760
I've looked into it, but it's immaculate.

642
00:46:18,760 --> 00:46:22,280
So it's not even, there isn't even a path to run it.

643
00:46:22,280 --> 00:46:24,960
So there's no interpreter to run it.

644
00:46:24,960 --> 00:46:28,880
It should have had the whole package, of course.

645
00:46:28,880 --> 00:46:32,360
Because that program was big, and MACLIS

646
00:46:32,360 --> 00:46:35,920
would have only been 2% of the whole thing.

647
00:46:40,480 --> 00:46:42,840
Winograd had quite a number of graduate students

648
00:46:42,840 --> 00:46:46,800
who finally left and did something else,

649
00:46:46,800 --> 00:46:51,400
because the project was, they just

650
00:46:51,400 --> 00:46:53,600
found it too hard to understand the code.

651
00:47:04,080 --> 00:47:07,920
But who has an idea of what to do next?

652
00:47:07,920 --> 00:47:14,440
I bet you all have some fantasy of how to make machines smarter.

653
00:47:16,960 --> 00:47:32,760
You could wait for the human brain people to, yes?

654
00:47:32,760 --> 00:47:35,440
I was thinking you could get more funding for science fiction

655
00:47:35,440 --> 00:47:38,320
writers, and then hopefully one of them

656
00:47:38,320 --> 00:47:39,880
will come up with a brilliant idea.

657
00:47:43,680 --> 00:47:45,360
Yes?

658
00:47:45,360 --> 00:47:48,240
Greg Egan had a computer.

659
00:47:48,240 --> 00:47:52,440
How many of you know the name of Greg Egan?

660
00:47:52,440 --> 00:47:55,640
Look at his web page.

661
00:47:55,640 --> 00:47:59,240
It has more exciting mathematical demonstrations

662
00:47:59,240 --> 00:48:01,640
than his.

663
00:48:01,640 --> 00:48:06,600
As far as I know, I don't know anything like it.

664
00:48:06,600 --> 00:48:12,160
Well, yes, Ken Perlin.

665
00:48:15,040 --> 00:48:20,240
Type Ken Perlin into your Google and get his web page.

666
00:48:20,240 --> 00:48:24,640
And he has, I think he must invent two or three

667
00:48:24,640 --> 00:48:28,880
incredibly brilliant small programs every week,

668
00:48:28,880 --> 00:48:31,360
because there are quite a few hundred

669
00:48:31,360 --> 00:48:37,240
of these gorgeous demonstrations of one principle after another.

670
00:48:50,400 --> 00:48:51,040
Oh, dear.

671
00:48:53,680 --> 00:48:54,680
I didn't look ahead.

672
00:49:01,360 --> 00:49:14,680
Another feature of Greg Egan is that he read The Society of Mind

673
00:49:14,680 --> 00:49:18,240
and worked a lot of ideas that I hadn't thought

674
00:49:18,240 --> 00:49:20,200
of into some of his stories.

675
00:49:21,200 --> 00:49:33,760
So but another feature of him is that no one has ever met him.

676
00:49:38,560 --> 00:49:41,680
That is, if you knock on his door, which is in Perth,

677
00:49:41,680 --> 00:49:44,360
it won't answer.

678
00:49:44,360 --> 00:49:50,040
And I was discussing this with Greg Benford

679
00:49:50,960 --> 00:49:53,400
and apparently there are a couple of people who have seen him,

680
00:49:53,400 --> 00:49:56,680
but nobody knows anyone who knows him.

681
00:49:59,600 --> 00:50:02,080
You could do that.

682
00:50:02,080 --> 00:50:03,920
Maybe that's the way to get a lot done.

683
00:50:07,440 --> 00:50:10,080
Make a list of all your friends and abandon them.

684
00:50:13,720 --> 00:50:15,660
Just think of all the free time you'll have.

685
00:50:20,720 --> 00:50:24,800
Is there any other Gregs?

686
00:50:24,800 --> 00:50:25,960
Oh, well, Greg Benford.

687
00:50:32,640 --> 00:50:33,680
But this isn't fair.

688
00:50:33,680 --> 00:50:35,000
I've lost the big chalk.

689
00:50:35,000 --> 00:50:50,640
Among the science fiction writers,

690
00:50:50,640 --> 00:50:53,280
there are only a few who actually know a lot of science.

691
00:50:56,760 --> 00:51:02,440
Benford is quite a good physicist and knows a lot

692
00:51:02,440 --> 00:51:07,680
and publishes serious physics papers.

693
00:51:07,680 --> 00:51:11,360
Larry Niven knew quite a bit of science

694
00:51:11,360 --> 00:51:16,080
but was never a practicing one.

695
00:51:16,080 --> 00:51:18,600
So in his science fiction, there are

696
00:51:18,600 --> 00:51:24,160
occasional technical errors, like in Ringworld.

697
00:51:24,160 --> 00:51:32,160
Ringworld was a big ring-shaped planet around the sun.

698
00:51:32,160 --> 00:51:39,840
And so here's the sun and here's this ring.

699
00:51:43,040 --> 00:51:46,640
And the ring is about 8,000 miles wide

700
00:51:46,640 --> 00:51:50,920
and goes all the way around its sun in something

701
00:51:50,920 --> 00:51:54,240
like Earth's orbit, which means that it

702
00:51:54,240 --> 00:52:01,880
has an area of the order of 100,000 Earths.

703
00:52:01,880 --> 00:52:05,280
Well, Larry didn't realize that there was a bug.

704
00:52:05,280 --> 00:52:11,760
And if you make a ring that's rotating around the sun,

705
00:52:11,760 --> 00:52:14,320
well, first of all, there's no particular reason

706
00:52:14,320 --> 00:52:17,000
why it should rotate.

707
00:52:17,000 --> 00:52:19,800
But if it didn't, it would squash.

708
00:52:22,840 --> 00:52:25,440
But it's not gravitationally stable.

709
00:52:25,440 --> 00:52:29,240
So as soon as it gets slightly off-center,

710
00:52:29,240 --> 00:52:34,880
then this part, if this is closer, it gets pulled in.

711
00:52:34,880 --> 00:52:37,440
And so finally, one part of it falls into the sun

712
00:52:37,440 --> 00:52:39,880
and the rest follows it.

713
00:52:39,880 --> 00:52:43,880
So in the sequel to Ringworld, some physicists

714
00:52:43,880 --> 00:52:46,620
pointed this out to Larry Niven.

715
00:52:46,620 --> 00:52:56,940
And so now Larry Niven had little wires.

716
00:53:00,140 --> 00:53:01,580
I forget what they're connected to.

717
00:53:04,580 --> 00:53:06,820
They're made of something called scryth,

718
00:53:06,820 --> 00:53:12,180
which is 10 to the 50th time stronger than anything real

719
00:53:12,180 --> 00:53:16,580
because it's pretty hard to hold a thing that massive.

720
00:53:17,580 --> 00:53:20,660
Oh, then they're little jets, which

721
00:53:20,660 --> 00:53:26,460
have been built by the original inhabitants.

722
00:53:26,460 --> 00:53:29,700
I forgot all the geometry.

723
00:53:29,700 --> 00:53:36,420
So now there's an active system for maintaining the Ringworld

724
00:53:36,420 --> 00:53:38,700
to not fall into the sun.

725
00:53:38,700 --> 00:53:44,660
And the sequel, this was built by some disappeared aliens

726
00:53:44,660 --> 00:53:46,540
millions of years ago.

727
00:53:46,540 --> 00:53:49,340
And the hardware is beginning to break down.

728
00:53:49,340 --> 00:53:53,060
So they need to find an alien race with enough engineering

729
00:53:53,060 --> 00:53:56,260
to fix it and blah, blah.

730
00:53:56,260 --> 00:54:02,440
Anyway, Larry Niven is awfully good at making up things

731
00:54:02,440 --> 00:54:05,700
with a pretty good scientific background.

732
00:54:05,700 --> 00:54:08,300
And then when he makes a mistake,

733
00:54:08,300 --> 00:54:10,180
he writes a sequel to correct it.

734
00:54:10,180 --> 00:54:15,060
So maybe that's actually better than getting it

735
00:54:15,060 --> 00:54:16,180
right the first time.

736
00:54:21,980 --> 00:54:25,820
He has a machine, the transporters in Star Trek.

737
00:54:28,820 --> 00:54:32,220
Niven's transporters are nice because they take out

738
00:54:32,220 --> 00:54:37,260
all the chemicals in your cells that shouldn't be there.

739
00:54:37,260 --> 00:54:41,880
So once they have transporters, automatically the people

740
00:54:41,880 --> 00:54:44,120
now become nearly immortal.

741
00:54:44,120 --> 00:54:47,560
Because if you're being scanned and reconstructed

742
00:54:47,560 --> 00:54:51,080
by a transporter, it might as well fix all your bugs

743
00:54:51,080 --> 00:54:51,840
while it's at it.

744
00:54:56,300 --> 00:55:02,400
Anyway, there's a good idea on every page of this guy,

745
00:55:02,400 --> 00:55:04,840
even if it's wrong.

746
00:55:04,840 --> 00:55:11,840
So when I read a regular novel, which is very rare,

747
00:55:11,840 --> 00:55:15,840
there's usually a good idea every chapter maybe.

748
00:55:15,840 --> 00:55:22,320
But I didn't write Larry Niven's name.

749
00:55:22,320 --> 00:55:25,560
Probably he had more ideas than any other writer,

750
00:55:25,560 --> 00:55:27,600
but they're not all correct.

751
00:55:41,840 --> 00:55:49,440
So how many of you are going to pursue AI,

752
00:55:49,440 --> 00:55:57,120
or are you going to work on trying to make machines smarter?

753
00:56:00,080 --> 00:56:00,680
Here's one.

754
00:56:04,400 --> 00:56:07,880
We sure could use them.

755
00:56:07,880 --> 00:56:15,280
I'm sort of amused at the there are

756
00:56:15,280 --> 00:56:17,640
a couple of transhumanist communities,

757
00:56:17,640 --> 00:56:19,400
whatever that means.

758
00:56:19,400 --> 00:56:22,640
And they're worried that the AI will,

759
00:56:22,640 --> 00:56:27,080
as many science fiction writers have, be hostile and take over

760
00:56:27,080 --> 00:56:30,920
and want everything for itself.

761
00:56:30,920 --> 00:56:34,160
And so there are serious discussions

762
00:56:34,160 --> 00:56:37,040
on how to make friendly AIs.

763
00:56:37,040 --> 00:56:38,480
And of course, the master of that

764
00:56:38,480 --> 00:56:44,280
was Isaac Asimov, who said that a robot should always

765
00:56:44,280 --> 00:56:50,240
put its interests beneath those of its owner or whatever.

766
00:56:50,240 --> 00:56:54,440
How many of you know Asimov's laws?

767
00:56:54,440 --> 00:56:55,840
And then there was the fourth law

768
00:56:55,840 --> 00:56:59,480
that he had to make, which is that it's

769
00:56:59,480 --> 00:57:04,120
all right to harm a human if it's

770
00:57:04,120 --> 00:57:08,720
to save a large number of others from being harmed.

771
00:57:16,960 --> 00:57:25,120
But it's like saying we shouldn't eat meat.

772
00:57:25,120 --> 00:57:28,640
It's very hard to make up ethical principles that

773
00:57:28,640 --> 00:57:31,920
are foolproof.

774
00:57:31,920 --> 00:57:37,040
I'm not sure what the safe AI people

775
00:57:37,040 --> 00:57:38,880
have as their Asimov laws.

776
00:57:50,720 --> 00:57:55,040
But if you walk around the AI world today,

777
00:57:55,040 --> 00:57:58,000
it's hard to take seriously the idea

778
00:57:58,000 --> 00:58:00,720
that they might form some sort of threat.

779
00:58:00,720 --> 00:58:14,760
Of course, it could happen overnight, but, well,

780
00:58:14,760 --> 00:58:17,920
we discussed the big threat the other day,

781
00:58:17,920 --> 00:58:20,360
somebody making a virus that kills everything.

782
00:58:23,160 --> 00:58:25,680
That's the road toward nanotechnology.

783
00:58:31,720 --> 00:58:34,200
I can't get them to propose anything.

784
00:58:36,880 --> 00:58:38,480
What happens in your class, Pat?

785
00:58:38,480 --> 00:58:39,960
I haven't been there for a long time.

786
00:58:40,960 --> 00:58:43,960
Oh, I don't know.

787
00:58:59,960 --> 00:59:00,960
Yes.

788
00:59:00,960 --> 00:59:03,960
Do you think efforts to augment human intelligence

789
00:59:03,960 --> 00:59:06,960
are going to stand?

790
00:59:06,960 --> 00:59:10,960
Rather than finding a machine that can tell us what's out,

791
00:59:10,960 --> 00:59:13,960
given all the ways that we can make people more intelligent

792
00:59:13,960 --> 00:59:16,160
and more healthy?

793
00:59:16,160 --> 00:59:22,680
That's a great question, because we're probably

794
00:59:22,680 --> 00:59:27,160
fairly close to the technology to do something

795
00:59:27,160 --> 00:59:31,040
exciting in that direction, except that we're afraid to.

796
00:59:31,040 --> 00:59:35,320
Because it seems to me that suppose you just wanted

797
00:59:35,320 --> 00:59:42,480
to do something modest, like improve some person's memory,

798
00:59:42,480 --> 00:59:49,920
I'll bet that with a few million dollars and I don't know

799
00:59:49,920 --> 00:59:53,640
how many years, you could do something

800
00:59:53,640 --> 00:59:57,000
without understanding how the brain works

801
00:59:57,000 --> 01:00:01,760
to attach accessories to it.

802
01:00:01,760 --> 01:00:05,440
I think I mentioned Brindley's first experiment

803
01:00:05,440 --> 01:00:13,840
on trying to restore vision to a blind person.

804
01:00:17,760 --> 01:00:19,640
I think I mentioned that last time.

805
01:00:32,080 --> 01:00:33,240
L-E-Y, I think.

806
01:00:37,960 --> 01:00:49,040
So what he did was here's his secretary, who's blind.

807
01:00:49,040 --> 01:00:57,680
And here's her occipital lobe of the brain.

808
01:00:57,680 --> 01:01:04,760
And here's this area, which is the primary visual.

809
01:01:04,760 --> 01:01:10,080
I think it's called area 17 for no good reason.

810
01:01:10,080 --> 01:01:13,580
That's the place where the signals get

811
01:01:13,580 --> 01:01:17,480
into the brain from the eye.

812
01:01:17,480 --> 01:01:20,480
Stupid, isn't it?

813
01:01:20,480 --> 01:01:23,580
Here's the eye, and here's the optic nerve.

814
01:01:23,580 --> 01:01:26,360
And then here's some other stuff.

815
01:01:26,360 --> 01:01:30,520
And it's all the way back here.

816
01:01:30,520 --> 01:01:32,320
I presume this is because we used

817
01:01:32,320 --> 01:01:36,800
to be an octopus or something, and the eye was here.

818
01:01:36,800 --> 01:01:44,880
But anyway, what's more, the wires cross

819
01:01:44,880 --> 01:01:46,280
and all sorts of things.

820
01:01:46,280 --> 01:01:49,360
But anyway, so here's this piece of brain.

821
01:01:49,360 --> 01:01:53,280
And what Brindley does is he makes a little gadget

822
01:01:53,280 --> 01:01:56,840
with 64 electrodes.

823
01:02:02,640 --> 01:02:05,280
And I've forgotten the details, but I

824
01:02:05,280 --> 01:02:11,240
think that these actually has little coils.

825
01:02:11,240 --> 01:02:13,520
I believe that the thing was there's

826
01:02:13,520 --> 01:02:16,920
no wire coming out of her head.

827
01:02:16,920 --> 01:02:22,640
But these little wires, little coils, and electrodes

828
01:02:23,040 --> 01:02:28,200
are in the 1950s.

829
01:02:28,200 --> 01:02:34,640
And silicone had just appeared, and it looked pretty harmless.

830
01:02:34,640 --> 01:02:37,480
So this is a little pad of silicone.

831
01:02:37,480 --> 01:02:40,600
He has to take the bone off the back of her skull,

832
01:02:40,600 --> 01:02:43,760
which is risky.

833
01:02:43,760 --> 01:02:45,960
So there's the gadget.

834
01:02:45,960 --> 01:02:53,360
And as I said, when you stimulate these,

835
01:02:53,360 --> 01:02:59,520
I believe 48 of them worked, and about eight of them hurt.

836
01:02:59,520 --> 01:03:03,640
And about eight of them didn't do anything.

837
01:03:03,640 --> 01:03:06,840
But if you fired up this one, then she

838
01:03:06,840 --> 01:03:12,720
described it as being like half a matchstick at arm's length.

839
01:03:12,720 --> 01:03:15,880
And they were little bars, which is interesting.

840
01:03:15,880 --> 01:03:18,840
They were not little round dots.

841
01:03:18,840 --> 01:03:21,200
But there were enough of them that she could read letters

842
01:03:21,200 --> 01:03:23,040
one at a time.

843
01:03:23,040 --> 01:03:26,200
And so that's the 1950s.

844
01:03:26,200 --> 01:03:30,040
And as far as I know, it's never been done again.

845
01:03:30,040 --> 01:03:34,360
Maybe there's some way to look that up.

846
01:03:34,360 --> 01:03:34,860
Yes?

847
01:03:34,860 --> 01:03:39,720
So they can improve the memory of rats recently.

848
01:03:39,720 --> 01:03:40,520
Of rats?

849
01:03:40,520 --> 01:03:43,280
Yeah, and they can turn off and turn on.

850
01:03:43,280 --> 01:03:44,880
OK.

851
01:03:44,880 --> 01:03:46,640
Yeah.

852
01:03:46,640 --> 01:03:54,320
But anyway, I suspect that if you got permission to try,

853
01:03:54,320 --> 01:04:00,120
I'll bet you could put 1,000 electrodes maybe

854
01:04:00,120 --> 01:04:04,800
in a couple of places on the cortex and just poke around.

855
01:04:04,800 --> 01:04:10,800
And I'll bet that you could do it without much danger

856
01:04:10,800 --> 01:04:15,800
and eventually get enough inputs and outputs

857
01:04:15,800 --> 01:04:17,560
to hook it up to something stupid

858
01:04:17,560 --> 01:04:20,760
like the Oxford Dictionary, which

859
01:04:20,760 --> 01:04:30,440
would be about that big, or make it a memory retrieval system

860
01:04:30,440 --> 01:04:34,600
where you think, I don't know what you'd think,

861
01:04:34,600 --> 01:04:41,240
but people having this gadget in their head

862
01:04:41,240 --> 01:04:45,520
for a long time with a screen producing outputs

863
01:04:45,520 --> 01:04:48,880
would learn how to control some of those little spots.

864
01:04:48,880 --> 01:04:52,200
And then another part of their brain

865
01:04:52,200 --> 01:04:55,520
would learn to control groups of them and so forth.

866
01:04:55,520 --> 01:04:59,720
And maybe after a few days or weeks or months,

867
01:04:59,760 --> 01:05:04,800
you'd be able to operate this as like a touch

868
01:05:04,800 --> 01:05:07,760
screen on your iPhone.

869
01:05:07,760 --> 01:05:15,360
And it would provide you with a huge, inexhaustible amount

870
01:05:15,360 --> 01:05:16,040
of memory.

871
01:05:18,720 --> 01:05:21,600
And for one in a dozen people, it

872
01:05:21,600 --> 01:05:24,980
might produce outputs that get right

873
01:05:24,980 --> 01:05:31,220
into your mind in some graphic or auditory or unspeakably

874
01:05:31,220 --> 01:05:33,380
alien sense.

875
01:05:33,380 --> 01:05:37,780
And you'd know something that you didn't know before.

876
01:05:37,780 --> 01:05:41,260
So that would be a very thrilling area.

877
01:05:41,260 --> 01:05:43,100
And there are lots of epileptics who

878
01:05:43,100 --> 01:05:45,940
are having pieces of their brain removed

879
01:05:45,940 --> 01:05:50,320
because it's saving their life.

880
01:05:50,320 --> 01:05:55,720
And it would be almost risk-free to slide this little gadget

881
01:05:55,720 --> 01:06:00,360
in while you're at it because now it's 60 years later.

882
01:06:00,360 --> 01:06:03,600
And we know which kinds of silicone implants

883
01:06:03,600 --> 01:06:04,560
are tolerated.

884
01:06:04,560 --> 01:06:07,360
And I'm sure there are other things.

885
01:06:07,360 --> 01:06:11,760
What substances can you put in the body that don't irritate it?

886
01:06:11,760 --> 01:06:12,760
Must be quite a few.

887
01:06:15,240 --> 01:06:17,640
I mean, I think we're sort of already doing this like

888
01:06:17,640 --> 01:06:21,600
with our computers or with our smartphones,

889
01:06:21,600 --> 01:06:24,000
except for the interface between us

890
01:06:24,000 --> 01:06:26,880
and the devices is a lot funkier.

891
01:06:26,880 --> 01:06:28,200
This is just more direct.

892
01:06:28,200 --> 01:06:32,400
Whereas right now, it's like we have to, I guess,

893
01:06:32,400 --> 01:06:35,160
use our bodies to actuate the machine

894
01:06:35,160 --> 01:06:37,840
and then use our senses to get the information back

895
01:06:37,840 --> 01:06:39,480
into our mind.

896
01:06:39,480 --> 01:06:41,600
And we're being very sluggish about it.

897
01:06:41,600 --> 01:06:44,480
I don't see why you couldn't put little pads

898
01:06:44,480 --> 01:06:49,040
in the toes of your shoes so you wouldn't need a mouse.

899
01:06:49,040 --> 01:06:51,760
Or they have like tongue interfaces where like

900
01:06:51,760 --> 01:06:52,640
Well, yes.

901
01:06:52,640 --> 01:06:56,280
The tongue displays where it sort of sends

902
01:06:56,280 --> 01:06:59,720
like electric signals on your tongue in a 2D grid

903
01:06:59,720 --> 01:07:03,560
so that you can like, I guess, feel like some shape.

904
01:07:03,560 --> 01:07:06,600
And there's also, I saw this talk last week

905
01:07:06,600 --> 01:07:11,400
of Microsoft Research working on this tongue controlling

906
01:07:11,400 --> 01:07:13,720
interface where you can use, no, seriously,

907
01:07:14,120 --> 01:07:15,760
they gave this to disabled people

908
01:07:15,760 --> 01:07:18,160
and put it in this wheelchair and they can use their tongue

909
01:07:18,160 --> 01:07:19,200
to control the wheelchair.

910
01:07:19,200 --> 01:07:21,160
And it's almost like they're mind controlling it.

911
01:07:21,160 --> 01:07:22,000
Yes, right.

912
01:07:24,840 --> 01:07:27,360
I think there's actually something that does it

913
01:07:27,360 --> 01:07:30,680
with scanning the activity in the brain.

914
01:07:30,680 --> 01:07:34,520
So I saw this head talk a lot back.

915
01:07:34,520 --> 01:07:35,600
On the emotive device?

916
01:07:35,600 --> 01:07:37,960
I actually tried it in the stuff I created.

917
01:07:37,960 --> 01:07:39,760
Oh.

918
01:07:39,760 --> 01:07:42,600
They think of that right and they can control

919
01:07:42,840 --> 01:07:45,680
the way, I don't know what it's like.

920
01:07:45,680 --> 01:07:48,320
Yeah, so I think it works.

921
01:07:48,320 --> 01:07:50,480
I did an internship at a video game company

922
01:07:50,480 --> 01:07:53,600
and used that exact device to make video games.

923
01:07:53,600 --> 01:07:56,200
And you have to stream it, except for it's sort of

924
01:07:56,200 --> 01:07:59,000
an engine because basically the way it works is like

925
01:07:59,000 --> 01:08:02,040
you have this streaming interface where they tell you

926
01:08:02,040 --> 01:08:05,720
to like push or pull and like it associates

927
01:08:05,720 --> 01:08:07,800
like your certain like thought patterns

928
01:08:07,800 --> 01:08:09,880
with like the push or with the pull.

929
01:08:09,880 --> 01:08:11,800
But I don't know, it's like,

930
01:08:12,680 --> 01:08:15,040
maybe I just didn't practice it enough.

931
01:08:15,040 --> 01:08:18,560
It's not that reliable.

932
01:08:18,560 --> 01:08:22,160
So it's like for a video game or for like toy applications,

933
01:08:22,160 --> 01:08:26,360
it's okay if you actually use that to like control,

934
01:08:26,360 --> 01:08:29,760
I don't know, like an Evangelion style giant robot.

935
01:08:35,280 --> 01:08:36,200
This is an old thing.

936
01:08:36,200 --> 01:08:38,320
I don't know if there are any better ones.

937
01:08:38,320 --> 01:08:43,080
Yes, the people with the tongue gadget

938
01:08:43,080 --> 01:08:45,400
claim very good results.

939
01:08:45,400 --> 01:08:47,040
I don't know what it's like to go around

940
01:08:47,040 --> 01:08:48,760
with that thing in your mouth, but.

941
01:08:50,200 --> 01:08:51,400
They made a wireless one.

942
01:08:51,400 --> 01:08:53,520
It's like in the shape of a retainer.

943
01:08:53,520 --> 01:08:56,640
Oh, okay, is it anchored to the teeth or something?

944
01:08:57,880 --> 01:09:00,040
It's basically like a retainer with, you know,

945
01:09:00,040 --> 01:09:02,760
wires to hold it on your teeth.

946
01:09:02,760 --> 01:09:04,320
It sticks all the way through your mouth.

947
01:09:04,320 --> 01:09:07,640
Oh, so you just, that doesn't sound so bad.

948
01:09:07,640 --> 01:09:08,760
Is it wireless?

949
01:09:08,760 --> 01:09:09,520
That would be.

950
01:09:09,520 --> 01:09:11,320
Yeah, yeah.

951
01:09:11,320 --> 01:09:14,840
There was a talk by Desme Tan from Microsoft Research

952
01:09:14,840 --> 01:09:17,480
and he showed stuff about that.

953
01:09:17,480 --> 01:09:23,480
This guy, I think it's spelled right, but I'm not sure,

954
01:09:23,480 --> 01:09:25,840
is a professor at Stanford.

955
01:09:25,840 --> 01:09:29,800
And he made an array of vibrators

956
01:09:29,800 --> 01:09:31,920
that go on your back.

957
01:09:31,920 --> 01:09:35,720
And I put it on once.

958
01:09:35,720 --> 01:09:37,680
It's 20 by 20, I think.

959
01:09:44,880 --> 01:09:51,280
And they all work, so it's better than the 8 by 8 thing.

960
01:09:51,280 --> 01:09:54,920
And I was able to recognize a telephone.

961
01:09:54,920 --> 01:09:59,880
It's connected to a video camera and it's just

962
01:09:59,880 --> 01:10:01,720
a pad on your back.

963
01:10:01,720 --> 01:10:08,560
And if you have a TV camera with the right kind of contrast

964
01:10:08,560 --> 01:10:15,560
and you can find the door and stuff like that, it's easy.

965
01:10:15,560 --> 01:10:17,600
And I recognized a couple of objects.

966
01:10:17,600 --> 01:10:20,040
I was only in it for about five minutes,

967
01:10:20,040 --> 01:10:26,240
but the interesting thing is that it didn't take any time

968
01:10:26,240 --> 01:10:33,640
to feel this input as a shape.

969
01:10:33,640 --> 01:10:34,880
I was pretty surprised.

970
01:10:37,880 --> 01:10:40,440
That was many years ago.

971
01:10:40,440 --> 01:10:43,400
I don't know what's happened since.

972
01:10:48,240 --> 01:10:50,160
There are reading devices for the blind

973
01:10:50,160 --> 01:10:53,360
that go on your finger.

974
01:10:53,440 --> 01:10:57,760
So instead of Braille, you go like this

975
01:10:57,760 --> 01:11:01,720
and it can make a sound or it can make

976
01:11:01,720 --> 01:11:04,440
a vibration in your fingertip.

977
01:11:04,440 --> 01:11:08,760
So the trouble with Braille is you have to have Braille.

978
01:11:08,760 --> 01:11:16,000
But this device, I can't remember who makes it.

979
01:11:16,000 --> 01:11:18,400
You just put this little thimble on

980
01:11:18,400 --> 01:11:23,040
and it has tiny vibrators so that it's like Braille.

981
01:11:23,040 --> 01:11:29,040
And you can feel the letters go by as you move your finger.

982
01:11:29,040 --> 01:11:32,880
And I've forgotten the professor's name for this.

983
01:11:32,880 --> 01:11:35,200
Does that ring a bell?

984
01:11:35,200 --> 01:11:39,120
Anyway, only about 10% of blind people

985
01:11:39,120 --> 01:11:44,240
find it easy to learn to read this way.

986
01:11:44,240 --> 01:11:50,560
So it's not widely in production.

987
01:11:53,760 --> 01:11:57,720
But that's all beside the point.

988
01:11:57,720 --> 01:12:05,600
I think at some point in some country

989
01:12:05,600 --> 01:12:13,960
we'll want to try this direct brain connection thing.

990
01:12:13,960 --> 01:12:15,720
Might be great.

991
01:12:15,720 --> 01:12:19,200
Might be really good for an older person who's

992
01:12:19,200 --> 01:12:29,000
become blind and a senior person whose lost vision

993
01:12:29,000 --> 01:12:32,040
might not be able to soup up their hearing

994
01:12:32,040 --> 01:12:37,640
and other senses well enough or learn fast enough

995
01:12:37,640 --> 01:12:40,800
to get around, so being.

996
01:12:44,360 --> 01:12:48,480
If you become blind deaf like Helen Keller

997
01:12:48,480 --> 01:12:49,800
when you're a few months old.

998
01:12:49,800 --> 01:12:51,200
Or was she three years old?

999
01:12:51,200 --> 01:12:52,120
Anybody remember?

1000
01:12:57,880 --> 01:13:00,800
So she just adapted and became quite a good writer.

1001
01:13:05,800 --> 01:13:08,600
When thinking about augmenting human intelligence,

1002
01:13:08,600 --> 01:13:12,400
there are really two big rounds to think about.

1003
01:13:12,400 --> 01:13:16,120
One round is developing the external device

1004
01:13:16,120 --> 01:13:19,200
that does the thinking for you that retrieves information.

1005
01:13:19,200 --> 01:13:21,160
And then the other round is the interface

1006
01:13:21,160 --> 01:13:23,360
between the external being and the human being.

1007
01:13:23,360 --> 01:13:25,240
Yes, what's the representations?

1008
01:13:25,240 --> 01:13:30,960
And it'd be wonderful to have a gadget with a bunch of brain

1009
01:13:30,960 --> 01:13:36,080
electrodes so you could discover what kinds of symbols

1010
01:13:36,080 --> 01:13:38,540
can people learn to distinguish.

1011
01:13:38,540 --> 01:13:41,840
And we'd probably learn more about the brain

1012
01:13:42,440 --> 01:13:47,760
a couple of years than in the previous century

1013
01:13:47,760 --> 01:13:52,000
because we could get some idea of representations.

1014
01:13:52,000 --> 01:13:56,000
Do people represent things as list structures

1015
01:13:56,000 --> 01:14:00,080
or as productions or computer science

1016
01:14:00,080 --> 01:14:03,700
has come up with lots of ways to represent knowledge?

1017
01:14:03,700 --> 01:14:07,400
I've never met a neuroscientist who even knows

1018
01:14:07,400 --> 01:14:13,120
what your smirk.

1019
01:14:13,120 --> 01:14:15,360
Maybe there are some I haven't met.

1020
01:14:15,360 --> 01:14:16,000
I'm sure.

1021
01:14:16,000 --> 01:14:17,200
Well, there's me.

1022
01:14:17,200 --> 01:14:18,040
That's why I smirk.

1023
01:14:21,280 --> 01:14:25,680
But I've complained about this about four times already.

1024
01:14:25,680 --> 01:14:31,000
But the best example is to look at the review of the emotion

1025
01:14:31,000 --> 01:14:36,400
machine on the Amazon book page for it

1026
01:14:36,400 --> 01:14:40,420
because there's this neurologist who says,

1027
01:14:40,420 --> 01:14:45,360
he writes all about these K lines and panologies

1028
01:14:45,360 --> 01:14:47,600
and isonomes.

1029
01:14:47,600 --> 01:14:49,520
And he has no evidence they exist.

1030
01:14:52,440 --> 01:14:56,460
And it's just so funny because I'm writing this book

1031
01:14:56,460 --> 01:14:58,520
to help those.

1032
01:14:58,520 --> 01:15:00,840
Here they're drowning in ignorance.

1033
01:15:00,840 --> 01:15:04,000
And I've just taught them this life raft.

1034
01:15:04,000 --> 01:15:07,360
And they say, well, he hasn't tested this life raft

1035
01:15:07,360 --> 01:15:10,880
to see if it exists.

1036
01:15:10,880 --> 01:15:13,560
They don't like the idea of a suggestion

1037
01:15:13,560 --> 01:15:16,760
if you haven't proved it.

1038
01:15:16,760 --> 01:15:20,760
That is not the way to do science.

1039
01:15:20,760 --> 01:15:23,200
You have to make hypotheses.

1040
01:15:23,200 --> 01:15:26,680
And if you're a neuroscientist, you

1041
01:15:26,680 --> 01:15:30,920
can't do that because it's not in your nature.

1042
01:15:30,920 --> 01:15:33,240
So you have to get them from someone else.

1043
01:15:33,320 --> 01:15:34,000
Very simple.

1044
01:15:40,640 --> 01:15:43,360
How come there are no programming languages

1045
01:15:43,360 --> 01:15:47,880
that have decent representations of anything?

1046
01:15:47,880 --> 01:15:51,280
People tell me they use C++, whatever that is.

1047
01:15:54,480 --> 01:15:58,240
I think Winston wrote a book on how to program in that.

1048
01:15:58,240 --> 01:16:00,640
And I asked him, do you actually know

1049
01:16:00,640 --> 01:16:03,040
how to program in that language?

1050
01:16:03,040 --> 01:16:04,560
He said, of course not.

1051
01:16:04,560 --> 01:16:06,840
But I know how to write books on how to program.

1052
01:16:09,640 --> 01:16:10,960
Do you remember that?

1053
01:16:10,960 --> 01:16:13,240
It's a great learning experience.

1054
01:16:13,240 --> 01:16:18,160
You actually said something like that because you write the book

1055
01:16:18,160 --> 01:16:20,440
and then you can get other people to put in the examples.

1056
01:16:23,920 --> 01:16:25,600
You should all read some of my.

1057
01:16:25,600 --> 01:16:29,640
I couldn't force myself to learn C without writing a book

1058
01:16:29,640 --> 01:16:30,140
about it.

1059
01:16:30,140 --> 01:16:31,080
It was too boring.

1060
01:16:31,080 --> 01:16:32,960
That's the way.

1061
01:16:32,960 --> 01:16:35,680
Right.

1062
01:16:35,680 --> 01:16:39,360
If you read my book on computation, which is still

1063
01:16:39,360 --> 01:16:42,600
actually current because it's about things

1064
01:16:42,600 --> 01:16:46,480
that they don't teach much.

1065
01:16:46,480 --> 01:16:49,600
And the best thing about the book

1066
01:16:49,600 --> 01:16:54,600
is that it's full of problems that you can actually solve.

1067
01:16:54,600 --> 01:16:57,360
But I wasn't good at making up problems.

1068
01:16:57,360 --> 01:17:02,720
So most of the problems there were made up

1069
01:17:02,720 --> 01:17:06,820
by a young graduate student named Manuel Bloom.

1070
01:17:09,580 --> 01:17:13,340
And I don't think I gave him enough credit in the book.

1071
01:17:13,340 --> 01:17:16,140
I just took it for granted that people

1072
01:17:16,140 --> 01:17:18,420
are good at making up problems.

1073
01:17:18,420 --> 01:17:22,900
And I didn't realize how rare this was.

1074
01:17:22,900 --> 01:17:29,140
He now has the property that Manuel and his wife, Lenore,

1075
01:17:29,140 --> 01:17:34,020
and his son, whose name I forgot, are all professors at CMU.

1076
01:17:36,860 --> 01:17:38,900
Computer science.

1077
01:17:38,900 --> 01:17:40,940
Yes.

1078
01:17:40,940 --> 01:17:42,580
Are there any other such families?

1079
01:17:53,900 --> 01:17:57,040
And as far as I can tell, Winston's book

1080
01:17:57,040 --> 01:18:02,020
has been substantially replaced by this Norwegian.

1081
01:18:02,020 --> 01:18:03,700
What's his name?

1082
01:18:03,700 --> 01:18:09,820
But there's almost no ideas about AI in that book.

1083
01:18:09,820 --> 01:18:15,660
So we have a textbook that is not helping the field progress

1084
01:18:15,660 --> 01:18:20,460
very much because it's giving a lot of details about methods

1085
01:18:20,460 --> 01:18:21,660
that I do.

1086
01:18:21,660 --> 01:18:28,140
Well, like lots of stuff about probabilistic learning,

1087
01:18:28,140 --> 01:18:31,020
which is very nice, but I don't think it has much future.

1088
01:18:38,900 --> 01:18:42,020
So if you meet anybody who's interested in AI,

1089
01:18:42,020 --> 01:18:43,700
get them to read Paks' book.

1090
01:18:43,700 --> 01:18:44,460
Yes.

1091
01:18:44,460 --> 01:18:46,740
So what happened?

1092
01:18:46,740 --> 01:18:49,900
Why did MIT change so much?

1093
01:18:49,900 --> 01:18:55,860
Why did so many machine learning people?

1094
01:18:55,860 --> 01:18:58,660
Well, there was a very strange phenomenon

1095
01:18:58,660 --> 01:19:04,660
called Rodney Brooks, who somehow

1096
01:19:04,660 --> 01:19:13,420
got a worldwide reputation for inventing

1097
01:19:13,420 --> 01:19:15,900
what was it called?

1098
01:19:15,900 --> 01:19:17,460
Subsumption.

1099
01:19:17,460 --> 01:19:22,060
Subsumption was a formulation related

1100
01:19:22,060 --> 01:19:25,100
to what the first computer people called

1101
01:19:25,100 --> 01:19:28,220
priority interrupt.

1102
01:19:28,220 --> 01:19:34,360
So instead of having high level programming languages,

1103
01:19:34,360 --> 01:19:36,700
Brooke pointed out that if you just

1104
01:19:36,700 --> 01:19:41,600
have a bunch of statements and give them priorities

1105
01:19:41,600 --> 01:19:46,000
and then take the one that you take all the statements that

1106
01:19:46,000 --> 01:19:49,760
apply to a situation and take the one with the highest

1107
01:19:49,760 --> 01:19:56,120
priority and do that, then you can write a PhD thesis

1108
01:19:56,120 --> 01:19:59,280
and make a movie of an experiment in which a robot

1109
01:19:59,280 --> 01:20:02,560
actually found a Coke bottle.

1110
01:20:02,560 --> 01:20:05,360
Now, the fact that the first 40 times

1111
01:20:05,360 --> 01:20:09,440
it didn't find the Coke bottle because when

1112
01:20:09,440 --> 01:20:13,460
it got near the table, it couldn't see it anymore,

1113
01:20:13,460 --> 01:20:21,440
and it had no memory was ignored by the worldwide community.

1114
01:20:21,440 --> 01:20:29,560
And so some of Rodney Brooks' ideas swept the entire world,

1115
01:20:29,560 --> 01:20:35,680
stopped research in Japan, because they all

1116
01:20:35,680 --> 01:20:40,480
turned towards saying, if you just observe the environment

1117
01:20:40,480 --> 01:20:44,160
properly, you can solve a lot of problems without making plans.

1118
01:20:47,560 --> 01:20:57,720
And he won the top prize that the AI Society gave out.

1119
01:20:57,720 --> 01:20:59,880
It's a complete mystery to me.

1120
01:20:59,880 --> 01:21:02,960
Here is four or five really bad ideas

1121
01:21:02,960 --> 01:21:05,640
that swept the world and replaced

1122
01:21:05,640 --> 01:21:09,720
almost all other AI research for a decade or two.

1123
01:21:09,720 --> 01:21:10,220
Wonderful.

1124
01:21:13,400 --> 01:21:15,280
That's why there are no robots that

1125
01:21:15,280 --> 01:21:23,600
can go into the reactors in Japan and fix things,

1126
01:21:23,600 --> 01:21:30,600
because all the robots we have only react to never mind.

1127
01:21:30,600 --> 01:21:31,100
Yeah.

1128
01:21:33,960 --> 01:21:37,960
So how do you see we're changing that from now on?

1129
01:21:37,960 --> 01:21:41,960
Because now we have a problem that most of the,

1130
01:21:41,960 --> 01:21:45,960
maybe most of the stuff in computer science

1131
01:21:45,960 --> 01:21:52,960
are both in AI, probabilistic inference

1132
01:21:52,960 --> 01:21:58,960
or some kind of supervised learning or supervised learning.

1133
01:21:58,960 --> 01:22:00,960
And that makes a lot of money.

1134
01:22:00,960 --> 01:22:02,760
But I mean, if you were to.

1135
01:22:02,760 --> 01:22:03,880
Well, that's the problem.

1136
01:22:07,040 --> 01:22:09,880
The best way to make money is to steal it.

1137
01:22:09,880 --> 01:22:14,000
So why doesn't everybody just turn and go down the corner

1138
01:22:14,000 --> 01:22:14,880
and mug someone?

1139
01:22:17,560 --> 01:22:21,320
I mean, there's a real problem here.

1140
01:22:21,320 --> 01:22:22,720
Because you can mug a lot of people

1141
01:22:22,720 --> 01:22:25,720
in the parallel using awkwardness.

1142
01:22:25,720 --> 01:22:26,220
Right.

1143
01:22:30,960 --> 01:22:33,960
That's really.

1144
01:22:37,280 --> 01:22:39,280
You got it.

1145
01:22:39,280 --> 01:22:47,280
And isn't there any other smart professors that, like,

1146
01:22:47,280 --> 01:22:50,240
why can they see that?

1147
01:22:50,240 --> 01:22:51,720
Why do they see that?

1148
01:22:51,720 --> 01:22:52,520
It beats me.

1149
01:22:52,520 --> 01:23:00,840
I'd like to hear some theories of why

1150
01:23:00,840 --> 01:23:03,800
the textbook by Russell and Norwood

1151
01:23:03,800 --> 01:23:08,760
doesn't mention the society of mind theories, for example.

1152
01:23:08,760 --> 01:23:12,120
I mean, they've been out there for 30 years.

1153
01:23:12,120 --> 01:23:14,400
So there's something.

1154
01:23:14,400 --> 01:23:16,600
I don't know what the problem is.

1155
01:23:16,600 --> 01:23:20,400
But 250,000 people are taking that course.

1156
01:23:20,400 --> 01:23:23,200
Well, so things are going to get worse or better.

1157
01:23:24,200 --> 01:23:24,700
Whoops.

1158
01:23:29,320 --> 01:23:31,240
Has anybody watched some of this course?

1159
01:23:31,240 --> 01:23:32,640
Has it started yet?

1160
01:23:32,640 --> 01:23:34,520
Oh, it's almost over.

1161
01:23:34,520 --> 01:23:36,400
OK.

1162
01:23:36,400 --> 01:23:39,640
Have any of you looked?

1163
01:23:39,640 --> 01:23:40,320
No one here?

1164
01:23:40,320 --> 01:23:54,640
I think we have more people who aren't taking classes here.

1165
01:23:54,640 --> 01:23:55,360
I couldn't hear.

1166
01:23:55,360 --> 01:23:56,960
I think we have more people who aren't already

1167
01:23:56,960 --> 01:23:57,960
taking classes here.

1168
01:23:57,960 --> 01:23:59,540
Most people I know are taking that

1169
01:23:59,540 --> 01:24:03,480
out of the working industry or their high school or whatever.

1170
01:24:04,480 --> 01:24:10,400
It seems to me that, for example, machine learning,

1171
01:24:10,400 --> 01:24:12,320
when people take machine learning,

1172
01:24:12,320 --> 01:24:14,160
they think they are very smart because they

1173
01:24:14,160 --> 01:24:17,240
can do calculations with a lot of symbols.

1174
01:24:17,240 --> 01:24:19,760
And they think, oh, I'm such a genius.

1175
01:24:19,760 --> 01:24:23,240
I'm not saying that I'm smart, but it seems to me

1176
01:24:23,240 --> 01:24:26,840
that dealing with symbols make people think

1177
01:24:26,840 --> 01:24:29,240
that they are very smart.

1178
01:24:29,240 --> 01:24:31,360
That makes the situation even worse

1179
01:24:31,360 --> 01:24:34,800
because you have maybe people that

1180
01:24:34,800 --> 01:24:38,360
could solve this problem.

1181
01:24:38,360 --> 01:24:41,680
And they are, oh, I can do a lot of calculations.

1182
01:24:41,680 --> 01:24:46,240
And there are these really hard formulas that only I know.

1183
01:24:51,560 --> 01:24:59,600
It seems that theory of symbols is more complex and harder.

1184
01:24:59,640 --> 01:25:03,400
And people tend to stick to theories

1185
01:25:03,400 --> 01:25:07,400
that have symbols and formulas.

1186
01:25:07,400 --> 01:25:10,520
I guess so, but somehow it doesn't.

1187
01:25:10,520 --> 01:25:14,320
To me, mathematics is really hard, and symbols are easy.

1188
01:25:14,320 --> 01:25:16,840
It's sort of funny.

1189
01:25:16,840 --> 01:25:19,440
Yeah, but there are still symbols.

1190
01:25:19,440 --> 01:25:19,940
Yeah.

1191
01:25:19,940 --> 01:25:20,440
Yeah.

1192
01:25:24,100 --> 01:25:27,580
Well, does the machine course ever

1193
01:25:27,580 --> 01:25:32,820
mention like Pat Winston's?

1194
01:25:32,820 --> 01:25:35,580
It sounds like technique.

1195
01:25:35,580 --> 01:25:38,020
Machine learning.

1196
01:25:38,020 --> 01:25:44,180
Yes, because what Winston does is

1197
01:25:44,180 --> 01:25:46,260
he looks at the differences between things

1198
01:25:46,260 --> 01:25:48,340
and gets rid of them.

1199
01:25:48,340 --> 01:25:51,860
And you can think of it as a sort of symbolic servo.

1200
01:25:51,860 --> 01:25:55,780
But there are a lot of very simple, common sense,

1201
01:25:55,780 --> 01:26:00,780
sensical operations in the methods that he describes.

1202
01:26:00,780 --> 01:26:06,060
But none of them are easy to express as equations.

1203
01:26:06,060 --> 01:26:14,260
And maybe programming isn't technical enough.

1204
01:26:14,260 --> 01:26:20,380
And you need to dress things up in algebra to make them.

1205
01:26:20,380 --> 01:26:23,940
Yeah, should we do that as an energy minimization

1206
01:26:23,940 --> 01:26:25,220
principle or something?

1207
01:26:25,220 --> 01:26:26,100
Yes.

1208
01:26:26,100 --> 01:26:27,140
That's cute.

1209
01:26:27,140 --> 01:26:29,660
Yes.

1210
01:26:29,660 --> 01:26:32,180
Getting closer to the goal, that's all it is.

1211
01:26:37,900 --> 01:26:42,900
I think I mentioned the Newell and Simon had this great idea

1212
01:26:42,900 --> 01:26:46,020
of getting closer to the goal.

1213
01:26:46,020 --> 01:26:49,180
And they gave up when they couldn't solve the missionary

1214
01:26:49,180 --> 01:26:54,860
and cannibals problem, because there's

1215
01:26:54,860 --> 01:26:58,700
a step where you have to take three people back in the boat.

1216
01:26:58,700 --> 01:27:01,320
And their program refused to do that,

1217
01:27:01,320 --> 01:27:04,940
because it wasn't getting them closer to the goal.

1218
01:27:04,940 --> 01:27:11,540
And the funny part is that if you used pairs of operations

1219
01:27:11,540 --> 01:27:14,820
as the unit, then their method would work.

1220
01:27:19,340 --> 01:27:23,880
Nice question is, how do people make a very beautiful theory

1221
01:27:23,880 --> 01:27:28,700
and then not take the next step, and someone else

1222
01:27:28,700 --> 01:27:30,020
does it n years later?

1223
01:27:33,820 --> 01:27:37,460
I guess it always happens.

1224
01:27:37,460 --> 01:27:39,620
Yes.

1225
01:27:39,620 --> 01:27:42,180
I was just going to say, I think machine learning

1226
01:27:42,180 --> 01:27:44,660
is an incredibly useful mission to describe it,

1227
01:27:44,660 --> 01:27:46,900
because it really isn't AI.

1228
01:27:46,900 --> 01:27:49,260
And I think a lot of the problems

1229
01:27:49,260 --> 01:27:51,340
between machine learning people and AI people

1230
01:27:51,340 --> 01:27:53,340
is that they're trying to do different things,

1231
01:27:53,340 --> 01:27:55,940
and they just have shared history

1232
01:27:55,940 --> 01:27:59,100
and see different important problems.

1233
01:27:59,100 --> 01:28:01,460
Well, they're not doing different things.

1234
01:28:01,460 --> 01:28:04,540
One is doing a subset of the other.

1235
01:28:04,540 --> 01:28:05,040
Yes.

1236
01:28:05,040 --> 01:28:07,660
I mean, machine learning is the subset of the world.

1237
01:28:07,660 --> 01:28:11,580
But they are making a lot of good progress

1238
01:28:11,580 --> 01:28:14,340
in solving certain types of really hard problems.

1239
01:28:14,340 --> 01:28:16,500
And it can be really useful, because a lot of these

1240
01:28:16,500 --> 01:28:19,220
are problems that people aren't able to solve themselves

1241
01:28:19,220 --> 01:28:21,700
and aren't good at.

1242
01:28:21,700 --> 01:28:23,860
The short answer is definitely genius.

1243
01:28:23,860 --> 01:28:25,500
And AI is the science.

1244
01:28:25,500 --> 01:28:27,060
One at a time.

1245
01:28:27,060 --> 01:28:30,020
Yeah.

1246
01:28:30,020 --> 01:28:33,900
You could say that about anything, though.

1247
01:28:33,900 --> 01:28:37,660
Surely, what's really important is not

1248
01:28:37,660 --> 01:28:40,180
solving all sorts of useful things,

1249
01:28:40,180 --> 01:28:43,300
but getting to a higher level where you can solve things

1250
01:28:43,300 --> 01:28:45,700
that no one else can solve.

1251
01:28:45,700 --> 01:28:47,780
That's probably what you mean important for what?

1252
01:28:47,780 --> 01:28:49,660
Important for the advancement of science?

1253
01:28:49,660 --> 01:28:50,160
Yes.

1254
01:28:50,160 --> 01:28:53,540
Important for miscellaneous applications

1255
01:28:53,540 --> 01:28:56,060
that are useful to people?

1256
01:28:56,060 --> 01:28:58,260
Well, in the long run, you're dead.

1257
01:28:58,260 --> 01:29:01,300
So things like science.

1258
01:29:01,300 --> 01:29:03,100
To say that everything that doesn't just

1259
01:29:03,100 --> 01:29:07,220
advance science is completely useless is crazy.

1260
01:29:07,220 --> 01:29:08,820
No, I'm saying science.

1261
01:29:08,820 --> 01:29:14,100
Science has disappeared, because the dark ages.

1262
01:29:14,100 --> 01:29:14,860
It's interesting.

1263
01:29:14,860 --> 01:29:19,860
But if you say it's just advancing science, yes.

1264
01:29:19,860 --> 01:29:21,800
I take the position that we should

1265
01:29:21,800 --> 01:29:29,420
spend a fraction of our world's resources on long-term things.

1266
01:29:29,420 --> 01:29:32,700
And it's no excuse.

1267
01:29:32,700 --> 01:29:35,940
And no one should say, let's spend all our resources

1268
01:29:35,940 --> 01:29:39,820
on immediate, useful money making labor-saving processes.

1269
01:29:39,820 --> 01:29:40,780
I'm not saying that.

1270
01:29:40,780 --> 01:29:43,060
I think we should spend a lot of good money on science.

1271
01:29:43,060 --> 01:29:45,700
But I'm saying that doesn't mean that anything that's not

1272
01:29:45,700 --> 01:29:46,980
science is useful.

1273
01:29:46,980 --> 01:29:50,460
Yeah, but if you look at AI, no money.

1274
01:29:50,460 --> 01:29:54,620
I only know about 10 people in the entire country

1275
01:29:54,620 --> 01:29:57,540
who are thinking about hard problems in AI.

1276
01:29:57,540 --> 01:30:01,100
So it's not just a mild complaint.

1277
01:30:01,100 --> 01:30:04,220
There's some kind of pathology.

1278
01:30:04,220 --> 01:30:05,780
There's no support.

1279
01:30:05,780 --> 01:30:08,180
I don't think anybody's denying that machine learning is

1280
01:30:08,180 --> 01:30:09,900
useful, which just the problem is

1281
01:30:09,900 --> 01:30:12,620
that it seems like the obsession with machine learning

1282
01:30:12,620 --> 01:30:18,620
is sort of overtaking anyone trying to make progress in AI.

1283
01:30:18,620 --> 01:30:20,580
So it's like, OK, in the ideal world,

1284
01:30:20,580 --> 01:30:23,000
it's like there are the people who work on machine learning.

1285
01:30:23,000 --> 01:30:25,080
But then there are also the people who are solving

1286
01:30:25,080 --> 01:30:25,980
the big AI problems.

1287
01:30:25,980 --> 01:30:29,300
But it seems like in the current world,

1288
01:30:29,300 --> 01:30:32,780
so many people are doing machine learning.

1289
01:30:32,780 --> 01:30:34,940
And there are so many resources for it,

1290
01:30:34,940 --> 01:30:38,340
such that there is not enough for people who are doing AI.

1291
01:30:40,900 --> 01:30:46,820
Yeah, my complaint is that there's almost no one doing AI.

1292
01:30:46,820 --> 01:30:48,780
It's very peculiar.

1293
01:30:48,780 --> 01:30:55,660
So what is it about the current culture of science

1294
01:30:55,660 --> 01:31:00,740
and technology that causes this to happen?

1295
01:31:00,740 --> 01:31:05,500
A great example would be, what's the gross product of,

1296
01:31:05,500 --> 01:31:09,180
what's the income of IBM?

1297
01:31:09,180 --> 01:31:10,780
It's a big company.

1298
01:31:10,780 --> 01:31:15,700
How come they're doing that funny thing?

1299
01:31:15,700 --> 01:31:17,300
I keep forgetting.

1300
01:31:17,300 --> 01:31:21,140
How come they're doing Watson?

1301
01:31:21,140 --> 01:31:21,900
Isn't that?

1302
01:31:21,900 --> 01:31:24,020
Because it's cheaper than Super Bowl ads.

1303
01:31:28,020 --> 01:31:30,140
OK, they got a lot more publicity out of it

1304
01:31:30,140 --> 01:31:32,300
than they would have gotten if it's got all that money

1305
01:31:32,300 --> 01:31:35,100
on Super Bowl ads.

1306
01:31:35,100 --> 01:31:35,620
I guess so.

1307
01:31:39,140 --> 01:31:40,460
But it's a serious company.

1308
01:31:40,460 --> 01:31:43,580
It fired its real AI group.

1309
01:31:43,580 --> 01:31:46,460
And what?

1310
01:31:46,460 --> 01:31:47,420
Maybe properly.

1311
01:31:47,420 --> 01:31:52,860
The question is, perhaps the supplementary observation

1312
01:31:52,860 --> 01:31:57,580
is that when we were doing stuff a long time ago,

1313
01:31:57,580 --> 01:31:59,180
it was regarded as a great success

1314
01:31:59,180 --> 01:32:02,860
if you could get a program to do something for the first time.

1315
01:32:02,860 --> 01:32:05,020
And I feel it was taken over by people

1316
01:32:05,020 --> 01:32:07,340
who want to measure how much better your thing is

1317
01:32:07,340 --> 01:32:08,820
than the next thing.

1318
01:32:08,820 --> 01:32:10,780
And there are only a few kinds of problems

1319
01:32:10,780 --> 01:32:15,540
that are susceptible to that kind of comparison.

1320
01:32:15,580 --> 01:32:18,380
As a consequence, people working on those kinds of problems

1321
01:32:18,380 --> 01:32:22,300
are not doing things that ought to be done,

1322
01:32:22,300 --> 01:32:24,300
like doing something for the first time.

1323
01:32:29,300 --> 01:32:30,980
So it's a lot easier to write an article

1324
01:32:30,980 --> 01:32:34,620
about how your parser is 1% better than somebody else's

1325
01:32:34,620 --> 01:32:36,380
parser than it is to get a program

1326
01:32:36,380 --> 01:32:39,820
to have common sense for instance.

1327
01:32:39,820 --> 01:32:41,940
Yeah.

1328
01:32:41,940 --> 01:32:45,140
Well, now there are four or five common sense research groups,

1329
01:32:45,140 --> 01:32:47,620
I think.

1330
01:32:47,620 --> 01:32:48,100
How many?

1331
01:32:58,260 --> 01:33:00,220
I don't know if any of them make a living yet.

1332
01:33:03,580 --> 01:33:05,580
Well, they're collecting common sense.

1333
01:33:05,580 --> 01:33:09,060
I'm not sure they have common sense.

1334
01:33:09,060 --> 01:33:11,540
And you can measure how many things you know,

1335
01:33:11,540 --> 01:33:15,300
but it's not clear that they're measuring

1336
01:33:15,300 --> 01:33:18,260
how effective they are with the stuff that they know.

1337
01:33:18,260 --> 01:33:18,740
Yeah.

1338
01:33:27,180 --> 01:33:28,700
I wonder how hard it would be to make

1339
01:33:28,700 --> 01:33:33,100
a program that knows mathematics,

1340
01:33:33,100 --> 01:33:37,420
and a lot of mathematics, and does the kinds of things

1341
01:33:37,420 --> 01:33:41,620
that mathematicians do.

1342
01:33:52,340 --> 01:33:54,580
But somebody would have to pay them to do it.

1343
01:33:54,580 --> 01:34:00,180
And where do you get support for common sense

1344
01:34:00,180 --> 01:34:02,980
in mathematical reasoning?

1345
01:34:02,980 --> 01:34:05,740
It's a beautiful subject.

1346
01:34:05,780 --> 01:34:08,540
Well, there was Len, and there was Maxima,

1347
01:34:08,540 --> 01:34:11,140
and there was Slabel, and there was Moses.

1348
01:34:11,140 --> 01:34:15,900
So there were lots of work to do.

1349
01:34:15,900 --> 01:34:16,980
Right.

1350
01:34:16,980 --> 01:34:17,740
And Wolfram.

1351
01:34:22,140 --> 01:34:25,580
It might be for all his faults, Wolfram's

1352
01:34:25,580 --> 01:34:29,460
going to get people to work on real AI,

1353
01:34:29,500 --> 01:34:37,340
because he's hired some people to try to make,

1354
01:34:37,340 --> 01:34:39,180
what's it called?

1355
01:34:39,180 --> 01:34:41,580
Wolfram Alpha, that's cool.

1356
01:34:41,580 --> 01:34:44,660
To make it have more and more common sense.

1357
01:34:52,780 --> 01:34:55,280
Yeah.

1358
01:34:55,280 --> 01:34:58,260
I was going to make one at the beginning.

1359
01:34:58,460 --> 01:35:00,820
One of the challenges to trying to make systems

1360
01:35:00,820 --> 01:35:04,140
that want to do math, or things that mathematicians do,

1361
01:35:04,140 --> 01:35:07,180
is deciding what problems are interesting

1362
01:35:07,180 --> 01:35:08,940
that one wants to work on.

1363
01:35:08,940 --> 01:35:10,900
There is automatic verification in proving,

1364
01:35:10,900 --> 01:35:13,940
although those are also rather difficult problems.

1365
01:35:13,940 --> 01:35:18,860
When you get complicated things, it's important as well.

1366
01:35:18,860 --> 01:35:20,220
What should be proven is something

1367
01:35:20,220 --> 01:35:24,660
that I've never seen tackled, and doesn't seem all that

1368
01:35:24,660 --> 01:35:28,180
feasible at the moment.

1369
01:35:28,580 --> 01:35:31,100
Well, the first piece of work was on a thesis

1370
01:35:31,100 --> 01:35:35,460
that did a kind of mathematical exploration on the basis

1371
01:35:35,460 --> 01:35:42,260
of some theory of why an expression or a transformation

1372
01:35:42,260 --> 01:35:43,620
might be interesting.

1373
01:35:43,620 --> 01:35:45,980
So even that had some precedent.

1374
01:35:45,980 --> 01:35:46,460
Oh, really?

1375
01:35:52,540 --> 01:35:54,580
It was a system that started from scratch,

1376
01:35:54,580 --> 01:35:56,500
and proved a few theorems, and looked around

1377
01:35:56,500 --> 01:35:59,180
for things that were interesting and embedded

1378
01:35:59,180 --> 01:36:05,140
the opposite of least common divisor, or something like that.

1379
01:36:05,140 --> 01:36:07,900
Yeah, that was AM.

1380
01:36:07,900 --> 01:36:09,820
And it was pretty exciting.

1381
01:36:09,820 --> 01:36:15,940
And that's something that anybody

1382
01:36:15,940 --> 01:36:24,220
could take up again any time, and be a great thesis topic.

1383
01:36:24,220 --> 01:36:30,580
Lenat, the phenomenon in AM was that it

1384
01:36:30,580 --> 01:36:35,180
discovered division, and prime numbers, and a number.

1385
01:36:35,180 --> 01:36:36,380
I think I talked about it.

1386
01:36:38,940 --> 01:36:43,660
And as Pat says, it had a bunch of criterion

1387
01:36:43,660 --> 01:36:45,820
for interestingness.

1388
01:36:45,820 --> 01:36:48,980
And it would find sets that were unusually

1389
01:36:48,980 --> 01:36:55,580
larger, unusually small, and try to make more things like them,

1390
01:36:55,580 --> 01:36:57,780
and that sort of thing.

1391
01:36:57,780 --> 01:37:01,940
But it's never been followed up.

1392
01:37:04,940 --> 01:37:07,500
What's the main problem with all the high school

1393
01:37:07,500 --> 01:37:10,180
Olympia math problems?

1394
01:37:10,180 --> 01:37:14,220
Yeah, high school algebra is a wonderful area.

1395
01:37:14,220 --> 01:37:18,060
And there was Bobrow's thesis, which did a little bit of it.

1396
01:37:18,060 --> 01:37:22,980
And now there's so much more known about natural language.

1397
01:37:22,980 --> 01:37:25,780
You'd think somebody would do a super Bobrow

1398
01:37:25,780 --> 01:37:30,780
program that might actually be good enough to connect

1399
01:37:30,780 --> 01:37:33,980
into Google so that it would solve problems

1400
01:37:33,980 --> 01:37:37,500
that real people faced every now and then.

1401
01:37:38,500 --> 01:37:52,900
So yeah, the appearance of the new Siri and Dragon things,

1402
01:37:52,900 --> 01:37:55,340
which actually answer useful questions,

1403
01:37:55,340 --> 01:37:58,660
and roll from alpha, might change.

1404
01:37:58,900 --> 01:38:04,300
There might be an industry in commonsense AI

1405
01:38:04,300 --> 01:38:09,420
that's actually useful because a few million people might start

1406
01:38:09,420 --> 01:38:13,460
using it as soon as it gets just a little bit better.

1407
01:38:13,460 --> 01:38:19,060
And I find that Dragon and Siri, I haven't used Siri,

1408
01:38:19,060 --> 01:38:24,500
but Dragon now has a thing called Dragon Search.

1409
01:38:24,500 --> 01:38:25,620
You talk to it.

1410
01:38:25,620 --> 01:38:28,620
And ask a question in ordinary English.

1411
01:38:28,620 --> 01:38:33,860
And it goes to the web and makes a Google Search sentence out

1412
01:38:33,860 --> 01:38:36,020
of what you said.

1413
01:38:36,020 --> 01:38:38,500
And a good fraction of the time, it

1414
01:38:38,500 --> 01:38:40,340
saves you a half hour of search.

1415
01:38:45,740 --> 01:38:49,500
None of those really have any idea of context, though.

1416
01:38:49,500 --> 01:38:51,900
Like, is there any context to it?

1417
01:38:51,900 --> 01:38:55,220
None of those really have any idea of context, though.

1418
01:38:55,220 --> 01:38:58,020
Like, a very simple improvement would be,

1419
01:38:58,020 --> 01:39:00,180
and Wolfram Althoff already does this a little bit,

1420
01:39:00,180 --> 01:39:03,460
is to say, assuming that you meant x, did you mean y or z?

1421
01:39:03,460 --> 01:39:04,740
And you can click those instead.

1422
01:39:04,740 --> 01:39:06,340
And that's as far as the conversation

1423
01:39:06,340 --> 01:39:07,740
with Wolfram Althoff goes.

1424
01:39:07,740 --> 01:39:09,620
Right, but you're a person.

1425
01:39:09,620 --> 01:39:15,300
And if it had a lot of information about you,

1426
01:39:15,300 --> 01:39:16,740
you could collect.

1427
01:39:16,740 --> 01:39:19,700
First of all, it would say, what kind of person are you?

1428
01:39:19,700 --> 01:39:22,180
And you'd give your biography.

1429
01:39:22,180 --> 01:39:25,700
And maybe we'll get common-sense databases that say,

1430
01:39:25,700 --> 01:39:27,220
well, here are the kinds of things

1431
01:39:27,220 --> 01:39:33,460
that somebody who likes pole vaulting is likely to know.

1432
01:39:33,460 --> 01:39:36,380
We already have the what kind of person

1433
01:39:36,380 --> 01:39:39,540
are you databases in social networks.

1434
01:39:39,540 --> 01:39:41,620
That's what it is, right?

1435
01:39:41,620 --> 01:39:45,700
Google's personalized search, yeah.

1436
01:39:45,700 --> 01:39:48,140
Well, I just joined Facebook by mistake.

1437
01:39:49,700 --> 01:39:55,300
And all sorts of people want to link.

1438
01:39:55,300 --> 01:39:57,940
And all you have to do is look at their biography

1439
01:39:57,940 --> 01:40:00,740
and see what music they like.

1440
01:40:00,740 --> 01:40:03,940
And you can see you don't want to have that person as a friend.

1441
01:40:06,420 --> 01:40:11,540
But I wish, has anybody written a program to automate this?

1442
01:40:14,340 --> 01:40:16,420
I could tell them, I don't like music

1443
01:40:16,420 --> 01:40:18,140
written in the last 20 years.

1444
01:40:20,540 --> 01:40:24,940
And that would get rid of about half of them right off.

1445
01:40:24,940 --> 01:40:28,500
Of course, it might be Freeman Dyson or somebody

1446
01:40:28,500 --> 01:40:31,940
happens to like one of these tunes.

1447
01:40:31,940 --> 01:40:33,540
I don't know.

1448
01:40:33,540 --> 01:40:39,260
So then you have to make a hierarchy.

1449
01:40:39,260 --> 01:40:42,540
If the person happens to be a really good physicist,

1450
01:40:43,180 --> 01:40:45,700
then we'll flush that particular filter.

1451
01:40:55,620 --> 01:40:58,060
Yeah, also Google's personalized search profile

1452
01:40:58,060 --> 01:41:00,140
thinks that I'm a man, because apparently I've

1453
01:41:00,140 --> 01:41:05,380
searched too many computer science things in the past.

1454
01:41:05,380 --> 01:41:06,140
That's pretty good.

1455
01:41:07,140 --> 01:41:09,660
OK.

1456
01:41:09,660 --> 01:41:12,700
Maybe thinks most people are a man.

1457
01:41:12,700 --> 01:41:14,020
Oh, I don't know.

1458
01:41:14,020 --> 01:41:16,500
With other women, they get it right.

1459
01:41:16,500 --> 01:41:21,740
But if you happen to be a woman who does computer tech-age

1460
01:41:21,740 --> 01:41:25,660
stuff, and if you happen to do a lot of search on programming,

1461
01:41:25,660 --> 01:41:28,180
for instance, like looking up APIs of things,

1462
01:41:28,180 --> 01:41:32,020
then it tends to try to make that you're a man.

1463
01:41:32,020 --> 01:41:34,540
Does it store the gender as a Boolean, or is it?

1464
01:41:37,100 --> 01:41:42,140
I saw this a while ago, and it was just like, according to this,

1465
01:41:42,140 --> 01:41:43,700
here's what they think that you are.

1466
01:41:43,700 --> 01:41:46,540
And at the top, it was like, we think you're a man.

1467
01:41:46,540 --> 01:41:49,540
I'm interested to know, though, how manly exactly you

1468
01:41:49,540 --> 01:41:51,460
become for certain stuff.

1469
01:41:51,460 --> 01:41:54,420
So I don't think they actually tell you

1470
01:41:54,420 --> 01:41:57,860
their internal representation of who you are.

1471
01:41:57,860 --> 01:42:00,980
But you just get to see a couple of categories,

1472
01:42:00,980 --> 01:42:05,100
like male, 25 to 35, interested in computers,

1473
01:42:05,100 --> 01:42:08,220
interested in, et cetera.

1474
01:42:08,220 --> 01:42:11,180
I'm sure they keep it at a level somewhere.

1475
01:42:16,620 --> 01:42:19,140
You could cross-correlate that with geography

1476
01:42:19,140 --> 01:42:21,940
and be like, this is the manliest level in a circle.

1477
01:42:26,620 --> 01:42:28,220
Is there anything in Facebook that

1478
01:42:28,220 --> 01:42:32,140
would help us get a commonsense database, or is it?

1479
01:42:32,140 --> 01:42:36,100
There's no cause for that.

1480
01:42:36,100 --> 01:42:40,940
Or is there no useful knowledge?

1481
01:42:40,940 --> 01:42:42,420
Must be something that could.

1482
01:42:42,420 --> 01:42:44,020
Well, apparently, a few years back

1483
01:42:44,020 --> 01:42:45,860
for how we was in this class, somebody

1484
01:42:45,860 --> 01:42:50,980
made a program that could find out who was gay or not,

1485
01:42:50,980 --> 01:42:54,780
even if they don't put gay or not gay on their profile,

1486
01:42:54,780 --> 01:42:57,620
based on who their friends are.

1487
01:42:57,620 --> 01:43:00,340
Yeah, my friend was gay when he found out.

1488
01:43:00,340 --> 01:43:02,740
Well, see, this is not a statistical correlation.

1489
01:43:02,740 --> 01:43:05,060
It's actually useful, because you

1490
01:43:05,060 --> 01:43:06,980
can specify that you're looking for someone

1491
01:43:06,980 --> 01:43:08,780
who is 0.75 female.

1492
01:43:18,780 --> 01:43:21,780
How many women's names begin with the letter X?

1493
01:43:21,780 --> 01:43:22,660
Begin with?

1494
01:43:22,660 --> 01:43:24,860
The letter X. That may be your problem.

1495
01:43:24,860 --> 01:43:25,360
No.

1496
01:43:29,700 --> 01:43:32,860
Yes, it could be.

1497
01:43:32,860 --> 01:43:35,540
I heard that China is allowing more names.

1498
01:43:37,940 --> 01:43:40,420
Is it allowing?

1499
01:43:40,420 --> 01:43:45,380
Well, I read an article which said there are only

1500
01:43:45,380 --> 01:43:53,020
about 100 names in most of China.

1501
01:43:53,020 --> 01:43:58,380
And in fact, there was a law that both your names

1502
01:43:58,380 --> 01:44:01,860
have to be from this small set.

1503
01:44:01,860 --> 01:44:07,340
And so there are only 10,000 different pairs of names.

1504
01:44:07,340 --> 01:44:08,700
Do I mean that?

1505
01:44:08,700 --> 01:44:09,420
Yeah.

1506
01:44:09,420 --> 01:44:11,340
Family names?

1507
01:44:11,340 --> 01:44:13,860
Are there a lot of family names?

1508
01:44:13,860 --> 01:44:18,020
Well, I feel like there aren't really restrictions for names,

1509
01:44:18,020 --> 01:44:20,380
as long as they're Chinese characters.

1510
01:44:20,380 --> 01:44:22,940
I heard about a lawsuit in China where somebody wanted

1511
01:44:22,940 --> 01:44:25,060
to name their kid something-something.com

1512
01:44:25,060 --> 01:44:25,980
in the government.

1513
01:44:29,500 --> 01:44:32,380
But as long as you pick real Chinese characters,

1514
01:44:32,380 --> 01:44:35,820
you can name your kid Big Poopyhead, and it's OK.

1515
01:44:35,820 --> 01:44:37,900
In fact, naming your kid Big Poopyhead

1516
01:44:37,900 --> 01:44:40,820
is actually rather common in the countryside,

1517
01:44:40,820 --> 01:44:43,020
because there's some superstition that

1518
01:44:43,020 --> 01:44:46,300
believes that if your kid has a really terrible name,

1519
01:44:46,300 --> 01:44:48,460
then he won't die during childhood,

1520
01:44:48,460 --> 01:44:50,540
because even the devil won't want him.

1521
01:44:52,940 --> 01:44:57,700
And for all that we know, it's true.

1522
01:44:57,700 --> 01:45:00,660
I was like, I'm just thinking about killing him.

1523
01:45:00,660 --> 01:45:02,940
Cobble says knowledge.

1524
01:45:02,940 --> 01:45:06,020
Are you only allowed one first and last name now?

1525
01:45:06,020 --> 01:45:08,420
Because it seems like you should only need two names,

1526
01:45:08,420 --> 01:45:11,260
and then just give everyone at most 64 names,

1527
01:45:11,260 --> 01:45:13,460
and that should cover all of China.

1528
01:45:13,460 --> 01:45:15,620
Well, I think in Chinese, it's like,

1529
01:45:15,620 --> 01:45:19,740
there's a set of last names that's pretty much standard.

1530
01:45:19,740 --> 01:45:23,380
And I'm not sure how many, but maybe a few hundred.

1531
01:45:23,380 --> 01:45:25,580
And then given names.

1532
01:45:25,580 --> 01:45:27,780
So family names are usually one character,

1533
01:45:27,780 --> 01:45:30,580
but occasionally they're two characters.

1534
01:45:30,580 --> 01:45:35,340
And given names are either one or two characters.

1535
01:45:35,340 --> 01:45:37,060
And you just get those two?

1536
01:45:37,060 --> 01:45:39,300
And you can pick any character.

1537
01:45:39,300 --> 01:45:41,260
Oh, well, there are a lot of characters.

1538
01:45:41,260 --> 01:45:41,820
Yeah.

1539
01:45:41,820 --> 01:45:43,580
Like several thousand?

1540
01:45:43,580 --> 01:45:45,300
Yeah.

1541
01:45:45,300 --> 01:45:49,780
So it's something like 1,000 by 1,000 or?

1542
01:45:49,780 --> 01:45:52,260
Or 3,000.

1543
01:45:52,260 --> 01:45:55,820
3,000 by 3,000 by maybe another 3,000.

1544
01:45:55,820 --> 01:45:58,500
The British solution is to just keep adding simple names.

1545
01:45:58,500 --> 01:46:02,300
Like my brother's name is Dylan Neville Thomas Bedford.

1546
01:46:02,300 --> 01:46:04,380
And you can just keep changing that name.

1547
01:46:04,380 --> 01:46:07,140
And then if you get stuck with too many John Smiths,

1548
01:46:07,140 --> 01:46:11,100
you just do thirds, fourths, fifths.

1549
01:46:11,100 --> 01:46:14,340
Well, the article I read must have been wrong then.

1550
01:46:14,340 --> 01:46:16,420
I misunderstood it.

1551
01:46:16,420 --> 01:46:19,220
Because it said somewhere, areas of China,

1552
01:46:19,220 --> 01:46:22,020
there are only 100 names.

1553
01:46:22,020 --> 01:46:26,020
So if you have more than 10,000 people,

1554
01:46:26,020 --> 01:46:29,340
you get people with the same name.

1555
01:46:29,340 --> 01:46:31,740
Anyway, that's their problem.

1556
01:46:40,980 --> 01:46:42,740
Maybe the person who wrote the article

1557
01:46:42,740 --> 01:46:44,780
couldn't tell the difference between their friend Xu

1558
01:46:44,780 --> 01:46:45,660
and their friend Xu.

1559
01:46:48,660 --> 01:46:51,340
Well, one thing that they also did historically in China

1560
01:46:51,340 --> 01:46:53,420
is that sometimes people have middle names.

1561
01:46:53,420 --> 01:46:56,900
And the middle name is based on this thing that

1562
01:46:56,900 --> 01:46:59,860
was written centuries ago that for each generation,

1563
01:46:59,860 --> 01:47:02,860
it has a set character that must be your middle name.

1564
01:47:02,860 --> 01:47:06,900
So your name will be like family name, this middle name,

1565
01:47:06,900 --> 01:47:10,140
and then your actual given name.

1566
01:47:10,140 --> 01:47:11,260
So I don't know.

1567
01:47:11,260 --> 01:47:16,380
But that's sort of like becoming more obsolete nowadays.

1568
01:47:16,380 --> 01:47:18,180
Well, in English, we have lots of names

1569
01:47:18,180 --> 01:47:22,860
like farmer and painter and so forth.

1570
01:47:22,860 --> 01:47:25,580
But people don't live up to them much.

1571
01:47:36,540 --> 01:47:38,580
Because it's difficult to tell what a given baby is

1572
01:47:38,580 --> 01:47:40,820
going to do.

1573
01:47:40,820 --> 01:47:42,380
But you can order it around.

1574
01:47:46,860 --> 01:47:49,140
But actually, the historic Chinese system

1575
01:47:49,140 --> 01:47:53,100
is kind of an interesting way of addressing people,

1576
01:47:53,100 --> 01:47:54,060
like indexing people.

1577
01:47:54,060 --> 01:47:57,020
Because you have the first character, which is the family.

1578
01:47:57,020 --> 01:47:59,420
And then the second character is the generation.

1579
01:47:59,420 --> 01:48:01,300
And then the third character is you.

1580
01:48:01,300 --> 01:48:05,700
So it's like if your family is X and you're from generation A,

1581
01:48:05,700 --> 01:48:11,300
then your siblings will be like XAB, XAC, whatever.

1582
01:48:11,300 --> 01:48:15,220
And then the next generation will be like XB, whatever.

1583
01:48:15,220 --> 01:48:18,580
OK, maybe I should have picked a different outfit instead.

1584
01:48:18,580 --> 01:48:19,940
But you know what I mean.

1585
01:48:19,940 --> 01:48:22,300
Oh, a generation number?

1586
01:48:22,300 --> 01:48:25,780
Kind of, except for it's a character.

1587
01:48:25,780 --> 01:48:31,780
There should be enough IPV6 as an example.

1588
01:48:31,860 --> 01:48:33,660
I do agree that you're just going to end up

1589
01:48:33,660 --> 01:48:37,260
with us all being numbers in X.

1590
01:48:37,260 --> 01:48:39,140
But that being said, I've always wanted

1591
01:48:39,140 --> 01:48:41,980
to name my kids something that rhymes with the letter like 1,

1592
01:48:41,980 --> 01:48:47,860
2, 3, 4, so that I can just call them by their number

1593
01:48:47,860 --> 01:48:51,580
without them realizing that I'm calling them by their number.

1594
01:48:51,580 --> 01:48:53,420
That's a great idea.

1595
01:48:53,420 --> 01:48:56,380
I picked 1 instead of X.

1596
01:48:56,380 --> 01:48:59,540
1, serving me a name.

1597
01:48:59,540 --> 01:49:05,820
It could be the number of letters in the name, like 4.

1598
01:49:05,820 --> 01:49:08,780
I just want to start with, please.

1599
01:49:08,780 --> 01:49:12,260
You name your fourth kid Forrest.

1600
01:49:12,260 --> 01:49:13,260
Fork, I'm here.

1601
01:49:17,020 --> 01:49:18,500
Or name your third kid Tray.

1602
01:49:21,900 --> 01:49:24,080
Eventually, the name could be longer,

1603
01:49:24,080 --> 01:49:26,180
and it could be a whole little semantic net

1604
01:49:26,180 --> 01:49:29,020
with a little list structure in it.

1605
01:49:33,420 --> 01:49:41,420
There was a nice science fiction story about some culture

1606
01:49:41,420 --> 01:49:45,700
where you had to know your genealogy back

1607
01:49:45,700 --> 01:49:47,500
hundreds of generations.

1608
01:49:47,500 --> 01:49:51,020
I forgot the author.

1609
01:49:51,020 --> 01:49:52,540
But it was made into a movie.

1610
01:49:56,180 --> 01:50:04,500
Well, who wants the last word?

1611
01:50:13,940 --> 01:50:14,940
That's good.

1612
01:50:18,420 --> 01:50:19,900
Someone else can have it.

1613
01:50:19,900 --> 01:50:23,580
Yeah, do we have any announcement to make?

1614
01:50:23,580 --> 01:50:26,100
No, just final projects we do on Sunday.

1615
01:50:26,100 --> 01:50:28,500
That's it.

1616
01:50:28,500 --> 01:50:30,820
Do you know what you're teaching next semester?

1617
01:50:30,820 --> 01:50:36,500
We're trying to make a plan for some kind of weekly seminar

1618
01:50:36,500 --> 01:50:41,860
that's not for credit, although I

1619
01:50:41,860 --> 01:50:45,180
might have to make it for credit for financial reasons,

1620
01:50:45,180 --> 01:50:46,340
or nobody will pay me.

1621
01:50:50,580 --> 01:50:54,180
But we could go back to giving everyone A's again.

1622
01:50:57,100 --> 01:51:01,580
So you won't be teaching this subject until next fall, then?

1623
01:51:01,580 --> 01:51:04,180
I guess not.

1624
01:51:04,180 --> 01:51:08,500
I haven't absolutely decided, but I'm pretty sure.

1625
01:51:08,500 --> 01:51:11,660
Can they take your class?

1626
01:51:11,660 --> 01:51:15,940
How many students do you have each year?

1627
01:51:15,940 --> 01:51:18,020
Well, we've got room for about 40,

1628
01:51:18,020 --> 01:51:19,940
and usually about 120 sign up.

1629
01:51:20,180 --> 01:51:20,660
Mm-hmm.

1630
01:51:23,460 --> 01:51:26,380
And then you weed them out?

1631
01:51:26,380 --> 01:51:26,880
Yes.

1632
01:51:30,820 --> 01:51:35,020
I'd like to borrow your weeding program.

1633
01:51:35,020 --> 01:51:38,140
OK, well, thank you all for coming, and it's been fun.

1634
01:51:38,140 --> 01:51:39,700
Thank you.

