1
00:00:00,000 --> 00:00:02,400
The following content is provided under a Creative

2
00:00:02,400 --> 00:00:03,800
Commons license.

3
00:00:03,800 --> 00:00:06,000
Your support will help MIT OpenCourseWare

4
00:00:06,000 --> 00:00:10,120
continue to offer high-quality educational resources for free.

5
00:00:10,120 --> 00:00:12,660
To make a donation or to view additional materials

6
00:00:12,660 --> 00:00:16,600
from hundreds of MIT courses, visit MIT OpenCourseWare

7
00:00:16,600 --> 00:00:17,640
at ocw.mit.edu.

8
00:00:23,360 --> 00:00:24,680
Well, I don't have the lecture.

9
00:00:25,560 --> 00:00:29,880
So good.

10
00:00:29,880 --> 00:00:31,200
I have a random question.

11
00:00:31,200 --> 00:00:32,680
Great.

12
00:00:32,680 --> 00:00:34,800
So you've been a teacher for a very long time.

13
00:00:34,800 --> 00:00:37,960
Have you noticed any patterns in students

14
00:00:37,960 --> 00:00:41,160
over the years or decades?

15
00:00:41,160 --> 00:00:43,360
Have I noticed any pattern in students?

16
00:00:43,360 --> 00:00:46,920
Yeah, or like intellectual patterns

17
00:00:46,920 --> 00:00:51,120
or things that people are interested in, just anything.

18
00:00:51,200 --> 00:00:51,880
Well, a few.

19
00:00:55,200 --> 00:00:58,560
The foreigners seem better educated than the Americans.

20
00:01:02,520 --> 00:01:05,600
There are more girls.

21
00:01:05,600 --> 00:01:09,880
When I came to MIT, it was about 20%.

22
00:01:09,880 --> 00:01:11,960
And I think now it's 53%.

23
00:01:11,960 --> 00:01:12,800
Does anyone know?

24
00:01:16,720 --> 00:01:19,080
What?

25
00:01:19,080 --> 00:01:20,880
48?

26
00:01:21,760 --> 00:01:25,720
I read that it actually went past 50 for a few minutes.

27
00:01:31,960 --> 00:01:35,440
No, I think I've complained about the future, though,

28
00:01:35,440 --> 00:01:44,960
which is that a large proportion of my students, by students,

29
00:01:44,960 --> 00:01:52,760
I mean the ones whose thesis I hate to say supervised,

30
00:01:52,760 --> 00:02:01,760
because in the case of Pat Winston, for example,

31
00:02:01,760 --> 00:02:04,560
I learned much more than I or Sussman.

32
00:02:07,280 --> 00:02:14,160
But most of the students became researchers or faculty

33
00:02:14,160 --> 00:02:24,480
members eventually, and now very few of them do.

34
00:02:27,960 --> 00:02:34,400
I'm not sure of all the reasons, but in the 1960s,

35
00:02:34,400 --> 00:02:37,720
which is a long time ago, the universities were still

36
00:02:37,720 --> 00:02:38,280
growing.

37
00:02:39,120 --> 00:02:45,200
And as an after-effect of World War II, I suppose,

38
00:02:45,200 --> 00:02:53,520
I really don't know what caused these major trends.

39
00:02:53,520 --> 00:03:02,320
But there were also a lot of career research institutions

40
00:03:02,320 --> 00:03:06,440
that were large and growing.

41
00:03:06,440 --> 00:03:09,880
Even General Motors had places where

42
00:03:09,880 --> 00:03:11,600
there was some basic research.

43
00:03:11,600 --> 00:03:15,320
IBM was a big research laboratory

44
00:03:15,320 --> 00:03:23,280
that was supporting some very abstract and basic research

45
00:03:23,280 --> 00:03:24,800
of various sorts.

46
00:03:24,800 --> 00:03:26,600
I don't think there's very much of that now.

47
00:03:29,520 --> 00:03:31,440
Even CBS Laboratory.

48
00:03:31,440 --> 00:03:34,800
Westinghouse was doing interesting robotics

49
00:03:34,800 --> 00:03:41,560
and, of course, Stanford Research Institute,

50
00:03:41,560 --> 00:03:46,460
which had no relation to Stanford, still exists,

51
00:03:46,460 --> 00:03:49,320
and it's still pretty good.

52
00:03:49,320 --> 00:03:54,160
But in those early days, it was one

53
00:03:54,160 --> 00:03:58,700
of the three or four richest computer

54
00:03:58,700 --> 00:04:02,680
science and artificial intelligence research places.

55
00:04:02,680 --> 00:04:05,680
There was a place called the Rand Corporation, which

56
00:04:05,680 --> 00:04:07,000
I think still exists.

57
00:04:07,000 --> 00:04:09,160
Does anybody?

58
00:04:09,160 --> 00:04:10,520
I don't know what it does.

59
00:04:10,520 --> 00:04:11,200
Any idea?

60
00:04:11,200 --> 00:04:16,000
They do government-assaulting sort of things,

61
00:04:16,000 --> 00:04:20,080
just translate writing in bigger fonts.

62
00:04:20,080 --> 00:04:22,720
Basically war games, but not necessarily

63
00:04:22,720 --> 00:04:27,320
about war, sort of economy games or politics games.

64
00:04:27,320 --> 00:04:37,400
But in the 60s, it had a lot of basic research.

65
00:04:37,400 --> 00:04:46,680
It had Newell and Simon and me and a few other people.

66
00:04:46,680 --> 00:04:51,640
And we just went there and could walk on the beach in Santa

67
00:04:51,640 --> 00:04:55,880
Monica and go to your office and talk and do things.

68
00:04:55,880 --> 00:04:58,060
And no one ever bothered us, and we

69
00:04:58,060 --> 00:05:00,160
wrote lots of little papers.

70
00:05:00,160 --> 00:05:04,840
And anyway, grumble, grumble.

71
00:05:04,840 --> 00:05:10,840
Another feature was that places like the National

72
00:05:10,840 --> 00:05:15,180
Institute of Health had five-year fellowships.

73
00:05:15,180 --> 00:05:17,840
And now you have to renew.

74
00:05:17,840 --> 00:05:22,080
There are very few appointments of that sort anywhere.

75
00:05:22,080 --> 00:05:26,400
And usually, no sooner do you get funded

76
00:05:26,400 --> 00:05:31,360
than you're starting to write proposals for the next year.

77
00:05:31,360 --> 00:05:37,920
And some people want reports every quarter.

78
00:05:37,920 --> 00:05:42,240
And Neil Gershenfeld, who was running a big lab here,

79
00:05:42,240 --> 00:05:44,340
wanted reports every month.

80
00:05:44,340 --> 00:05:50,600
And some of us finally gave up on that.

81
00:05:50,600 --> 00:05:51,680
That's a long answer.

82
00:05:54,760 --> 00:06:00,560
So if you want a career in being a professor,

83
00:06:00,560 --> 00:06:05,240
it's just harder to find now than it was then.

84
00:06:05,240 --> 00:06:08,080
And so a lot of people recognize this pretty early

85
00:06:08,080 --> 00:06:14,640
and find some place to work in Wall Street and stuff like that.

86
00:06:14,640 --> 00:06:17,280
There are lots of jobs for smart people,

87
00:06:17,280 --> 00:06:21,960
but then you have to sneak your research in on the side.

88
00:06:25,800 --> 00:06:27,640
Anybody can think of a way to fix it?

89
00:06:31,960 --> 00:06:35,560
In the last 20 years, Taiwan made 100 new math departments.

90
00:06:35,560 --> 00:06:37,160
I read somewhere.

91
00:06:37,160 --> 00:06:42,560
I don't know if any of you know anything about Taiwan.

92
00:06:42,560 --> 00:06:47,200
I just wonder if that where they succeed.

93
00:06:47,280 --> 00:06:48,280
Successful?

94
00:06:51,480 --> 00:06:53,680
Is there a lot of research there?

95
00:06:53,680 --> 00:06:54,180
No.

96
00:06:56,680 --> 00:06:59,600
Very often, when a government decides on the right thing

97
00:06:59,600 --> 00:07:00,880
to do, it doesn't work.

98
00:07:00,880 --> 00:07:06,760
Because I had some friends in Italy

99
00:07:06,760 --> 00:07:09,360
who were trying to start an AI group.

100
00:07:09,360 --> 00:07:16,000
And they had accumulated a critical mass in what's

101
00:07:16,000 --> 00:07:21,600
a big city in the north, Milan.

102
00:07:21,600 --> 00:07:26,600
And then some government committee said,

103
00:07:26,600 --> 00:07:30,440
oh, there's a bunch of computer scientists there,

104
00:07:30,440 --> 00:07:34,840
but there's no good computer scientists in Pisa and Verona.

105
00:07:34,840 --> 00:07:41,480
So the government can order a professor to leave one place

106
00:07:41,480 --> 00:07:43,400
and go somewhere else.

107
00:07:43,400 --> 00:07:47,040
So the next year, there were no groups.

108
00:07:47,040 --> 00:07:51,440
And occasionally, there are people like Isaac Newton

109
00:07:51,440 --> 00:07:52,800
who like to work alone.

110
00:07:56,120 --> 00:08:03,560
But I got the impression that the product of the Italian

111
00:08:03,560 --> 00:08:06,280
researchers diminished after that.

112
00:08:06,280 --> 00:08:07,040
Might be wrong.

113
00:08:08,040 --> 00:08:13,000
I'll bet a more technical question.

114
00:08:23,600 --> 00:08:24,120
Thanks.

115
00:08:24,120 --> 00:08:29,280
You had a complicated diagram concerning story.

116
00:08:29,280 --> 00:08:32,160
Do you recall the many layers?

117
00:08:32,160 --> 00:08:34,040
Yeah.

118
00:08:34,040 --> 00:08:36,520
Was that meant to be a bi-directional diagram

119
00:08:36,520 --> 00:08:40,600
or does it work from the bottom up and goes to the top down?

120
00:08:40,600 --> 00:08:44,560
I'm confused about whether that, let's see if I can find it.

121
00:08:44,560 --> 00:08:45,840
Why did this shut down?

122
00:08:51,240 --> 00:08:52,960
Do I dare press Start?

123
00:08:52,960 --> 00:08:55,120
It's alive on the screen.

124
00:08:55,120 --> 00:08:58,320
Oh my gosh.

125
00:08:58,320 --> 00:09:00,120
I never saw that phenomenon before.

126
00:09:06,520 --> 00:09:09,000
Contact displays?

127
00:09:09,000 --> 00:09:11,000
Yeah, I can't.

128
00:09:11,000 --> 00:09:14,000
Is that a shift button that changes?

129
00:09:14,000 --> 00:09:16,000
Oh, there we are.

130
00:09:16,000 --> 00:09:16,920
What?

131
00:09:16,920 --> 00:09:17,680
Did it go on?

132
00:09:17,680 --> 00:09:18,680
It's a strange person.

133
00:09:27,640 --> 00:09:29,640
Oh, it's on.

134
00:09:29,640 --> 00:09:30,120
Oh, well.

135
00:09:37,520 --> 00:09:40,520
It might be in this random lecture.

136
00:09:51,520 --> 00:09:52,680
How do I get rid of those?

137
00:10:02,320 --> 00:10:04,120
Yes, there's a sort of bug in the.

138
00:10:07,160 --> 00:10:11,520
In the toolbox thing on the Macintosh,

139
00:10:11,520 --> 00:10:14,760
which is if you make one of these too long,

140
00:10:14,760 --> 00:10:18,960
there's no way to get rid of it except to restart

141
00:10:18,960 --> 00:10:21,680
the machine in some other mode.

142
00:10:21,680 --> 00:10:23,480
I can't catch it.

143
00:10:23,480 --> 00:10:28,400
Anyway, maybe this works.

144
00:10:28,400 --> 00:10:37,280
Oh, well.

145
00:10:37,280 --> 00:10:40,120
That diagram.

146
00:10:40,120 --> 00:10:41,840
There's two hierarchical diagrams.

147
00:10:41,840 --> 00:10:44,800
The theme of the motion machine book

148
00:10:44,800 --> 00:10:51,200
is mostly the six layers of instinctive built-in reactions,

149
00:10:51,200 --> 00:10:57,480
learned conditioned reactions, and going up

150
00:10:57,480 --> 00:11:01,160
to reflective and self-reflective and so on.

151
00:11:01,160 --> 00:11:05,800
And the other diagram starts out with just a neural net

152
00:11:05,800 --> 00:11:08,520
and then things like K lines, which

153
00:11:08,520 --> 00:11:12,640
are ways to organize groups of activities

154
00:11:12,640 --> 00:11:16,240
and then frames and trance frames.

155
00:11:16,240 --> 00:11:21,480
The trance frame is a way of representing knowledge

156
00:11:21,480 --> 00:11:26,720
in terms of how action affects a situation

157
00:11:26,720 --> 00:11:31,400
or a particular situation and action produces a new one.

158
00:11:31,400 --> 00:11:36,040
And then a story is usually a chain of trance frames.

159
00:11:36,040 --> 00:11:37,720
And of course, a meaningful story

160
00:11:37,720 --> 00:11:42,680
is one which I didn't have a level for.

161
00:11:42,680 --> 00:11:44,840
Good stories and useless stories.

162
00:11:45,840 --> 00:11:52,920
So somewhere at a very high level,

163
00:11:52,920 --> 00:11:55,840
we all have knowledge of, if you're

164
00:11:55,840 --> 00:12:01,280
facing some sort of problem, what kind of strategy

165
00:12:01,280 --> 00:12:05,280
might be good for solving that kind of problem.

166
00:12:05,280 --> 00:12:09,920
And in that case, each layer is made

167
00:12:09,920 --> 00:12:17,400
of things in the lower layer, whereas in the society of mind

168
00:12:17,400 --> 00:12:21,400
hierarchy, each layer does different things

169
00:12:21,400 --> 00:12:24,600
that operates on the result of the other layers.

170
00:12:24,600 --> 00:12:29,640
So there's sort of, I guess if you look at any mechanism,

171
00:12:29,640 --> 00:12:32,560
you'll have a diagram of what the parts do

172
00:12:32,560 --> 00:12:33,480
and how they relate.

173
00:12:33,480 --> 00:12:36,720
And you'll have a diagram of which

174
00:12:36,720 --> 00:12:39,000
isn't in the machinery of what are

175
00:12:39,000 --> 00:12:41,840
the functions of the different sets of parts

176
00:12:41,840 --> 00:12:44,400
and how are those functions related.

177
00:12:44,400 --> 00:12:49,640
So that might be a bug in both books

178
00:12:49,640 --> 00:12:55,760
that I drew the diagrams to look pretty similar.

179
00:12:55,760 --> 00:13:00,400
And it's a bad analogy.

180
00:13:00,400 --> 00:13:00,920
Yeah.

181
00:13:00,920 --> 00:13:03,720
Yeah, it's certainly interesting.

182
00:13:03,720 --> 00:13:05,360
There's a stimulus response model

183
00:13:05,360 --> 00:13:08,640
where if you put a story beneath it,

184
00:13:08,640 --> 00:13:10,840
where the interpretive mechanism.

185
00:13:10,840 --> 00:13:12,680
But does it flow the other way?

186
00:13:12,680 --> 00:13:15,640
Is it generative from bottom to top as well?

187
00:13:15,640 --> 00:13:20,600
Well, in some sense, this trance frame says,

188
00:13:20,600 --> 00:13:22,720
here's a piece of knowledge which says,

189
00:13:22,720 --> 00:13:25,200
if you're in such a situation, this

190
00:13:25,200 --> 00:13:28,600
is a way to get to another situation.

191
00:13:28,600 --> 00:13:34,920
In the traditional behavioristic, behaviorist

192
00:13:34,920 --> 00:13:42,100
is a word for the class of generations of psychologists

193
00:13:42,100 --> 00:13:45,360
who try to explain behavior just in terms

194
00:13:45,360 --> 00:13:49,160
of reacting to situations.

195
00:13:49,160 --> 00:13:58,160
And that wasn't connected to what am I trying to say?

196
00:13:58,160 --> 00:14:04,040
In the standard behaviorist models,

197
00:14:04,040 --> 00:14:07,620
which were sort of occupied most of psychology

198
00:14:07,620 --> 00:14:13,680
from the 19th century up to the 1950s

199
00:14:13,680 --> 00:14:21,120
when modern cognitive psychology really started,

200
00:14:21,120 --> 00:14:25,640
you just looked at the animal as a collection of reactions.

201
00:14:25,640 --> 00:14:27,600
And then in cognitive psychology,

202
00:14:27,600 --> 00:14:32,720
you start to look at the animal as having goals and problems.

203
00:14:32,720 --> 00:14:38,520
And then some machinery is used to go

204
00:14:38,520 --> 00:14:44,680
from the way you describe your situation

205
00:14:44,680 --> 00:14:48,880
to generating a plan for what you're going to do about that.

206
00:14:48,880 --> 00:14:52,280
And then the plan ends up being made of little actions,

207
00:14:52,280 --> 00:14:53,320
of course.

208
00:14:53,320 --> 00:15:03,360
But before 1950, there were only a few psychologists who,

209
00:15:03,360 --> 00:15:06,280
and philosophers, I should say, going all the way back

210
00:15:06,280 --> 00:15:12,600
to people like David Hume and Spinoza and maybe

211
00:15:12,600 --> 00:15:17,600
Immanuel Kant, they made up.

212
00:15:17,600 --> 00:15:22,400
If you read their stuff and ignore the philosophy,

213
00:15:22,400 --> 00:15:26,800
you see that there was a very slow progress over, really,

214
00:15:26,800 --> 00:15:34,400
three centuries of trying to get from logic, which sort of first

215
00:15:34,400 --> 00:15:37,040
appears around the time of Leibniz.

216
00:15:37,040 --> 00:15:38,280
When is Leibniz?

217
00:15:38,280 --> 00:15:40,280
1650 or so?

218
00:15:46,240 --> 00:15:48,560
Yes, they never met, I believe.

219
00:15:49,560 --> 00:15:58,760
So a lot of philosophy has, I don't know

220
00:15:58,760 --> 00:16:01,840
how to describe the rest of it, but a lot of it

221
00:16:01,840 --> 00:16:07,160
is trying to make high-level theories of how thinking works.

222
00:16:07,160 --> 00:16:10,160
And it's, of course, mixed with all sorts of problems

223
00:16:10,160 --> 00:16:13,560
about why the world exists and ethics

224
00:16:13,560 --> 00:16:16,400
and what are good things to do and bad

225
00:16:16,400 --> 00:16:20,240
and all sorts of mixed-up things.

226
00:16:20,240 --> 00:16:23,040
And psychology doesn't appear.

227
00:16:23,040 --> 00:16:26,240
I don't think there is a name for that field

228
00:16:26,240 --> 00:16:32,280
until the 1880s or so.

229
00:16:32,280 --> 00:16:36,640
Who's the first psychologist you can think of?

230
00:16:36,640 --> 00:16:38,120
William James?

231
00:16:38,120 --> 00:16:42,840
William James is around 1890.

232
00:16:42,840 --> 00:16:46,280
There's a guy named Wundt in Austria, I think.

233
00:16:46,280 --> 00:16:51,400
Sigmund Freud starts publishing around 1890.

234
00:16:51,400 --> 00:16:56,540
Francis Galton in England is maybe the first recognizable

235
00:16:56,540 --> 00:16:57,880
psychologist.

236
00:16:57,880 --> 00:17:03,100
He has a big book called An Inquiry into Human Faculty,

237
00:17:03,100 --> 00:17:09,000
which makes good reading right now,

238
00:17:09,000 --> 00:17:13,680
because each chapter is about a different aspect of what

239
00:17:13,680 --> 00:17:16,520
would be called modern cognitive psychology.

240
00:17:16,520 --> 00:17:18,240
How do people recognize things?

241
00:17:20,760 --> 00:17:25,520
What kinds of memory cues do you use to retrieve stuff?

242
00:17:25,520 --> 00:17:34,360
All sorts of sort of term papers, the chapters,

243
00:17:34,360 --> 00:17:35,960
some little theory.

244
00:17:35,960 --> 00:17:37,800
And you'd say, I can do better than that.

245
00:17:37,800 --> 00:17:39,400
And indeed, you could.

246
00:17:39,400 --> 00:17:41,960
But at that time, no one could.

247
00:17:41,960 --> 00:17:42,920
Yes?

248
00:17:42,920 --> 00:17:45,800
What psychology is thinking about how people think,

249
00:17:45,800 --> 00:17:48,240
which I think people have been dealing with in Aristotle

250
00:17:48,240 --> 00:17:51,040
and in a lot of them?

251
00:17:51,040 --> 00:17:56,280
Aristotle has more good ideas than, as far as I'm concerned,

252
00:17:56,280 --> 00:18:01,880
everyone else put together for the next 1,000 years.

253
00:18:01,880 --> 00:18:04,000
It's just very remarkable.

254
00:18:04,000 --> 00:18:07,040
And we don't know anything about that,

255
00:18:07,040 --> 00:18:10,060
because there are no manuscripts.

256
00:18:10,060 --> 00:18:10,560
Anybody?

257
00:18:13,700 --> 00:18:18,020
There's that wonderful play by, who's the Italian?

258
00:18:25,020 --> 00:18:27,940
What?

259
00:18:27,940 --> 00:18:29,020
No, no, a recent one.

260
00:18:29,020 --> 00:18:29,520
No.

261
00:18:35,380 --> 00:18:39,380
No, he's sort of contemporary.

262
00:18:40,020 --> 00:18:43,900
Anyway, he has a play about searching for the lost.

263
00:18:47,140 --> 00:18:51,580
There's some record that Aristotle had a book of jokes,

264
00:18:51,580 --> 00:18:54,140
or rather a book.

265
00:18:54,140 --> 00:18:57,260
He has books on ethics and things like that.

266
00:18:57,260 --> 00:19:00,620
And there's a book about humor, which is lost.

267
00:19:00,620 --> 00:19:03,820
And most scholars think it's not important,

268
00:19:03,820 --> 00:19:10,580
because if you look at the 10 existing books on Aristotle,

269
00:19:10,580 --> 00:19:14,020
I think there's about 10 allegedly by him.

270
00:19:14,020 --> 00:19:15,820
They're students' notes.

271
00:19:15,820 --> 00:19:22,180
And almost every subject appears in at least two of them anyway.

272
00:19:22,180 --> 00:19:26,060
So one conjecture is that there really isn't any very much

273
00:19:26,060 --> 00:19:29,380
lost from ancient times.

274
00:19:30,260 --> 00:19:41,980
Anyway, if you ever read books, you might as well

275
00:19:41,980 --> 00:19:48,820
read one or two of Aristotle's, because the translations,

276
00:19:48,820 --> 00:19:50,500
I'm told, are pretty good.

277
00:19:50,500 --> 00:19:56,100
And you can actually get ideas from it.

278
00:19:56,100 --> 00:19:57,940
Yes?

279
00:19:57,940 --> 00:20:02,180
I don't know if you ever heard about the theory of Giulio

280
00:20:02,180 --> 00:20:02,940
Tonini.

281
00:20:02,940 --> 00:20:07,540
Humberto Eco is the writer.

282
00:20:07,540 --> 00:20:09,220
Sorry.

283
00:20:09,220 --> 00:20:10,420
How does memory work?

284
00:20:10,420 --> 00:20:13,980
Something about your extension.

285
00:20:13,980 --> 00:20:15,140
Sorry.

286
00:20:15,140 --> 00:20:19,380
Giulio Tonini, he tries to explain consciousness.

287
00:20:19,380 --> 00:20:23,020
You say that consciousness is a suitcase word.

288
00:20:23,020 --> 00:20:27,020
But I don't quite agree with his definition.

289
00:20:27,060 --> 00:20:31,740
But basically, his definition is that the more

290
00:20:31,740 --> 00:20:37,220
the information is integrated, the more conscious being is.

291
00:20:40,060 --> 00:20:42,500
The more information you have?

292
00:20:42,500 --> 00:20:45,940
The more integrated the information is.

293
00:20:45,940 --> 00:20:51,420
So for example, I don't know.

294
00:20:51,420 --> 00:20:54,540
He gives an example of a MacBook that has a lot of information

295
00:20:54,540 --> 00:20:56,460
that's not integrated.

296
00:20:56,460 --> 00:21:02,060
It's not correlated, and so it's not very conscious.

297
00:21:02,060 --> 00:21:03,860
That sounds like an important idea,

298
00:21:03,860 --> 00:21:06,980
and there ought to be a name for it.

299
00:21:06,980 --> 00:21:08,100
Yeah, it has a name.

300
00:21:08,100 --> 00:21:09,820
I think it's IDT.

301
00:21:09,820 --> 00:21:15,340
And this guy is a neuroscientist and a psychologist.

302
00:21:15,340 --> 00:21:19,420
And he sees some edge cases of people

303
00:21:19,420 --> 00:21:22,660
that split the brain in half.

304
00:21:22,660 --> 00:21:27,260
And it seems that both halves are kind of conscious.

305
00:21:27,260 --> 00:21:31,940
But I don't agree, because that people, they still

306
00:21:31,940 --> 00:21:34,260
have the information that's integrated.

307
00:21:34,260 --> 00:21:36,940
But it seems that they are not conscious.

308
00:21:36,940 --> 00:21:40,460
So there must be some action into that information,

309
00:21:40,460 --> 00:21:42,980
even if it's passive or active.

310
00:21:42,980 --> 00:21:45,540
But it seems very interesting.

311
00:21:45,540 --> 00:21:53,780
Well, which of my 30 features that go into that suitcase

312
00:21:53,780 --> 00:21:55,980
do they have?

313
00:21:55,980 --> 00:21:58,820
It doesn't make any sense to say something is conscious or not,

314
00:21:58,820 --> 00:22:00,340
does it?

315
00:22:00,340 --> 00:22:07,860
You just said it yourself, that there's

316
00:22:07,860 --> 00:22:10,980
some degree of integration, perhaps.

317
00:22:10,980 --> 00:22:13,620
Yeah, yeah, yeah.

318
00:22:13,620 --> 00:22:17,180
But can you say what you mean by integration?

319
00:22:17,180 --> 00:22:19,420
You probably need to say 20 things,

320
00:22:19,420 --> 00:22:22,940
and many of them might be independent.

321
00:22:27,540 --> 00:22:31,420
Here's an example of something.

322
00:22:31,460 --> 00:22:41,220
Many years ago, people in the 1950s and 60s,

323
00:22:41,220 --> 00:22:47,260
it was very popular to talk about the left and right brain.

324
00:22:47,260 --> 00:22:49,180
Have you heard people say, what's

325
00:22:49,180 --> 00:22:52,340
the difference between the left brain and the right brain?

326
00:22:52,340 --> 00:22:55,780
It's supposed to be cream versus rational.

327
00:22:55,780 --> 00:22:59,460
Rational versus emotional?

328
00:22:59,460 --> 00:23:02,420
Now, I haven't heard anybody discuss that

329
00:23:02,420 --> 00:23:05,180
for the last 15 or 20 years.

330
00:23:05,180 --> 00:23:08,100
Although it seems to have become really a mesh

331
00:23:08,100 --> 00:23:09,500
with popular culture now.

332
00:23:09,500 --> 00:23:11,700
If you ask anybody what they know about the brain,

333
00:23:11,700 --> 00:23:13,100
one of the first things they'll say

334
00:23:13,100 --> 00:23:15,060
is, well, I'm more of a right brain person

335
00:23:15,060 --> 00:23:16,020
or a left brain person.

336
00:23:16,020 --> 00:23:17,540
That seems to be a sticking thing.

337
00:23:17,540 --> 00:23:20,500
They used to, but I haven't heard that

338
00:23:20,500 --> 00:23:23,380
for at least 15 years.

339
00:23:23,380 --> 00:23:26,300
I have not heard a single person, psychologists,

340
00:23:26,300 --> 00:23:27,380
mention it.

341
00:23:27,380 --> 00:23:29,100
Have you?

342
00:23:29,100 --> 00:23:32,060
I think fMRI has all the obsolete of that theory.

343
00:23:35,340 --> 00:23:36,900
There's one thing that's good for it.

344
00:23:36,900 --> 00:23:38,940
It's disproving that.

345
00:23:38,940 --> 00:23:41,100
Anyway, I mentioned in the Society of Mind,

346
00:23:41,100 --> 00:23:45,080
I think I had a grumble about it, which is that,

347
00:23:45,080 --> 00:23:50,300
as far as I can tell, it appears to be true

348
00:23:50,300 --> 00:23:55,660
that language is located in most people

349
00:23:55,660 --> 00:24:00,020
in two very definite areas in the left brain,

350
00:24:00,020 --> 00:24:03,980
but occasionally in the right brain of some people.

351
00:24:03,980 --> 00:24:07,140
But other than that, as far as I can see,

352
00:24:07,140 --> 00:24:09,500
when you actually catalog the differences

353
00:24:09,500 --> 00:24:18,180
that the psychologists reported in the 1960s and 70s,

354
00:24:18,180 --> 00:24:20,220
then the things in the left brain

355
00:24:20,220 --> 00:24:24,780
were largely adult kinds of thinking,

356
00:24:24,780 --> 00:24:30,020
and the things in the right brain were largely childish.

357
00:24:30,020 --> 00:24:32,500
It wasn't that they were rational or not.

358
00:24:32,500 --> 00:24:39,100
It was that they weren't very hierarchical and tower-like.

359
00:24:39,100 --> 00:24:44,700
And I think there was a nice romantic idea

360
00:24:44,700 --> 00:24:48,180
of contrasting emotions and intellect

361
00:24:48,180 --> 00:24:54,700
and all those dumbbell distinctions.

362
00:24:54,700 --> 00:24:57,460
And projecting them onto the brain.

363
00:24:57,460 --> 00:25:01,740
But I don't know what started me on that track.

364
00:25:01,740 --> 00:25:04,580
But it's interesting that it was very, very popular,

365
00:25:04,580 --> 00:25:07,580
and psychologists talked about it all the time

366
00:25:07,580 --> 00:25:09,500
when I was a student.

367
00:25:09,500 --> 00:25:15,260
And I haven't seen it mentioned by any cognitive psychologist

368
00:25:15,260 --> 00:25:21,420
for, yeah?

369
00:25:21,420 --> 00:25:23,500
So he mentioned his theory.

370
00:25:23,500 --> 00:25:29,780
But I believe we don't test our theory with edge cases.

371
00:25:29,780 --> 00:25:34,820
So mental ill people, or people that, probably

372
00:25:34,820 --> 00:25:37,900
there are a lot of people that, not a lot,

373
00:25:37,900 --> 00:25:40,420
but some percentage of people that are mental ill

374
00:25:40,420 --> 00:25:45,700
or don't form so well in some part of the brain.

375
00:25:45,700 --> 00:25:51,340
And maybe we can have some idea of what

376
00:25:51,500 --> 00:25:56,300
consciousness is, just by seeing people

377
00:25:56,300 --> 00:25:59,940
that don't have some part of the brain that might interfere

378
00:25:59,940 --> 00:26:02,540
with something.

379
00:26:02,540 --> 00:26:09,220
Like the split brain may give a reason why what's consciousness.

380
00:26:09,220 --> 00:26:13,380
Because maybe some half of brain,

381
00:26:13,380 --> 00:26:15,740
like both halves are consciousness.

382
00:26:15,740 --> 00:26:20,340
But I don't understand what you're trying to,

383
00:26:20,340 --> 00:26:22,780
you're trying to construct a meaning for the word

384
00:26:22,780 --> 00:26:25,140
consciousness.

385
00:26:25,140 --> 00:26:30,300
Well, Tononi is definitely onto something interesting.

386
00:26:30,300 --> 00:26:33,460
And I think the reason that he uses the word consciousness

387
00:26:33,460 --> 00:26:36,500
is that it's in the sense that people talk about losing

388
00:26:36,500 --> 00:26:38,020
or regaining it.

389
00:26:38,020 --> 00:26:43,660
And so he can actually experimentally test his theory.

390
00:26:43,660 --> 00:26:47,140
People who are asleep, or in a coma, or dreaming,

391
00:26:47,140 --> 00:26:50,780
or locked in a persistent vegetative state,

392
00:26:50,780 --> 00:26:52,860
in each of those cases, his theory actually

393
00:26:52,860 --> 00:26:56,660
agrees with the common sense idea of whether this person is

394
00:26:56,660 --> 00:26:59,340
conscious in a temporary way.

395
00:26:59,340 --> 00:27:03,020
But then, is that different from,

396
00:27:03,020 --> 00:27:05,700
if you use the word thinking instead,

397
00:27:05,700 --> 00:27:07,460
you could say when somebody is in a coma,

398
00:27:07,460 --> 00:27:10,100
they're not thinking and they're not there.

399
00:27:10,100 --> 00:27:11,580
I don't think that it's good for him

400
00:27:11,580 --> 00:27:13,180
to use the word consciousness.

401
00:27:13,180 --> 00:27:15,740
And I think that the word consciousness, to many people,

402
00:27:15,740 --> 00:27:18,580
refers to a lot of things that his theory does not

403
00:27:18,580 --> 00:27:19,860
treat at all.

404
00:27:19,860 --> 00:27:24,540
See, it's really dangerous if you like,

405
00:27:24,540 --> 00:27:26,380
is it Pinker who likes consciousness?

406
00:27:26,380 --> 00:27:28,940
I forget.

407
00:27:28,940 --> 00:27:34,100
It's dangerous to feel sure that there's

408
00:27:34,100 --> 00:27:37,540
something very important, and a central mystery,

409
00:27:37,540 --> 00:27:43,100
and what does he call it, the hard problem of psychology.

410
00:27:43,100 --> 00:27:49,180
And so here's a really very smart guy, Steven Pinker.

411
00:27:49,180 --> 00:27:52,340
And as far as I can see, he does nothing but harm

412
00:27:52,340 --> 00:27:55,940
to the people he talks to, because he gets them

413
00:27:55,940 --> 00:28:00,420
to do bad experiments and waste their time.

414
00:28:00,420 --> 00:28:05,620
So instead of trying to revive consciousness,

415
00:28:05,620 --> 00:28:08,480
it's worth considering that might be a very bad thing

416
00:28:08,480 --> 00:28:12,060
to do to yourself and other people.

417
00:28:12,060 --> 00:28:14,860
What problem are you trying to solve?

418
00:28:14,860 --> 00:28:18,880
Is there any way, or the problem of qualia, for example?

419
00:28:18,880 --> 00:28:23,780
Because the standard view, and this

420
00:28:23,780 --> 00:28:29,300
is something that still is a serious disease even today

421
00:28:29,300 --> 00:28:31,180
in philosophy.

422
00:28:31,180 --> 00:28:36,740
That is, the idea that the redness of red things

423
00:28:36,740 --> 00:28:40,020
is a very fundamental thing.

424
00:28:40,020 --> 00:28:41,220
It's indivisible.

425
00:28:41,220 --> 00:28:43,500
It's not describable.

426
00:28:43,500 --> 00:28:47,660
It's like, to those philosophers,

427
00:28:47,660 --> 00:28:54,300
that's just as important as when, who was the Greek?

428
00:28:54,300 --> 00:28:55,540
Democritus, was it?

429
00:28:55,540 --> 00:28:56,860
Who discovered atoms?

430
00:28:56,860 --> 00:28:58,980
Yeah, Democritus.

431
00:28:58,980 --> 00:29:02,140
The idea of atoms was an enormous breakthrough.

432
00:29:02,140 --> 00:29:09,820
Of course, it took 2,000 years before people realized that,

433
00:29:09,820 --> 00:29:13,720
yes, there are atoms, and they're not.

434
00:29:13,720 --> 00:29:15,580
They're actually complicated systems

435
00:29:15,580 --> 00:29:20,980
made of quarks and five or 10 other things.

436
00:29:20,980 --> 00:29:22,740
So now we don't have atoms anymore.

437
00:29:25,300 --> 00:29:31,380
But I think Pinker has the idea that red is irreducible.

438
00:29:31,380 --> 00:29:32,540
And you can't describe it.

439
00:29:32,540 --> 00:29:35,860
It's like the atom of thought.

440
00:29:35,860 --> 00:29:38,820
And these qualia are the fundamental problem

441
00:29:38,820 --> 00:29:40,060
of psychology.

442
00:29:40,060 --> 00:29:42,220
To me, it's exactly the opposite.

443
00:29:42,220 --> 00:29:46,420
Why do we have a word for it?

444
00:29:46,420 --> 00:29:51,060
When I say red, do you experience the same thing

445
00:29:51,060 --> 00:29:52,700
as anyone else who says red?

446
00:29:52,700 --> 00:29:55,900
And it seems to me that somebody who

447
00:29:55,900 --> 00:30:02,380
got sick after eating a tomato has a different qualia for red.

448
00:30:06,060 --> 00:30:09,420
Blood, violent things, bad.

449
00:30:09,420 --> 00:30:12,820
Maybe another child has all sorts

450
00:30:12,820 --> 00:30:16,660
of pleasant associations with things that are red.

451
00:30:16,660 --> 00:30:20,380
And the concept of red is it's not

452
00:30:20,380 --> 00:30:24,020
that it's inexpressible because it's indivisible.

453
00:30:24,020 --> 00:30:26,500
It's inexpressible because it's connected

454
00:30:26,500 --> 00:30:31,780
with thousands of other ideas and experiences.

455
00:30:31,780 --> 00:30:35,380
And therefore, there's no way to make a compact definition

456
00:30:36,340 --> 00:30:36,380
of it.

457
00:30:36,380 --> 00:30:38,220
But it's exactly the opposite.

458
00:30:38,220 --> 00:30:40,380
It's not the hard problem of psychology.

459
00:30:46,900 --> 00:30:49,820
It's something that will fall out automatically

460
00:30:49,820 --> 00:30:52,780
without any effort when you have a pretty good theory

461
00:30:52,780 --> 00:30:55,540
of psychology.

462
00:30:55,540 --> 00:31:00,540
But why do we have this qualia of things that are red?

463
00:31:00,540 --> 00:31:03,220
Why do we have descriptions of things?

464
00:31:03,220 --> 00:31:06,460
Because the animals that don't have compact descriptions

465
00:31:06,460 --> 00:31:11,820
of things get eaten very quickly because they can't recognize

466
00:31:11,820 --> 00:31:14,060
things that might hurt them.

467
00:31:14,060 --> 00:31:15,780
It's very important to have machinery

468
00:31:15,780 --> 00:31:19,060
for recognizing real things.

469
00:31:19,060 --> 00:31:22,180
And real things have features.

470
00:31:22,180 --> 00:31:28,260
In fact, there is such a thing as redness, namely

471
00:31:28,300 --> 00:31:34,060
the frequencies of light of, what, around 400 nanometers?

472
00:31:34,060 --> 00:31:34,980
What's the frequency?

473
00:31:34,980 --> 00:31:36,460
700 nanometers.

474
00:31:36,460 --> 00:31:37,380
What?

475
00:31:37,380 --> 00:31:39,260
Other end, 700 nanometers.

476
00:31:39,260 --> 00:31:39,780
That far?

477
00:31:39,780 --> 00:31:41,660
That's infrared, isn't it?

478
00:31:41,660 --> 00:31:43,500
A little bit, 650, 680.

479
00:31:43,500 --> 00:31:48,580
Anyway, one of the things somebody pointed out to me

480
00:31:48,580 --> 00:31:53,420
in later life is that there's only one yellow.

481
00:31:53,420 --> 00:31:54,740
There are a lot of shades of red.

482
00:31:58,260 --> 00:32:01,860
Interesting how tiny the yellow spectrum is.

483
00:32:01,860 --> 00:32:02,940
I don't know what it means.

484
00:32:07,860 --> 00:32:13,780
If you look around a room, I don't see a single one.

485
00:32:13,780 --> 00:32:16,380
If you saw one, it might be a lion.

486
00:32:16,380 --> 00:32:17,420
Is there a what?

487
00:32:17,420 --> 00:32:19,140
It might be a lion.

488
00:32:19,140 --> 00:32:20,980
Oh, a lion.

489
00:32:20,980 --> 00:32:23,700
Yes, does anybody see anything yellow in here?

490
00:32:23,700 --> 00:32:26,380
It's probably really hard to generate the frequency

491
00:32:26,380 --> 00:32:27,300
of the yellow light.

492
00:32:27,300 --> 00:32:29,780
I can't do it because I have a lot of money.

493
00:32:32,780 --> 00:32:35,300
Yes, what element has a bright yellow line?

494
00:32:38,620 --> 00:32:39,660
Sodium.

495
00:32:39,660 --> 00:32:41,580
Sodium.

496
00:32:41,580 --> 00:32:42,700
Yeah, orange-ish.

497
00:32:42,700 --> 00:32:44,500
It's orange, yeah.

498
00:32:44,500 --> 00:32:46,420
Yellow is the sun.

499
00:32:46,420 --> 00:32:47,780
Yes.

500
00:32:47,780 --> 00:32:50,340
Maybe that's very important.

501
00:32:50,340 --> 00:32:52,820
Probably.

502
00:32:53,100 --> 00:32:57,060
There's yellow chalk in the bin.

503
00:32:57,060 --> 00:32:58,460
Yeah, yeah, that's right.

504
00:32:58,460 --> 00:33:00,740
It's in the bin.

505
00:33:00,740 --> 00:33:01,740
That's great.

506
00:33:01,740 --> 00:33:05,740
These lights are really hot.

507
00:33:05,740 --> 00:33:11,220
So this color is called warm white.

508
00:33:11,220 --> 00:33:13,660
There's a yellow one.

509
00:33:13,660 --> 00:33:14,860
In the store, yeah.

510
00:33:14,860 --> 00:33:18,340
Wow, you're on the table.

511
00:33:18,340 --> 00:33:19,740
Warm white.

512
00:33:19,740 --> 00:33:21,580
Warm white.

513
00:33:21,580 --> 00:33:24,100
What is it in Finland?

514
00:33:24,100 --> 00:33:25,540
I don't know.

515
00:33:25,540 --> 00:33:31,900
It's called the lights like that which comes from the tungsten.

516
00:33:31,900 --> 00:33:37,300
They're awful, outlawed tungsten light bulbs,

517
00:33:37,300 --> 00:33:39,220
which are not tungsten anymore.

518
00:33:39,220 --> 00:33:41,700
Yes, that's right.

519
00:33:41,700 --> 00:33:45,740
I've stocked up on 20 watt tungsten bulbs

520
00:33:45,740 --> 00:33:48,540
because my house is full of fluorescent bulbs

521
00:33:48,540 --> 00:33:52,060
that are remote controlled by things.

522
00:33:52,060 --> 00:33:55,740
And if there's no incandescent bulb in one of the sockets,

523
00:33:55,740 --> 00:33:59,980
then the remote controller breaks.

524
00:33:59,980 --> 00:34:04,900
These are the things you buy with what are they called?

525
00:34:09,140 --> 00:34:11,020
Little units that.

526
00:34:11,020 --> 00:34:11,980
X10.

527
00:34:11,980 --> 00:34:14,220
X10, right.

528
00:34:14,220 --> 00:34:18,500
The old X10 units are the receivers

529
00:34:18,500 --> 00:34:23,380
burn out if there's no resistive load on them.

530
00:34:23,380 --> 00:34:29,500
So I have to have enough incandescent bulbs

531
00:34:29,500 --> 00:34:33,380
for the next 20 years or get rid of the X10s.

532
00:34:38,220 --> 00:34:41,740
I think they're illegal in Japan, aren't they?

533
00:34:41,740 --> 00:34:44,300
In Japan, they are still out there.

534
00:34:44,300 --> 00:34:46,140
They're still there?

535
00:34:46,140 --> 00:34:49,380
You can still find them in some shops,

536
00:34:49,380 --> 00:34:54,740
and people buy them so that they can still get 70-year-olds

537
00:34:54,740 --> 00:34:56,620
and get a lot of something else.

538
00:34:56,620 --> 00:34:59,300
I bought a lot of LED light bulbs

539
00:34:59,300 --> 00:35:01,620
at the swap fest the other day.

540
00:35:04,980 --> 00:35:05,740
Back to AI.

541
00:35:06,740 --> 00:35:08,740
So in the reading, you seem to imply

542
00:35:08,740 --> 00:35:13,740
that evolution is a bad strategy for creating AI because, one,

543
00:35:13,740 --> 00:35:15,740
it'll take a lot of time, and two,

544
00:35:15,740 --> 00:35:18,740
because it'll get stuck in local minima maximum.

545
00:35:18,740 --> 00:35:22,740
But if we had infinite time and enough mutation,

546
00:35:22,740 --> 00:35:24,740
do you think it would be possible to create

547
00:35:24,740 --> 00:35:27,740
a good artificial intelligence using evolution?

548
00:35:27,740 --> 00:35:32,740
Well, if there's somebody in charge,

549
00:35:32,740 --> 00:35:40,900
if there's somebody in charge, if you have evolution

550
00:35:40,900 --> 00:35:45,220
on a big planet, then you get a lot of life forms.

551
00:35:45,220 --> 00:35:50,520
And so the problem is that you might

552
00:35:50,520 --> 00:35:55,220
have some really stupid life form that eats the smart ones.

553
00:35:56,100 --> 00:36:02,860
So but I have a more serious objection to evolution.

554
00:36:02,860 --> 00:36:07,220
See, there have been several projects in the last,

555
00:36:07,220 --> 00:36:10,380
well, since computer science started,

556
00:36:10,380 --> 00:36:16,820
of trying to make problem solvers smart

557
00:36:16,820 --> 00:36:22,980
by imitating evolution, which is variation and selection.

558
00:36:22,980 --> 00:36:28,780
So I know of about five or six such projects which were

559
00:36:28,780 --> 00:36:31,100
fairly well-funded and serious.

560
00:36:34,140 --> 00:36:36,860
Most interesting, maybe, was the one

561
00:36:36,860 --> 00:36:42,820
of Doug Lenitz, which was just him by himself.

562
00:36:42,820 --> 00:36:48,980
So if you look up Douglas Lenitz's thesis, which

563
00:36:48,980 --> 00:36:56,580
was called AM, Automated Mathematician,

564
00:36:56,580 --> 00:37:02,380
and a second publication called Eurisco, E-U-R-I-S-K-O,

565
00:37:02,380 --> 00:37:08,260
those were projects in which he did variation and selection.

566
00:37:08,260 --> 00:37:11,680
And he imitated chromosomes by having strings

567
00:37:11,680 --> 00:37:17,940
of simple operations, which were usually things like adding

568
00:37:17,940 --> 00:37:22,980
and subtracting and conditional jump and so forth.

569
00:37:22,980 --> 00:37:26,180
But there are several bugs with organic evolution.

570
00:37:26,180 --> 00:37:33,940
And the most serious one, which is that evolution doesn't

571
00:37:33,940 --> 00:37:37,740
remember what killed the losers.

572
00:37:37,740 --> 00:37:41,580
So there's no record in the genes

573
00:37:41,580 --> 00:37:45,260
of the mutations which were lethal.

574
00:37:45,260 --> 00:37:50,380
And in fact, it's almost the opposite.

575
00:37:50,380 --> 00:37:55,580
I'm told that in the human genome,

576
00:37:55,580 --> 00:38:00,540
I believe it's still 90% doesn't do anything,

577
00:38:00,540 --> 00:38:01,500
some large fraction.

578
00:38:01,500 --> 00:38:03,580
Some people think they didn't think they did something,

579
00:38:03,580 --> 00:38:05,340
actually, to do stuff.

580
00:38:05,340 --> 00:38:07,500
But they once did, presumably.

581
00:38:07,500 --> 00:38:12,500
About 90% of the human genome and a lot of other animals

582
00:38:12,500 --> 00:38:15,460
is not transcribed into proteins.

583
00:38:15,460 --> 00:38:21,900
And a fair amount of it is old inactive viruses.

584
00:38:21,900 --> 00:38:27,060
So it has maybe 90% of some really deadly virus

585
00:38:27,060 --> 00:38:31,900
that got incorporated into the genome and gets copied.

586
00:38:31,900 --> 00:38:33,780
So the big bug in evolution to me

587
00:38:33,780 --> 00:38:38,760
is that if you're going to build a system that's

588
00:38:38,760 --> 00:38:44,080
going to try to develop a new kind of program by trial

589
00:38:44,080 --> 00:38:50,040
and error, the standard approach is to imitate Darwin.

590
00:38:50,040 --> 00:38:56,920
And you mutate these programs, you give them a test,

591
00:38:56,920 --> 00:39:01,200
and you then copy the programs that pass the test

592
00:39:01,200 --> 00:39:03,640
and repeat the cycle.

593
00:39:03,640 --> 00:39:07,280
So what happens is you collect, because you're mutating them

594
00:39:07,280 --> 00:39:10,100
as you go along, you're collecting genes

595
00:39:10,100 --> 00:39:12,360
that help solve problems.

596
00:39:12,360 --> 00:39:14,320
But you're not collecting information

597
00:39:14,320 --> 00:39:17,320
about genes that make the animal worse

598
00:39:17,320 --> 00:39:19,800
or make it fail to solve problems.

599
00:39:19,800 --> 00:39:23,600
So this is true of all of evolution, as far as I can see,

600
00:39:23,600 --> 00:39:26,480
that there's no record kept of the worst

601
00:39:26,480 --> 00:39:28,600
things that can happen.

602
00:39:28,600 --> 00:39:40,480
And so every lethal mutation eventually kills someone.

603
00:39:40,480 --> 00:39:45,440
A lethal mutation is one, you know,

604
00:39:45,440 --> 00:39:49,440
you have two copies of every gene, one from a mother

605
00:39:49,440 --> 00:39:51,360
and one from a father.

606
00:39:51,440 --> 00:40:00,200
And if you get two copies of the same gene,

607
00:40:00,200 --> 00:40:05,600
and most genes are have a lot of genes

608
00:40:05,600 --> 00:40:08,880
are recessive in the sense that unless you get two of them,

609
00:40:08,880 --> 00:40:10,780
they're not expressed.

610
00:40:10,780 --> 00:40:13,320
If you have a lethal recessive gene,

611
00:40:13,320 --> 00:40:16,440
that usually means that you can have one of that gene

612
00:40:16,440 --> 00:40:19,320
and you're not sick, but if you have two of them,

613
00:40:19,320 --> 00:40:21,360
it eventually kills you.

614
00:40:21,360 --> 00:40:24,080
And it might kill you before birth,

615
00:40:24,080 --> 00:40:26,980
so you don't even get an embryo.

616
00:40:26,980 --> 00:40:30,680
Or it might kill you when you're 40 years old,

617
00:40:30,680 --> 00:40:34,600
as in that horrible Huntington's disease,

618
00:40:34,600 --> 00:40:39,120
where you can carry one and not suffer, but if you get two,

619
00:40:39,120 --> 00:40:40,740
it kills you in middle age, which

620
00:40:40,740 --> 00:40:43,520
is very expensive for society.

621
00:40:43,520 --> 00:40:45,240
Anyway, there's no record.

622
00:40:45,240 --> 00:40:53,200
What you want to do is for each problem solver that

623
00:40:53,200 --> 00:40:59,400
doesn't work, you want your evolution program

624
00:40:59,400 --> 00:41:03,440
to see why it doesn't work and not make that kind of gene

625
00:41:03,440 --> 00:41:06,040
again or whatever was responsible for it.

626
00:41:06,040 --> 00:41:09,160
So that's a big bug in Darwinian evolution.

627
00:41:09,160 --> 00:41:12,980
And the interesting fact is that every lethal recessive gene

628
00:41:12,980 --> 00:41:17,500
will eventually, on the average, kill someone.

629
00:41:17,500 --> 00:41:19,700
This is not a well-known.

630
00:41:19,700 --> 00:41:21,540
You see the arithmetic?

631
00:41:21,540 --> 00:41:24,540
Because it has to wait till there are two of them,

632
00:41:24,540 --> 00:41:27,560
and then it kills that person.

633
00:41:27,560 --> 00:41:30,860
And if you calculate the probabilities,

634
00:41:30,860 --> 00:41:33,740
there's a half chance of getting each of them

635
00:41:33,740 --> 00:41:36,860
in each generation.

636
00:41:36,860 --> 00:41:40,060
The math shows that eventually, there's

637
00:41:40,060 --> 00:41:45,020
one premature death for each recessive gene.

638
00:41:45,020 --> 00:41:46,940
It's kind of funny.

639
00:41:46,940 --> 00:41:51,580
So it would be nice if we had some way to clean them up once

640
00:41:51,580 --> 00:41:56,920
and for all, and then everybody would be a lot healthier.

641
00:41:56,920 --> 00:42:01,180
I bet within the next 20 or 30 years,

642
00:42:01,180 --> 00:42:04,980
we'll see some project which is to get rid of,

643
00:42:04,980 --> 00:42:08,180
just take somebody's genome, sweep out

644
00:42:08,180 --> 00:42:13,580
all the lethal recessives, and get rid of 100 diseases

645
00:42:13,580 --> 00:42:14,100
or more.

646
00:42:14,100 --> 00:42:17,260
And suddenly, everybody will live

647
00:42:17,260 --> 00:42:21,360
to be 150 years instead of 100.

648
00:42:21,360 --> 00:42:23,500
Something like that ought to happen.

649
00:42:23,500 --> 00:42:24,380
Yeah?

650
00:42:24,380 --> 00:42:27,920
There's some theories about why recessive genes

651
00:42:27,920 --> 00:42:31,060
stay in the population despite killing off people.

652
00:42:31,060 --> 00:42:33,800
And there are some genes for which it seems to be the case

653
00:42:33,800 --> 00:42:37,420
that when you get to a recessive gene, you die.

654
00:42:37,420 --> 00:42:40,540
But having the heterozygous combination

655
00:42:40,540 --> 00:42:43,020
gives you some benefit, like you benefit

656
00:42:43,020 --> 00:42:46,020
against a different disease, and that's why they persist.

657
00:42:46,020 --> 00:42:49,380
So just getting rid of all of the recessive lethal genes

658
00:42:49,380 --> 00:42:51,820
might cause problems.

659
00:42:51,820 --> 00:42:53,900
Wow, I hadn't thought of that.

660
00:42:53,900 --> 00:42:55,260
Are there some examples?

661
00:42:55,260 --> 00:42:57,580
Sickle cell, malaria.

662
00:42:57,580 --> 00:43:00,220
Yeah, sickle cell, malaria.

663
00:43:00,220 --> 00:43:04,220
So if you have sickle cell, you cannot get malaria.

664
00:43:04,220 --> 00:43:06,700
If you have heterozygous from the sickle cell disease,

665
00:43:06,700 --> 00:43:08,220
you're not mutating as well.

666
00:43:08,220 --> 00:43:10,700
But that's not very beneficial, because you usually

667
00:43:10,700 --> 00:43:12,620
die when you're around 40.

668
00:43:12,620 --> 00:43:13,120
No, no, no.

669
00:43:13,120 --> 00:43:15,740
If you have heterozygous for sickle cell disease,

670
00:43:15,740 --> 00:43:17,460
then you don't have sickle cell disease,

671
00:43:17,460 --> 00:43:19,000
but you have benefits against malaria.

672
00:43:19,000 --> 00:43:20,820
Oh, I didn't know that.

673
00:43:20,820 --> 00:43:25,380
That's the example commonly given in all biology classes,

674
00:43:25,380 --> 00:43:28,500
but I'm sure there must be other examples.

675
00:43:28,500 --> 00:43:30,220
I never took a biology class.

676
00:43:30,220 --> 00:43:35,460
That's good.

677
00:43:38,300 --> 00:43:41,860
So we could probably find one that we just

678
00:43:41,860 --> 00:43:45,060
have to tailor it a little bit.

679
00:43:45,060 --> 00:43:48,340
So the mosquitoes don't like it.

680
00:43:48,340 --> 00:43:51,220
Is that what it is?

681
00:43:51,220 --> 00:43:54,700
It's just bad enough blood that the mosquitoes will adore you,

682
00:43:54,700 --> 00:43:57,980
but not bad enough if you die.

683
00:43:57,980 --> 00:43:59,860
Does it keep the mosquito from biting you,

684
00:43:59,860 --> 00:44:03,500
or does it make the mosquitoes sick, or what?

685
00:44:03,500 --> 00:44:05,380
It makes the entire sense.

686
00:44:05,380 --> 00:44:08,420
It's just, yeah.

687
00:44:08,420 --> 00:44:10,220
Some stuff I've read about viruses,

688
00:44:10,220 --> 00:44:14,100
people changing their theory about viruses.

689
00:44:14,100 --> 00:44:16,340
One thing is that maybe in some sense,

690
00:44:16,340 --> 00:44:21,180
we're symbiotic with viruses in some sense.

691
00:44:21,180 --> 00:44:24,340
Like you say, the jump comes in the genome,

692
00:44:24,340 --> 00:44:27,140
maybe some process, but it takes advantage of that.

693
00:44:27,140 --> 00:44:29,260
So one thought I had is maybe the viruses

694
00:44:29,260 --> 00:44:35,180
are the things that remember why the losers lost.

695
00:44:35,180 --> 00:44:36,340
That's a good point.

696
00:44:36,340 --> 00:44:40,540
There's still lots of things we don't know and wrongly believe.

697
00:44:45,860 --> 00:44:49,900
With this synthetic life rule, there

698
00:44:49,900 --> 00:44:53,140
are two groups starting to make maybe more.

699
00:44:53,140 --> 00:44:57,460
There are probably some secret groups trying to make them too.

700
00:44:57,500 --> 00:45:03,300
Also, in some sense, the bacteria that live in the human body

701
00:45:03,300 --> 00:45:07,660
might be far more than the cells that are dealing with lures

702
00:45:07,660 --> 00:45:08,300
and so forth.

703
00:45:08,300 --> 00:45:11,260
And so they're starting to think that maybe

704
00:45:11,260 --> 00:45:14,140
the entire genome of all the bacteria that colonize you

705
00:45:14,140 --> 00:45:16,300
are also part of that equation in some way.

706
00:45:16,300 --> 00:45:20,540
So it could be that some of the genetic information

707
00:45:20,540 --> 00:45:22,820
in evolution is not kept in your own genome,

708
00:45:22,820 --> 00:45:26,100
but kept in all the organisms that

709
00:45:26,100 --> 00:45:28,900
live with you externally.

710
00:45:28,900 --> 00:45:31,260
Yeah, it's a what?

711
00:45:31,260 --> 00:45:32,500
Is there a comix for that?

712
00:45:32,500 --> 00:45:36,300
Yes, there is, that somebody was trying to sequence the.

713
00:45:36,300 --> 00:45:37,380
Bacteriomics?

714
00:45:37,380 --> 00:45:40,420
Yeah, I don't know what you did, biologists.

715
00:45:40,420 --> 00:45:42,980
Do you know what that's called?

716
00:45:42,980 --> 00:45:43,820
How many different?

717
00:45:43,820 --> 00:45:46,580
Somebody's trying to sequence every genome of everything

718
00:45:46,580 --> 00:45:48,900
that lives in your gut.

719
00:45:48,900 --> 00:45:50,620
Yeah, how many different?

720
00:45:50,620 --> 00:45:52,620
I understand there are more bacterial cells

721
00:45:52,620 --> 00:45:57,140
than somatic cells by a factor of 100 or something,

722
00:45:57,140 --> 00:45:59,220
because bacteria are so small.

723
00:45:59,220 --> 00:46:03,540
But how many different bacteria infests a person?

724
00:46:03,540 --> 00:46:07,540
Is it hundreds, or tens, or thousands?

725
00:46:07,540 --> 00:46:09,460
I guess that's what they're trying to find out.

726
00:46:14,780 --> 00:46:16,260
Yeah?

727
00:46:16,260 --> 00:46:19,020
So when you say, like in evolution,

728
00:46:19,020 --> 00:46:24,820
wouldn't it be nice if you kept everything that went bad?

729
00:46:24,820 --> 00:46:27,740
And then you said, and then we could analyze it

730
00:46:27,740 --> 00:46:29,460
to see what went wrong, right?

731
00:46:29,460 --> 00:46:34,100
But isn't it that when we're doing evolution algorithms

732
00:46:34,100 --> 00:46:38,860
or anything like that, we don't have a clear idea of what

733
00:46:38,860 --> 00:46:40,940
helps and what doesn't help to solve the problem?

734
00:46:40,940 --> 00:46:46,060
So even though we have the information of the solver

735
00:46:46,060 --> 00:46:53,180
that didn't work, I feel like if we had a way to know what went

736
00:46:53,180 --> 00:46:57,940
wrong, then we would already have information enough

737
00:46:57,940 --> 00:47:00,420
to know what is right.

738
00:47:00,420 --> 00:47:01,660
Oh, yes.

739
00:47:01,660 --> 00:47:04,660
So how do you decide what went wrong?

740
00:47:04,660 --> 00:47:07,860
I was thinking of a fairly high-level system,

741
00:47:07,860 --> 00:47:12,500
because when Lenit or Larry Fogel,

742
00:47:12,500 --> 00:47:23,900
I was another one of these learning by evolution systems,

743
00:47:23,900 --> 00:47:29,460
I'm not suggesting that we could make a simple evolution

744
00:47:29,460 --> 00:47:34,060
simulation that would think of reasons why it failed.

745
00:47:34,060 --> 00:47:35,700
So this would be a high-level one

746
00:47:35,700 --> 00:47:38,660
if you're writing a big AI program.

747
00:47:38,660 --> 00:47:44,060
For example, when you learn arithmetic, after a while,

748
00:47:44,060 --> 00:47:48,420
you learn not to divide by 0.

749
00:47:48,420 --> 00:47:52,820
So what do we call negative knowledge?

750
00:47:56,500 --> 00:47:58,900
What are the common-sense things?

751
00:47:58,900 --> 00:48:02,780
Is there a name for the things you should never do?

752
00:48:02,780 --> 00:48:05,180
Because when people talk about, you know,

753
00:48:05,180 --> 00:48:09,020
they search tree as a possibility to prune the tree.

754
00:48:09,020 --> 00:48:10,580
You prune the tree.

755
00:48:10,580 --> 00:48:14,060
But we have rule-based systems, and they

756
00:48:14,060 --> 00:48:17,740
got very popular around 1980 and wiped out

757
00:48:17,740 --> 00:48:24,140
most of the symbolic AI for a long time.

758
00:48:24,140 --> 00:48:29,220
But there aren't any rules that say don't do X.

759
00:48:29,220 --> 00:48:30,300
Are there ever?

760
00:48:30,300 --> 00:48:32,780
Do they have some?

761
00:48:32,780 --> 00:48:37,700
So the question is, when are they invoked?

762
00:48:37,700 --> 00:48:43,260
In a certain situation, turn off this bank of rules, maybe.

763
00:48:47,500 --> 00:48:51,540
So I'm not suggesting that you could make a very simple system

764
00:48:51,540 --> 00:48:53,700
do that because, in fact, figuring out

765
00:48:53,700 --> 00:48:59,300
why this mutation was bad might be a very hard problem.

766
00:48:59,300 --> 00:49:01,580
But as you build smarter and smarter ones,

767
00:49:02,340 --> 00:49:06,220
then you want to put, well, what I called critics,

768
00:49:06,220 --> 00:49:09,100
or what I don't know if Freud had a name for them.

769
00:49:11,860 --> 00:49:15,020
At some point, you want to have prohibited actions.

770
00:49:15,020 --> 00:49:22,620
And in Sigmund Freud's early model of psychology,

771
00:49:22,620 --> 00:49:27,780
there was a place for things that you would go away from

772
00:49:27,780 --> 00:49:33,780
or not do, and these sensors, he called them.

773
00:49:33,780 --> 00:49:42,100
And they never appeared in the main line of psychology.

774
00:49:42,100 --> 00:49:45,900
When they threw out Freud, who had a few bad ideas,

775
00:49:45,900 --> 00:49:50,300
they threw out all his good ideas, practically.

776
00:49:50,300 --> 00:49:51,980
You might be pleased to hear that some

777
00:49:51,980 --> 00:49:53,980
of the monkey neuroscientists are

778
00:49:53,980 --> 00:49:57,620
starting to find some critics.

779
00:49:57,620 --> 00:50:00,620
It's pretty hand-wavy stuff as of now,

780
00:50:00,620 --> 00:50:02,900
but at least they're thinking about it.

781
00:50:02,900 --> 00:50:06,620
There's certain tasks where the monkey is cued

782
00:50:06,620 --> 00:50:10,300
to pay attention to one thing or another, usually

783
00:50:10,300 --> 00:50:12,260
if any, it's color versus orientation.

784
00:50:12,260 --> 00:50:15,380
And what they've found is that orientation has dominance.

785
00:50:15,380 --> 00:50:17,540
And so when the cue is telling the monkey

786
00:50:17,540 --> 00:50:19,940
that they have to ignore the orientation

787
00:50:19,940 --> 00:50:22,540
and pay attention to the color, the part

788
00:50:22,540 --> 00:50:26,300
of those neurons which are responsible for looking

789
00:50:26,300 --> 00:50:30,140
at the orientation are being actively inhibited

790
00:50:30,140 --> 00:50:33,060
by another group of neurons, which they're now

791
00:50:33,060 --> 00:50:34,540
calling a critic.

792
00:50:34,540 --> 00:50:39,860
Are these in the same organ, or is it a little nearby nucleus

793
00:50:39,860 --> 00:50:41,460
that's inhibited?

794
00:50:41,460 --> 00:50:42,660
That's nice.

795
00:50:42,660 --> 00:50:50,100
So that would be a good place for a word

796
00:50:50,100 --> 00:50:53,060
for a negative knowledge.

797
00:50:53,060 --> 00:50:56,620
Why do you call it negative knowledge?

798
00:50:56,620 --> 00:51:02,340
It would have too many different senses, but advice not to take.

799
00:51:08,620 --> 00:51:11,060
So this question would imply that there's

800
00:51:11,060 --> 00:51:16,940
a metric for intelligence, but is there a limit to intelligence?

801
00:51:16,940 --> 00:51:20,020
Is it possible to say one day we have this artificial intelligence

802
00:51:20,020 --> 00:51:24,740
that is the most intelligent possible being?

803
00:51:24,740 --> 00:51:33,500
Seems unlikely, because presumably the survival

804
00:51:33,500 --> 00:51:36,060
value of a particular system depends

805
00:51:36,060 --> 00:51:37,500
on the world the thing is in.

806
00:51:39,940 --> 00:51:48,460
It might be that for all worlds above a certain complexity,

807
00:51:48,460 --> 00:51:54,260
maybe there are some overall strategies that are universally

808
00:51:54,260 --> 00:51:55,820
better than others or something.

809
00:51:58,820 --> 00:52:01,780
Measuring intelligence doesn't make any sense,

810
00:52:01,780 --> 00:52:07,380
because I think you have to go the way Howard Gardner did

811
00:52:07,380 --> 00:52:11,300
and say, well, there's social intelligence,

812
00:52:11,300 --> 00:52:15,620
and I don't know.

813
00:52:15,660 --> 00:52:20,100
Can anybody rattle off his list?

814
00:52:20,100 --> 00:52:25,340
What are his eight ways of thinking?

815
00:52:30,740 --> 00:52:32,380
Just look up Howard Gardner.

816
00:52:36,540 --> 00:52:39,740
So the amount of intelligence is clearly

817
00:52:39,740 --> 00:52:44,260
it's a useful intuitive idea that

818
00:52:44,260 --> 00:52:47,780
for any particular machine, you could imagine another one that

819
00:52:47,780 --> 00:52:51,820
can do everything that one can do and more,

820
00:52:51,820 --> 00:52:56,300
but you're going to get a lattice, not an ordered thing,

821
00:52:56,300 --> 00:52:58,860
and the lattice won't.

822
00:52:58,860 --> 00:53:01,260
At some point, it'll start getting inconsistent,

823
00:53:01,260 --> 00:53:05,100
and this will be better than that one for this and not that.

824
00:53:05,100 --> 00:53:05,600
Go ahead.

825
00:53:10,580 --> 00:53:12,620
Go ahead with about nine different types

826
00:53:12,620 --> 00:53:18,060
of intelligences according to this Wikipedia article.

827
00:53:18,060 --> 00:53:21,300
Logical, mathematical, spatial, linguistic, bodily,

828
00:53:21,300 --> 00:53:24,900
kinesthetic, musical, interpersonal, intrapersonal,

829
00:53:24,900 --> 00:53:26,660
naturalistic, and existential.

830
00:53:26,660 --> 00:53:29,020
There you go.

831
00:53:29,020 --> 00:53:32,500
And if you take any one of those,

832
00:53:32,500 --> 00:53:36,180
when I was a mathematician, I was really good at topology

833
00:53:36,180 --> 00:53:38,300
but not at algebra.

834
00:53:38,300 --> 00:53:42,580
And at some point, that stopped me

835
00:53:42,580 --> 00:53:46,900
from being even better at topology.

836
00:53:46,900 --> 00:53:50,880
So if you take any one of those, I

837
00:53:50,880 --> 00:53:53,380
think Howard wants to keep it simple,

838
00:53:53,380 --> 00:53:59,900
but I wonder if he has a subpsychologist who's chopped

839
00:53:59,900 --> 00:54:02,660
up mathematics into the right.

840
00:54:02,660 --> 00:54:04,860
What are the right eight feet?

841
00:54:08,060 --> 00:54:11,020
How many of you are bad at some kind of mathematics

842
00:54:11,020 --> 00:54:11,820
and know why?

843
00:54:18,660 --> 00:54:22,220
Really bad at Fourier series, just because I don't like them.

844
00:54:22,220 --> 00:54:29,700
I wonder what Newton would have thought about them.

845
00:54:37,060 --> 00:54:45,980
In my PhD thesis, it was mostly about neural networks.

846
00:54:45,980 --> 00:54:47,540
And there were some people who thought

847
00:54:47,540 --> 00:54:49,820
that you could put information, if you had

848
00:54:49,820 --> 00:54:53,820
a bunch of neurons in a circle, then you

849
00:54:53,820 --> 00:55:01,540
could put in a string of signals of different durations

850
00:55:01,540 --> 00:55:06,700
and store the bits in this circular thing.

851
00:55:06,700 --> 00:55:11,600
Because in World War II, there were no digital computer

852
00:55:11,600 --> 00:55:15,700
memories, but there were some computer-like things

853
00:55:15,700 --> 00:55:19,540
that stored signals in a tube of mercury

854
00:55:19,540 --> 00:55:23,220
with a speaker and a microphone.

855
00:55:23,220 --> 00:55:27,180
And it was possible to store a lot of information

856
00:55:27,180 --> 00:55:32,060
in sort of analog bits for a long time.

857
00:55:32,060 --> 00:55:33,420
But what you would do is you would

858
00:55:33,420 --> 00:55:35,200
have something that would regenerate them

859
00:55:35,200 --> 00:55:39,440
and synchronize them with a clock each time around.

860
00:55:39,440 --> 00:55:44,500
And I was trying to prove a theorem that,

861
00:55:44,500 --> 00:55:48,900
given what we knew about the delay in neurons,

862
00:55:48,900 --> 00:55:51,600
if you stimulate a neuron very strongly,

863
00:55:51,600 --> 00:55:55,700
it reacts more quickly than if you just stimulated a little bit

864
00:55:55,700 --> 00:55:59,740
above threshold, that it takes a longer time to fire.

865
00:55:59,740 --> 00:56:04,220
So I was trying to prove that in neural networks in something

866
00:56:04,220 --> 00:56:07,980
like a human brain, you couldn't store a lot of information

867
00:56:07,980 --> 00:56:10,020
in circular loops.

868
00:56:10,020 --> 00:56:14,620
And I kept having trouble proving that,

869
00:56:14,620 --> 00:56:21,300
and I ran into John Nash, who was another student a bit ahead

870
00:56:21,300 --> 00:56:22,540
of me.

871
00:56:22,540 --> 00:56:25,260
And he listened to me for a minute,

872
00:56:25,260 --> 00:56:30,660
and he said, expand it in Fourier series.

873
00:56:30,660 --> 00:56:33,540
And after about two days, I figured out

874
00:56:33,540 --> 00:56:38,500
what he probably meant, and I proved this nice theorem.

875
00:56:38,500 --> 00:56:48,020
And it turned out it's also, and it had been discovered

876
00:56:48,020 --> 00:56:51,740
a long time ago, was called a Lipschitz condition.

877
00:56:51,740 --> 00:56:55,540
And if you have a certain condition like this,

878
00:56:55,540 --> 00:56:58,740
then the information will go away.

879
00:56:58,740 --> 00:57:03,300
But if you don't, you can keep the information

880
00:57:03,300 --> 00:57:07,020
around for a very long time.

881
00:57:07,020 --> 00:57:11,940
So in this case, the proof showed

882
00:57:11,940 --> 00:57:15,860
that you couldn't store, unless you had a renormalizer

883
00:57:15,860 --> 00:57:18,020
or a clock somewhere, you couldn't

884
00:57:18,020 --> 00:57:26,500
store circular information in a mammalian brain very well.

885
00:57:26,500 --> 00:57:30,900
It's a nice example of something where one person had

886
00:57:30,900 --> 00:57:33,700
a different way of looking at it.

887
00:57:33,700 --> 00:57:40,460
Nash was pretty famous for his result in game theory,

888
00:57:40,460 --> 00:57:44,980
but I suspect he might have been responsible for five or 10

889
00:57:44,980 --> 00:57:45,900
other things that he.

890
00:57:50,380 --> 00:57:54,220
Norbert Wiener had this habit of talking to a student,

891
00:57:54,220 --> 00:57:56,700
and he says, what are you working on?

892
00:57:56,700 --> 00:57:59,980
And the student would explain it, and Wiener said,

893
00:57:59,980 --> 00:58:03,460
oh, well, you just do this.

894
00:58:03,460 --> 00:58:06,900
And I was present at a meeting of the,

895
00:58:06,900 --> 00:58:10,020
I was in the math department, where

896
00:58:10,020 --> 00:58:12,500
they had a meeting about who would tell Wiener

897
00:58:12,500 --> 00:58:13,980
not to do that anymore.

898
00:58:22,380 --> 00:58:25,860
Some student had, oh, well, it's a true story.

899
00:58:34,460 --> 00:58:37,220
I wonder what else I've forgotten.

900
00:58:40,900 --> 00:58:41,900
Yes?

901
00:58:41,900 --> 00:58:42,380
I'm curious.

902
00:58:42,380 --> 00:58:44,540
You say this could be updated with a clock.

903
00:58:44,540 --> 00:58:49,140
Is there any evidence to suggest that biologically one

904
00:58:49,140 --> 00:58:51,660
could or could not construct a clock?

905
00:58:51,660 --> 00:58:54,500
There are lots of clocks.

906
00:58:54,500 --> 00:58:57,260
I suspect that if I had thought about it more,

907
00:58:57,260 --> 00:59:03,700
I would have, because I'm talking the middle 1950s,

908
00:59:03,700 --> 00:59:07,460
and people knew a lot about brain waves.

909
00:59:07,460 --> 00:59:14,740
And there are three or four fairly large synchronous

910
00:59:14,740 --> 00:59:17,420
activities in the brain.

911
00:59:17,420 --> 00:59:20,100
And I don't think anybody knows much about what they're for.

912
00:59:20,100 --> 00:59:20,820
Do you know?

913
00:59:20,820 --> 00:59:24,740
Have you heard any rumors of what is the delta wave for?

914
00:59:24,740 --> 00:59:27,940
Well, actually, the monkey experiment

915
00:59:27,940 --> 00:59:31,860
I was just talking about relies on sort of assumption

916
00:59:31,860 --> 00:59:37,340
that the beta wave is for suppression,

917
00:59:37,340 --> 00:59:39,900
and the alpha wave is for activation.

918
00:59:39,900 --> 00:59:41,900
And I think people are still sort of debating

919
00:59:41,900 --> 00:59:45,420
about the delta and beta waves.

920
00:59:45,420 --> 00:59:48,780
The alpha wave, what's the 10%?

921
00:59:48,780 --> 00:59:50,680
I think that's the big one.

922
00:59:50,680 --> 00:59:55,920
And it goes away when you are thinking hard.

923
00:59:55,920 --> 01:00:00,520
That is, if you're not focusing much on anything,

924
01:00:00,520 --> 01:00:04,440
then it's a fairly nice regular 10 per second.

925
01:00:04,440 --> 01:00:09,200
And if anything gets your attention and you focus on it,

926
01:00:09,200 --> 01:00:13,960
then the alpha wave pretty much gets noisy and disappears,

927
01:00:13,960 --> 01:00:15,560
I think.

928
01:00:15,560 --> 01:00:17,800
I don't know what the others do.

929
01:00:21,640 --> 01:00:25,760
Is that correlated with any event?

930
01:00:25,760 --> 01:00:27,520
The usual room shutting down.

931
01:00:30,600 --> 01:00:35,720
I brought all this, but I decided not to use it anyway.

932
01:00:35,720 --> 01:00:38,560
I think it's correlated with a certain period of time

933
01:00:38,560 --> 01:00:42,240
after the signal from the computer stops changing.

934
01:00:42,240 --> 01:00:44,840
Oh, you mean it might wake up again.

935
01:00:44,840 --> 01:00:47,800
No, it shifts down at the same time every five.

936
01:00:47,800 --> 01:00:49,240
It's not always the same time.

937
01:00:49,240 --> 01:01:05,320
It's usually at 8.30.

938
01:01:05,320 --> 01:01:11,360
I wonder if Steve Jobs had this little thing has two batteries.

939
01:01:11,360 --> 01:01:12,840
And at one end, there's a dot.

940
01:01:12,840 --> 01:01:16,400
And the other end, there's a slot,

941
01:01:16,400 --> 01:01:18,840
which is for a screwdriver.

942
01:01:18,840 --> 01:01:21,120
But it's also the minus sign of the battery.

943
01:01:25,920 --> 01:01:35,800
It could have been plus, but what's that?

944
01:01:35,800 --> 01:01:38,440
It's for a quarter, so you can put a coin in it.

945
01:01:38,440 --> 01:01:40,800
Any coin, actually.

946
01:01:40,800 --> 01:01:43,880
Yeah, so you don't actually need a screwdriver.

947
01:01:43,880 --> 01:01:44,840
I don't have a coin.

948
01:01:48,840 --> 01:01:59,160
Of course, it's usually one.

949
01:01:59,160 --> 01:01:59,920
It's somewhere.

950
01:02:04,680 --> 01:02:05,520
No tips.

951
01:02:14,000 --> 01:02:14,800
Good question.

952
01:02:14,800 --> 01:02:15,320
Yeah?

953
01:02:15,600 --> 01:02:17,280
Do you think or I just want to close,

954
01:02:17,280 --> 01:02:19,560
will there ever be elected as a leader of a government?

955
01:02:24,880 --> 01:02:28,200
In most science fiction stories, it doesn't give us a choice.

956
01:02:33,320 --> 01:02:37,440
The moon is a harsh mistress.

957
01:02:37,440 --> 01:02:40,280
That was Robert Heinlein, wasn't it?

958
01:02:40,280 --> 01:02:44,160
It had a really smart computer emerged

959
01:02:44,160 --> 01:02:48,400
from the internet on the moon.

960
01:02:48,400 --> 01:02:50,400
Yeah?

961
01:02:50,400 --> 01:02:52,680
Yes?

962
01:02:52,680 --> 01:02:54,920
I was curious whether you have any ideas how

963
01:02:54,920 --> 01:02:59,040
we could attempt to determine the representations

964
01:02:59,040 --> 01:03:01,080
of information that our people or animals use

965
01:03:01,080 --> 01:03:03,040
to solve problems.

966
01:03:03,040 --> 01:03:05,600
Clearly, this is a crucial problem to intelligence.

967
01:03:05,600 --> 01:03:08,640
And lots of AI has gone into various ways

968
01:03:08,640 --> 01:03:11,200
of representing information.

969
01:03:11,200 --> 01:03:15,280
So it would be really interesting to see how it's done.

970
01:03:15,280 --> 01:03:17,880
And I was wondering if you had ideas

971
01:03:17,880 --> 01:03:21,040
of how that could be tested.

972
01:03:21,040 --> 01:03:21,880
That's wonderful.

973
01:03:25,840 --> 01:03:30,680
What are the cognitive psychologists

974
01:03:30,680 --> 01:03:32,600
doing about representations?

975
01:03:32,600 --> 01:03:36,120
Have you run across any?

976
01:03:36,160 --> 01:03:38,920
They study reaction times, you know,

977
01:03:38,920 --> 01:03:41,840
with scientific or colloquial ways of starting

978
01:03:41,840 --> 01:03:45,680
to determine the representations.

979
01:03:45,680 --> 01:03:48,320
You know, they don't have very good ways of studying that

980
01:03:48,320 --> 01:03:50,120
directly from experiments.

981
01:03:50,120 --> 01:03:51,960
Yeah.

982
01:03:51,960 --> 01:03:57,560
All right, these rule-based systems are still the,

983
01:03:57,560 --> 01:04:00,360
I haven't read a modern cognitive psycho.

984
01:04:00,360 --> 01:04:05,360
Has anybody read a modern cognitive psychology book?

985
01:04:05,360 --> 01:04:08,440
Do they have trance frames or scripts?

986
01:04:12,800 --> 01:04:14,320
What's happening in that realm?

987
01:04:14,320 --> 01:04:43,400
I'm trying to remember what, I guess

988
01:04:43,400 --> 01:04:49,160
I've never seen any Winston-like diagrams in anything but AI.

989
01:04:49,160 --> 01:04:53,320
But there must be some somewhere.

990
01:04:53,320 --> 01:05:00,640
That's 1970.

991
01:05:00,640 --> 01:05:02,520
Who has taken a psychology course?

992
01:05:05,800 --> 01:05:08,360
Is that true?

993
01:05:08,360 --> 01:05:10,000
What's in it?

994
01:05:10,000 --> 01:05:11,320
They talk about babies a lot.

995
01:05:15,200 --> 01:05:16,920
Well, there's a little industry of trying

996
01:05:16,920 --> 01:05:19,480
to show that Piaget was wrong.

997
01:05:19,480 --> 01:05:23,240
Is that what they say about babies?

998
01:05:23,240 --> 01:05:27,320
When do babies get conservation of quantity or something?

999
01:05:27,320 --> 01:05:29,280
Yeah, basically just go throughout the whole

1000
01:05:29,280 --> 01:05:31,400
development stage and explain that.

1001
01:05:35,040 --> 01:05:38,360
But I have not seen Winston in any of my psychology readings.

1002
01:05:41,000 --> 01:05:48,800
Well, there is a problem with the low resolution of brain

1003
01:05:48,800 --> 01:05:50,200
scanning.

1004
01:05:50,200 --> 01:05:58,240
So if you can only tell when a square centimeter of brain

1005
01:05:58,240 --> 01:06:01,400
is more active than another part,

1006
01:06:01,400 --> 01:06:02,960
then it's hard to imagine how you

1007
01:06:02,960 --> 01:06:06,120
could look for the representation of an arch

1008
01:06:06,120 --> 01:06:10,600
as a block on top of two others.

1009
01:06:10,600 --> 01:06:17,960
But you should be able to make a hypothesis about

1010
01:06:17,960 --> 01:06:22,880
a representation and then design an experiment in which you

1011
01:06:22,880 --> 01:06:27,240
show a picture of an arch and then quickly

1012
01:06:27,240 --> 01:06:32,880
show a picture where there's a little space between them

1013
01:06:32,880 --> 01:06:35,200
so it's not being supported by.

1014
01:06:35,200 --> 01:06:38,160
And blink those on and off and see

1015
01:06:38,160 --> 01:06:42,320
if different kinds of changes in the representation

1016
01:06:42,320 --> 01:06:47,320
cause different kinds of brain activity.

1017
01:06:47,320 --> 01:06:54,200
But I suspect that most experiments on watching brain

1018
01:06:54,200 --> 01:06:59,360
activity are from giving a stimulus and not a pair

1019
01:06:59,360 --> 01:07:02,560
of quickly changing ones, maybe.

1020
01:07:02,560 --> 01:07:06,520
So you want to find out what parts of the brain

1021
01:07:06,520 --> 01:07:10,920
are activated when a certain kind of difference appears.

1022
01:07:10,920 --> 01:07:14,720
And it shouldn't be hard to make such experiments,

1023
01:07:14,720 --> 01:07:20,240
but my impression is that they don't do that so much as you

1024
01:07:20,240 --> 01:07:23,280
show a certain face for a couple of seconds

1025
01:07:23,280 --> 01:07:25,840
and then you show something else and you

1026
01:07:25,840 --> 01:07:29,560
look to see if the activity moves somewhere.

1027
01:07:29,560 --> 01:07:33,720
But if your resolution is low, maybe

1028
01:07:33,720 --> 01:07:39,440
you should be putting in stimuli that change

1029
01:07:39,440 --> 01:07:43,040
so that you're finding the response to the changes.

1030
01:07:43,040 --> 01:07:44,520
It's just a.

1031
01:07:44,520 --> 01:07:48,160
One of the problems is that there is a delay.

1032
01:07:48,160 --> 01:07:52,600
You can, in brain waves, you can get like a,

1033
01:07:52,600 --> 01:07:55,840
you can get low real-time reaction, but if MRI is.

1034
01:07:55,840 --> 01:08:00,680
Yeah, it usually takes several seconds to get anything.

1035
01:08:00,680 --> 01:08:05,880
You have to do a, you'd have to repeat it many times.

1036
01:08:05,880 --> 01:08:09,320
And I think it still takes several seconds

1037
01:08:09,320 --> 01:08:10,880
to get any information, doesn't it?

1038
01:08:20,960 --> 01:08:23,880
The first brain wave experiments were in the late,

1039
01:08:23,880 --> 01:08:28,320
in the 1940s, and that Englishman,

1040
01:08:28,320 --> 01:08:31,680
Gray Walter, who also made that first robot

1041
01:08:31,680 --> 01:08:33,080
turtle and things like that.

1042
01:08:36,000 --> 01:08:42,000
I was just reading some of the, some papers

1043
01:08:42,000 --> 01:08:46,840
he wrote in the middle 1950s.

1044
01:08:46,840 --> 01:08:49,600
They're not very illuminating about AI,

1045
01:08:49,600 --> 01:08:54,200
but they show you what some people were

1046
01:08:54,200 --> 01:08:58,240
thinking in the days before computer science.

1047
01:09:00,960 --> 01:09:03,440
Yeah?

1048
01:09:03,440 --> 01:09:05,320
When you talk about the, when you're

1049
01:09:05,320 --> 01:09:09,080
talking about neural networks and the machines that

1050
01:09:09,080 --> 01:09:13,280
accumulate huge libraries of statistical data,

1051
01:09:13,280 --> 01:09:18,520
you use that they cannot develop much governance

1052
01:09:18,520 --> 01:09:24,840
because they don't have these, sorry, they, here.

1053
01:09:29,720 --> 01:09:33,840
Because they don't have higher reflective levels.

1054
01:09:33,840 --> 01:09:37,920
What are these higher reflective levels?

1055
01:09:37,920 --> 01:09:41,480
Well, that's thinking about what you were thinking a minute ago.

1056
01:09:43,880 --> 01:09:47,920
You think something, and then you say, that was a bad idea.

1057
01:09:47,920 --> 01:09:50,280
Why did I get that?

1058
01:09:50,280 --> 01:09:54,760
Or now I realize I didn't understand something.

1059
01:09:54,760 --> 01:10:01,040
I've wasted five minutes because reflective thinking is

1060
01:10:01,040 --> 01:10:04,280
thinking about your recent thoughts.

1061
01:10:04,280 --> 01:10:09,680
Maybe all thinking is any coherent train of thinking.

1062
01:10:09,680 --> 01:10:12,940
Each thought is something about the previous thought,

1063
01:10:12,940 --> 01:10:15,900
but it doesn't have the word I in it.

1064
01:10:18,740 --> 01:10:22,260
You say, why did I waste so much time?

1065
01:10:22,260 --> 01:10:24,780
Why did I focus on this rather than that?

1066
01:10:28,500 --> 01:10:29,740
What did that person say?

1067
01:10:29,740 --> 01:10:31,020
Maybe I missed the point.

1068
01:10:36,500 --> 01:10:39,820
Maybe most of your thinking is, what did I just think?

1069
01:10:39,820 --> 01:10:41,180
Maybe I missed the point.

1070
01:10:42,940 --> 01:10:45,060
Yeah?

1071
01:10:45,060 --> 01:10:48,700
So here we have to talk a lot about cognitive science

1072
01:10:48,700 --> 01:10:49,500
and psychology.

1073
01:10:49,500 --> 01:10:52,660
And I'm curious, how important do you

1074
01:10:52,660 --> 01:10:55,420
think brain and cognitive science and psychology

1075
01:10:55,420 --> 01:10:58,940
are to the field of AI, and whether the right way of trying

1076
01:10:58,940 --> 01:11:01,420
to build intelligent machines and understanding intelligences

1077
01:11:01,420 --> 01:11:03,300
through understanding what we've already seen,

1078
01:11:03,300 --> 01:11:06,740
or it's playing around with computers

1079
01:11:06,740 --> 01:11:08,900
and trying to make systems that solve the problems we

1080
01:11:08,900 --> 01:11:09,740
want to solve?

1081
01:11:09,740 --> 01:11:14,700
I'm glad you asked that because I

1082
01:11:14,700 --> 01:11:18,820
don't think it's very important because I think we all,

1083
01:11:18,820 --> 01:11:23,100
we've got to the point where we know that people solve problems

1084
01:11:23,100 --> 01:11:26,460
and we all know how to think about how

1085
01:11:26,460 --> 01:11:27,540
we solve some problems.

1086
01:11:27,540 --> 01:11:31,580
We don't know the details of how we did it.

1087
01:11:31,580 --> 01:11:40,940
But I think if you look at what's been done in AI,

1088
01:11:40,940 --> 01:11:45,460
it's more than clear enough where the present system

1089
01:11:45,460 --> 01:11:53,140
stopped and where they fail.

1090
01:11:53,140 --> 01:11:56,380
And we keep thinking of ways to fix them.

1091
01:11:57,340 --> 01:12:01,540
And we get sidetracked because you get some idea

1092
01:12:01,540 --> 01:12:04,420
and it's too hard to program.

1093
01:12:04,420 --> 01:12:06,540
Somebody says, use C++.

1094
01:12:06,540 --> 01:12:10,740
And somebody else says, why don't you go back to Lisp?

1095
01:12:13,300 --> 01:12:18,180
And I guess my answer is, I don't

1096
01:12:18,180 --> 01:12:23,140
think we need desperately to know more about psychology

1097
01:12:23,140 --> 01:12:24,940
because we already have programs that

1098
01:12:24,940 --> 01:12:28,980
are pretty good at things and we can see where they get stuck.

1099
01:12:28,980 --> 01:12:35,020
But it would be nice if there were a community out there

1100
01:12:35,020 --> 01:12:41,500
helping us because the AI groups are all alone

1101
01:12:41,500 --> 01:12:46,620
and they don't communicate very well with each other

1102
01:12:46,620 --> 01:12:51,100
and they're not very well supported.

1103
01:12:51,100 --> 01:12:58,420
But I bet as we make machines smarter,

1104
01:12:58,420 --> 01:13:00,500
the psychologists will pay more attention

1105
01:13:00,500 --> 01:13:03,420
and they'll come back and tell us better things.

1106
01:13:03,420 --> 01:13:10,740
And eventually, there'll be a real cognitive science,

1107
01:13:10,740 --> 01:13:12,140
sort of like physics.

1108
01:13:12,140 --> 01:13:17,940
Physics got very well with Newton and Galileo

1109
01:13:17,940 --> 01:13:23,820
and quantum mechanics.

1110
01:13:23,820 --> 01:13:26,780
But now they have a great community.

1111
01:13:26,780 --> 01:13:30,940
And when some serious problem comes up,

1112
01:13:30,940 --> 01:13:36,060
somebody will spend a billion dollars for a new accelerator

1113
01:13:36,060 --> 01:13:37,140
or something.

1114
01:13:37,140 --> 01:13:39,660
There's nothing like that in AI.

1115
01:13:39,660 --> 01:13:43,660
If you say, why did the Newell-Simon general problem

1116
01:13:44,460 --> 01:13:46,820
get stuck on the missionary and cannibals?

1117
01:13:49,460 --> 01:13:51,740
Somebody should say, well, here's a billion dollars.

1118
01:13:51,740 --> 01:13:53,940
I know it's not enough, but maybe you

1119
01:13:53,940 --> 01:13:57,780
can make it a little smarter.

1120
01:13:57,780 --> 01:14:00,180
Nobody's offering this.

1121
01:14:04,020 --> 01:14:06,860
Two somewhat, I guess, somewhat related questions.

1122
01:14:06,860 --> 01:14:12,220
So first, since AI is mostly an engineering discipline,

1123
01:14:12,220 --> 01:14:15,700
it's a question of, how can we make machines that solve

1124
01:14:15,700 --> 01:14:17,900
these problems that look intelligent?

1125
01:14:17,900 --> 01:14:19,220
Do you think this is going to lead

1126
01:14:19,220 --> 01:14:20,980
to a better understanding of intelligence?

1127
01:14:20,980 --> 01:14:25,740
And how important do you think that is to this more,

1128
01:14:25,740 --> 01:14:27,980
I guess, mostly scientific, but also slightly philosophical

1129
01:14:27,980 --> 01:14:30,820
question?

1130
01:14:30,820 --> 01:14:36,460
I think it's just an engineering question that there just

1131
01:14:36,460 --> 01:14:39,700
isn't a way to get bright people, enough bright people

1132
01:14:39,700 --> 01:14:43,660
to compete with each other to make better AI systems.

1133
01:14:47,300 --> 01:14:50,020
Anybody have a theory?

1134
01:14:50,020 --> 01:14:53,420
You see, I'm speaking from the point of view,

1135
01:14:53,420 --> 01:14:58,060
feeling that there hasn't been much progress in recent years.

1136
01:14:58,060 --> 01:15:00,140
And maybe I'm wrong.

1137
01:15:00,140 --> 01:15:03,500
And there's a lot of great stuff just ready to be exploited,

1138
01:15:03,500 --> 01:15:06,460
but I don't see it.

1139
01:15:06,460 --> 01:15:10,260
I think we're kind of in an actual delivery room of sorts

1140
01:15:10,260 --> 01:15:15,980
where people are doing a lot of the work in terms of,

1141
01:15:15,980 --> 01:15:17,940
for instance, tuning the parameters

1142
01:15:17,940 --> 01:15:20,900
and choosing machine learning approximations in order

1143
01:15:20,900 --> 01:15:25,220
to solve problems that there are incentives out there to solve.

1144
01:15:25,220 --> 01:15:29,820
And in principle, if we had AI that was good,

1145
01:15:29,820 --> 01:15:32,780
the AI would do that work instead of programmers

1146
01:15:32,780 --> 01:15:35,140
having to tune parameters and figure out

1147
01:15:35,140 --> 01:15:37,580
which algorithms are good for different problems.

1148
01:15:37,580 --> 01:15:41,540
But as of now, the way the incentives are structured,

1149
01:15:41,540 --> 01:15:44,260
it's going to take a big energy push

1150
01:15:44,260 --> 01:15:48,500
to sort of get over the hump of actually creating

1151
01:15:48,500 --> 01:15:50,740
the infrastructure that's necessary for that stuff

1152
01:15:50,740 --> 01:15:52,060
to happen automatically.

1153
01:15:54,900 --> 01:15:58,100
There are AI groups.

1154
01:15:58,100 --> 01:16:02,940
There are few people at Georgia Tech and Carnegie Mellon,

1155
01:16:02,940 --> 01:16:07,800
although my impression is that they're mostly playing robot

1156
01:16:07,800 --> 01:16:08,940
soccer or something.

1157
01:16:11,700 --> 01:16:17,260
So a lot of the people who are empowered to do the right

1158
01:16:17,260 --> 01:16:20,300
thing are, I mean, look at Stanford.

1159
01:16:20,300 --> 01:16:23,940
It's wonderful to make these self-driving cars,

1160
01:16:23,940 --> 01:16:28,500
but I don't think a single thing has been learned from that.

1161
01:16:29,500 --> 01:16:38,540
Maybe a little has been learned from the Watson thing, but.

1162
01:16:38,540 --> 01:16:41,540
They won't come out in the source code.

1163
01:16:41,540 --> 01:16:42,040
Right.

1164
01:16:42,040 --> 01:16:46,980
And if they did, I think they could read the Society of Mind

1165
01:16:46,980 --> 01:16:50,100
that says, have a lot of different methods

1166
01:16:50,100 --> 01:16:52,780
and find some way to integrate them.

1167
01:16:52,780 --> 01:16:54,820
What's missing in the Society of Mind

1168
01:16:55,060 --> 01:16:58,380
is better ideas on how to integrate them.

1169
01:16:58,380 --> 01:17:01,660
And Watson might have some, but on the other hand,

1170
01:17:01,660 --> 01:17:03,540
it might not.

1171
01:17:03,540 --> 01:17:07,940
Maybe if it can end up with an answer that's

1172
01:17:07,940 --> 01:17:14,140
one word, like a person or a sport, then it's done.

1173
01:17:14,140 --> 01:17:18,540
And so it may be that we know it's at the lower levels,

1174
01:17:18,540 --> 01:17:23,180
and we don't know what's the best way to do it.

1175
01:17:23,820 --> 01:17:25,780
We don't know what's at the higher levels,

1176
01:17:25,780 --> 01:17:27,940
and maybe it's no good.

1177
01:17:27,940 --> 01:17:30,940
On the other hand, maybe there are 10 very important ideas

1178
01:17:30,940 --> 01:17:35,380
there, and you'd have to read that long paper

1179
01:17:35,380 --> 01:17:38,580
and try to guess what they were.

1180
01:17:38,580 --> 01:17:39,900
Do we have a spy in there?

1181
01:17:44,300 --> 01:17:46,340
Are they telling us something?

1182
01:17:46,340 --> 01:17:48,580
I get little bits and pieces about it, yeah.

1183
01:17:49,180 --> 01:17:53,900
I mean, I think it is kind of, you know,

1184
01:17:53,900 --> 01:17:56,940
I think the good news about that is it has made some progress,

1185
01:17:56,940 --> 01:17:59,220
and it is kind of a society of models,

1186
01:17:59,220 --> 01:18:02,620
and they have some supervisory processes that

1187
01:18:02,620 --> 01:18:05,180
try to figure out which, actually, the most important

1188
01:18:05,180 --> 01:18:06,580
thing they do is try to figure out

1189
01:18:06,580 --> 01:18:09,940
which methods are good for which kind of questions.

1190
01:18:09,940 --> 01:18:12,740
That would be good.

1191
01:18:12,740 --> 01:18:15,140
So they might have some good critics and selectors

1192
01:18:15,140 --> 01:18:16,220
like things.

1193
01:18:16,220 --> 01:18:19,420
Yeah, so there's some of that, I think, in there.

1194
01:18:19,420 --> 01:18:22,700
I don't think there are a lot of very brand new techniques,

1195
01:18:22,700 --> 01:18:27,700
but I think there's probably some of that.

1196
01:18:27,700 --> 01:18:30,180
They fired their other AI group, but I

1197
01:18:30,180 --> 01:18:32,420
don't think it was getting very far either.

1198
01:18:37,460 --> 01:18:43,940
You know, the one I mean, Eric Mueller and no, he moved.

1199
01:18:43,940 --> 01:18:45,420
He worked on the Watson.

1200
01:18:45,420 --> 01:18:49,220
No, I mean, Doug Reichen's group.

1201
01:18:49,220 --> 01:18:53,860
It was doing more mathematical AI than heuristic AI.

1202
01:18:56,460 --> 01:18:58,980
Any other company doing anything?

1203
01:18:58,980 --> 01:19:04,220
What are the common sense groups in Korea and places like that?

1204
01:19:04,220 --> 01:19:07,700
Well, I'll find out in December when I go there.

1205
01:19:07,700 --> 01:19:09,260
Henry's going to visit some of them.

1206
01:19:09,900 --> 01:19:10,900
The mysterious east.

1207
01:19:20,900 --> 01:19:22,380
Yes?

1208
01:19:22,380 --> 01:19:26,380
So there has been, like, since a long time ago,

1209
01:19:26,380 --> 01:19:30,860
from the nuclear factor assessment,

1210
01:19:30,860 --> 01:19:32,860
there has been, like, machines that

1211
01:19:32,860 --> 01:19:36,860
are trying to build, like, a conveyor, right?

1212
01:19:36,860 --> 01:19:38,860
So there's, like, a layer, right?

1213
01:19:38,860 --> 01:19:40,340
There are critics.

1214
01:19:40,340 --> 01:19:44,340
And even though the idea died out in the 80s,

1215
01:19:44,340 --> 01:19:46,340
but then there's still some machines,

1216
01:19:46,340 --> 01:19:48,340
like maybe Watson has all critics.

1217
01:19:48,340 --> 01:19:52,820
But at the, like, reflective layer,

1218
01:19:52,820 --> 01:19:58,300
I feel like it does a lot of different things.

1219
01:19:58,300 --> 01:20:01,300
So what do you think is missing from that layer

1220
01:20:01,300 --> 01:20:04,780
that, like, no project has pulled back yet?

1221
01:20:06,860 --> 01:20:07,360
OK.

1222
01:20:13,180 --> 01:20:20,620
I'm not sure what you're asking, but there

1223
01:20:20,620 --> 01:20:23,220
is Pat Winston's group working on stories.

1224
01:20:23,220 --> 01:20:29,620
And my impression is that that's making definite progress.

1225
01:20:29,620 --> 01:20:36,700
And if he can integrate it with Henry Lieberman's

1226
01:20:36,700 --> 01:20:40,020
kind of large, common-sense knowledge base,

1227
01:20:40,020 --> 01:20:41,580
maybe something great will happen.

1228
01:20:41,580 --> 01:20:46,740
But progress is a little bit slow.

1229
01:20:46,740 --> 01:20:54,620
Jerry Sussman is still full of ideas,

1230
01:20:54,620 --> 01:20:56,860
but he keeps teaching courses in physics.

1231
01:20:57,620 --> 01:21:06,020
And he's out there fixing telescopes.

1232
01:21:06,020 --> 01:21:11,660
And he's absolutely a prodigy.

1233
01:21:11,660 --> 01:21:17,200
And now he's working on this theory of propagators,

1234
01:21:17,200 --> 01:21:21,380
which he claims is relevant to AI.

1235
01:21:21,380 --> 01:21:23,540
And I don't understand it yet.

1236
01:21:24,540 --> 01:21:26,220
But, uh.

1237
01:21:26,220 --> 01:21:27,940
It's good.

1238
01:21:27,940 --> 01:21:28,580
What?

1239
01:21:28,580 --> 01:21:30,660
I swear.

1240
01:21:30,660 --> 01:21:33,060
I'd like to see it solve some interesting problem.

1241
01:21:37,460 --> 01:21:39,820
So we have a lot of resources here.

1242
01:21:39,820 --> 01:21:43,420
But if you look at the world as a whole.

1243
01:21:44,420 --> 01:21:50,780
And for example, you talked about how

1244
01:21:50,780 --> 01:21:56,300
we should combine the perception in the common-sense group

1245
01:21:56,300 --> 01:21:59,780
with the huge common-sense knowledge base, right?

1246
01:21:59,780 --> 01:22:09,780
So I feel like doing that, we need some new, newly-invented

1247
01:22:09,780 --> 01:22:11,580
missionary.

1248
01:22:11,580 --> 01:22:12,820
Yeah, to what extent is?

1249
01:22:12,820 --> 01:22:14,420
Well, if you would like to work on it,

1250
01:22:14,420 --> 01:22:18,380
please come see me after this.

1251
01:22:18,380 --> 01:22:19,860
It's a very lively group.

1252
01:22:30,900 --> 01:22:32,440
What's happening to Lenat's group?

1253
01:22:32,440 --> 01:22:34,660
Is he just hiding, or is he?

1254
01:22:34,660 --> 01:22:37,860
No, I think Doug Lenat is a side project.

1255
01:22:37,860 --> 01:22:42,780
And it's been steadily growing.

1256
01:22:42,780 --> 01:22:45,660
And I think one thing that, so what was,

1257
01:22:45,660 --> 01:22:47,940
he had a very interesting article recently

1258
01:22:47,940 --> 01:22:51,740
about using it for common-sense for medical queries.

1259
01:22:51,740 --> 01:22:54,260
So the Watson guys said that they

1260
01:22:54,260 --> 01:22:56,140
wanted to apply Watson to medicine.

1261
01:22:56,140 --> 01:22:59,460
But I think Lenat had a really good article

1262
01:22:59,460 --> 01:23:01,460
about applying it to medical queries.

1263
01:23:01,460 --> 01:23:06,140
It was things like, so the doctors would ask things

1264
01:23:06,140 --> 01:23:11,060
like which operations for some disease or something

1265
01:23:11,060 --> 01:23:12,860
had complications.

1266
01:23:12,860 --> 01:23:14,700
And the system would have to understand

1267
01:23:14,700 --> 01:23:16,060
what's a complication, right?

1268
01:23:16,060 --> 01:23:19,500
And a complication is when things don't go right.

1269
01:23:19,500 --> 01:23:23,740
So having a drug reaction could be a complication.

1270
01:23:23,740 --> 01:23:29,020
Leaving a scalpel in the patient could be a complication.

1271
01:23:29,020 --> 01:23:34,020
So you have to understand some of the ideas of,

1272
01:23:34,020 --> 01:23:36,660
common-sense ideas of what might be a complication

1273
01:23:36,660 --> 01:23:40,340
or what might cause trouble, those kind of things.

1274
01:23:41,340 --> 01:23:45,340
He did a very nice system for the Cleveland Clinic.

1275
01:23:45,340 --> 01:23:46,820
And the doctors loved it.

1276
01:23:46,820 --> 01:23:49,300
And they wrote about it in the online magazine.

1277
01:23:49,300 --> 01:23:51,300
But that was a real success.

1278
01:23:51,300 --> 01:23:54,780
Oh, I haven't seen that.

1279
01:23:54,780 --> 01:23:57,140
Dr. Lenat.

1280
01:23:57,140 --> 01:23:59,820
I think the problem is that the reason

1281
01:23:59,820 --> 01:24:03,500
that you haven't heard a lot of applications for psych

1282
01:24:03,500 --> 01:24:07,020
for so long is because they were funded for decades

1283
01:24:07,020 --> 01:24:09,900
by three-letter agencies in the government.

1284
01:24:10,860 --> 01:24:13,900
I think they did actually quite good work for them,

1285
01:24:13,900 --> 01:24:16,300
because otherwise, the program would have continued

1286
01:24:16,300 --> 01:24:18,620
for 25 years.

1287
01:24:18,620 --> 01:24:20,860
But the problem is, when they do something good

1288
01:24:20,860 --> 01:24:24,300
for secret agencies, nobody else funds out about it.

1289
01:24:24,300 --> 01:24:26,300
It doesn't advance the field.

1290
01:24:30,020 --> 01:24:35,660
I have a great story about that, which is almost unbelievable.

1291
01:24:35,660 --> 01:24:39,980
Which is, I was at a meeting with Doug Lenat.

1292
01:24:39,980 --> 01:24:44,820
This was a long time ago when it was just starting.

1293
01:24:44,820 --> 01:24:50,020
And this was in a building a block from the White House.

1294
01:24:50,020 --> 01:24:56,380
And it had all these people from some agency

1295
01:24:56,380 --> 01:25:01,140
about whether AI could help them with their problems.

1296
01:25:01,140 --> 01:25:07,780
And somebody pulled out some slides

1297
01:25:07,780 --> 01:25:09,220
and was about to give a lecture.

1298
01:25:13,340 --> 01:25:19,340
But the shelf that had the projector on it had hinges.

1299
01:25:19,340 --> 01:25:23,820
And all the screws were missing on one side.

1300
01:25:23,820 --> 01:25:25,580
And it fell down like this.

1301
01:25:25,580 --> 01:25:28,520
And they fussed for a long time.

1302
01:25:28,520 --> 01:25:36,480
And couldn't get the projector to line up.

1303
01:25:36,480 --> 01:25:43,440
And then I had this thing.

1304
01:25:45,800 --> 01:25:48,080
And I took three screws out of it.

1305
01:25:48,080 --> 01:25:49,280
It had three hinges.

1306
01:25:49,280 --> 01:25:51,840
And I took three screws out of here

1307
01:25:51,840 --> 01:25:57,200
and put them in here, and here, and here.

1308
01:25:57,200 --> 01:26:00,440
And then the shelf stayed up, and the show went on.

1309
01:26:03,600 --> 01:26:10,860
It's like the joke about the anyway.

1310
01:26:10,860 --> 01:26:14,160
So they were astounded, because I actually

1311
01:26:14,160 --> 01:26:16,720
fixed this stupid thing.

1312
01:26:16,720 --> 01:26:18,640
And I said, well, why didn't you?

1313
01:26:18,640 --> 01:26:22,180
And they said, we asked for maintenance three weeks ago.

1314
01:26:22,180 --> 01:26:24,920
And they never got around to it.

1315
01:26:24,920 --> 01:26:27,840
And I said, this is the agency.

1316
01:26:27,840 --> 01:26:28,720
And they said, yes.

1317
01:26:33,040 --> 01:26:35,760
Then they said, but why did you have that thing with you?

1318
01:26:38,260 --> 01:26:40,320
Today, you never get past the metal detector.

1319
01:26:46,080 --> 01:26:50,600
When I was a kid, I heard some story, oh, never mind,

1320
01:26:50,600 --> 01:26:53,400
about when a car wheel rolls off,

1321
01:26:53,400 --> 01:26:56,760
you take one screw from each of the other three.

1322
01:26:56,760 --> 01:26:58,320
So I was doing exactly that.

1323
01:26:58,320 --> 01:27:03,600
And these agency people had never

1324
01:27:03,600 --> 01:27:06,680
thought of doing it themselves.

1325
01:27:06,680 --> 01:27:09,320
So what does it mean when you have a government run

1326
01:27:09,320 --> 01:27:12,080
by people who can't fix this hinge?

1327
01:27:17,200 --> 01:27:20,480
I once met a freshman who didn't know which way to turn a screw.

1328
01:27:24,400 --> 01:27:31,120
At MIT, how many of you have to try both?

1329
01:27:45,760 --> 01:27:48,160
The left hand.

1330
01:27:48,160 --> 01:27:49,160
Some rule, right?

1331
01:27:53,400 --> 01:27:55,080
If you're screwing in at weird angles,

1332
01:27:55,080 --> 01:27:57,200
like upset pieces and stuff.

1333
01:27:57,200 --> 01:27:58,160
Yes, sometimes.

1334
01:28:08,680 --> 01:28:09,160
That's right.

1335
01:28:12,440 --> 01:28:15,480
Enough stories.

1336
01:28:15,480 --> 01:28:18,080
Are you sure?

1337
01:28:18,080 --> 01:28:19,520
So has he?

1338
01:28:19,520 --> 01:28:22,760
Oh, can you send us a pointer to that paper?

1339
01:28:22,760 --> 01:28:23,560
Lenin's?

1340
01:28:23,560 --> 01:28:24,240
Oh, yeah, sure.

1341
01:28:24,240 --> 01:28:24,920
That would be nice.

1342
01:28:27,560 --> 01:28:31,560
He's one of the great pioneers of AI.

1343
01:28:34,040 --> 01:28:38,080
I guess that's a question about extracting

1344
01:28:38,080 --> 01:28:42,040
a different knowledge from experience.

1345
01:28:42,040 --> 01:28:45,120
I feel like this is something that's

1346
01:28:45,120 --> 01:28:49,560
done in the respective layer in this.

1347
01:28:49,560 --> 01:28:51,880
So it could be done in all layers,

1348
01:28:51,880 --> 01:28:59,280
but maybe it's probably done in the respective layer.

1349
01:28:59,280 --> 01:29:03,760
So how do you think it does that?

1350
01:29:03,760 --> 01:29:06,120
How do you retrieve your knowledge?

1351
01:29:06,120 --> 01:29:13,640
How do you turn that experience into a piece of a rule

1352
01:29:13,640 --> 01:29:16,960
used by the critics?

1353
01:29:16,960 --> 01:29:20,240
How do you learn from an experience?

1354
01:29:20,240 --> 01:29:24,200
You do something, and then you get some knowledge,

1355
01:29:24,200 --> 01:29:25,240
and where do you put it?

1356
01:29:41,600 --> 01:29:46,640
If we could answer that, we could all quit and go home.

1357
01:29:46,640 --> 01:29:53,240
You're asking the central problem of how do you learn

1358
01:29:53,240 --> 01:29:57,480
from experience and later retrieve how you learned.

1359
01:29:57,480 --> 01:30:00,400
I just can't think of any way to answer that

1360
01:30:00,400 --> 01:30:06,880
except write a whole book and then have everybody

1361
01:30:06,880 --> 01:30:08,160
find out what's wrong with it.

1362
01:30:08,160 --> 01:30:17,520
Yeah, science is making the best mistakes.

1363
01:30:20,520 --> 01:30:22,080
If you make a really good mistake,

1364
01:30:22,080 --> 01:30:24,640
then somebody will fix it, and you'll get progress.

1365
01:30:27,720 --> 01:30:34,240
If you make a silly mistake, then nothing is gained.

1366
01:30:38,560 --> 01:30:40,200
Maybe we should have to search into how

1367
01:30:40,200 --> 01:30:43,120
to make better mistakes.

1368
01:30:43,120 --> 01:30:44,600
Well, I didn't decide what to try.

1369
01:30:49,880 --> 01:30:55,080
That's my complaint about the probabilistic methods,

1370
01:30:55,080 --> 01:30:58,440
because if there are a lot of, well,

1371
01:30:58,440 --> 01:31:00,440
I talked about the other day, if there

1372
01:31:00,440 --> 01:31:05,960
are a lot of different aspects of the situation, like 100,

1373
01:31:05,960 --> 01:31:09,680
then there's 2 to the 100th conditional probabilities

1374
01:31:09,680 --> 01:31:11,840
to think about.

1375
01:31:11,840 --> 01:31:18,480
And so probabilistic learning machines

1376
01:31:18,480 --> 01:31:23,520
work wonderfully well on small problems

1377
01:31:23,520 --> 01:31:29,560
where the search trees aren't too big, but they don't.

1378
01:31:29,560 --> 01:31:31,440
But the hard problem is what to do

1379
01:31:31,440 --> 01:31:34,480
when there's a lot of different factors

1380
01:31:34,480 --> 01:31:37,920
and you don't know which are important.

1381
01:31:37,920 --> 01:31:42,800
And in lots of situations, just first order correlations.

1382
01:31:42,800 --> 01:31:45,520
So if there are 100 factors, and you just

1383
01:31:45,520 --> 01:31:49,160
look at the probabilities of each of them,

1384
01:31:49,160 --> 01:31:54,000
and then there's 5,000 pairs of things,

1385
01:31:54,000 --> 01:31:59,480
and you look at the 5,000 joint conditional probabilities

1386
01:31:59,480 --> 01:32:03,840
of two things, and maybe five of them pop up,

1387
01:32:03,840 --> 01:32:06,760
and you've only got five things to look at,

1388
01:32:06,760 --> 01:32:12,400
and that's where that kind of AI system works.

1389
01:32:12,400 --> 01:32:15,600
And it's become immensely popular.

1390
01:32:15,600 --> 01:32:18,360
And the trouble is it'll never get smarter,

1391
01:32:18,360 --> 01:32:24,160
because if you have to look five steps ahead, then instead

1392
01:32:24,160 --> 01:32:31,120
of 10 possibilities, you have 100,000.

1393
01:32:31,160 --> 01:32:41,080
Anyway, my concern is that there are quite a lot of millions

1394
01:32:41,080 --> 01:32:44,000
of dollars going into AI research,

1395
01:32:44,000 --> 01:32:47,520
but most of it is going into dead ends.

1396
01:32:47,520 --> 01:32:51,000
So it's not as though there weren't a maybe.

1397
01:32:51,000 --> 01:32:54,400
Maybe there is enough money, but it's

1398
01:32:54,400 --> 01:32:57,640
going to the easy problems instead of the hard ones.

1399
01:33:01,120 --> 01:33:06,120
Who has an easy question?

1400
01:33:06,120 --> 01:33:08,120
I have an easy question.

1401
01:33:08,120 --> 01:33:12,120
So a lot of smart people like to play games and procrastinate.

1402
01:33:12,120 --> 01:33:14,120
Do you think artificial intelligence will also

1403
01:33:14,120 --> 01:33:16,120
like to play games and procrastinate?

1404
01:33:21,120 --> 01:33:27,120
Well, there's the opposite question.

1405
01:33:27,120 --> 01:33:30,120
I got a message from somebody I don't remember.

1406
01:33:30,120 --> 01:33:34,120
Somebody I don't remember who I had complained

1407
01:33:34,120 --> 01:33:41,200
that nobody has been able to get Push Singh's AI

1408
01:33:41,200 --> 01:33:42,280
program to work.

1409
01:33:44,760 --> 01:33:47,520
And somebody suggested what?

1410
01:33:47,520 --> 01:33:49,640
The E-M1?

1411
01:33:49,640 --> 01:33:51,560
Yeah.

1412
01:33:51,560 --> 01:33:56,240
And somebody suggested that I forget the name.

1413
01:33:56,240 --> 01:34:01,520
There's some group of people who like problems.

1414
01:34:01,520 --> 01:34:04,280
And I can't remember what it is.

1415
01:34:04,280 --> 01:34:07,840
It's just some bunch of people out on the web

1416
01:34:07,840 --> 01:34:11,240
who like to solve programming problems.

1417
01:34:11,240 --> 01:34:14,520
And this person suggested sending the code

1418
01:34:14,520 --> 01:34:17,440
to that group of a couple of thousand people.

1419
01:34:17,440 --> 01:34:20,080
And maybe they would self-organize

1420
01:34:20,080 --> 01:34:23,720
to try to figure out how it works and fix it.

1421
01:34:23,720 --> 01:34:26,400
So do you think that would work?

1422
01:34:26,400 --> 01:34:28,280
Could that work?

1423
01:34:28,280 --> 01:34:29,200
Yeah.

1424
01:34:29,200 --> 01:34:31,080
We have a big bunch of code.

1425
01:34:31,080 --> 01:34:33,680
It's partly commented.

1426
01:34:33,680 --> 01:34:38,080
Could we get 1,000 really aimless hackers out there

1427
01:34:38,080 --> 01:34:41,120
with lots of ability to?

1428
01:34:41,120 --> 01:34:42,200
So maybe I'll try it.

1429
01:34:50,720 --> 01:34:51,200
Sort of.

1430
01:34:51,200 --> 01:34:52,720
The danger with that is that they

1431
01:34:52,720 -
[01:42:35.160 --> 01:42:38.320]  Do you think AI research is structured in a way
[01:42:38.320 --> 01:42:40.560]  that could ever be broken down?
[01:42:40.560 --> 01:42:44.120]  Well, don't these crowd things usually start with some?
[01:42:44.120 --> 01:42:46.760]  They must start with some sort of leader,
[01:42:46.760 --> 01:42:51.520]  but then they become self-organizing.
[01:42:51.520 --> 01:42:55.320]  I mean, they work because every participant
[01:42:55.320 --> 01:43:00.560]  has a specific and distinct head, basically the same style
[01:43:00.560 --> 01:43:04.040]  that they have.
[01:43:04.040 --> 01:43:08.320]  I mean, they don't become more complex than that.
[01:43:08.320 --> 01:43:10.560]  I mean, it becomes a community, or whatever.
[01:43:10.560 --> 01:43:15.920]  But it doesn't, you know, like the idea of lowering the floor
[01:43:15.920 --> 01:43:23.000]  of doing AI research so that more people can contribute.
[01:43:23.000 --> 01:43:25.240]  It's a nice question.
[01:43:25.240 --> 01:43:26.800]  Well, let's think about it.
[01:43:26.840 --> 01:43:37.000]  If we take this PhD thesis code, it
[01:43:37.000 --> 01:43:40.840]  wouldn't be much good to send the whole thing to everyone.
[01:43:40.840 --> 01:43:43.560]  Well, of course, it wouldn't cost anything to do that.
[01:43:43.560 --> 01:43:47.520]  But you need somebody to make a first pass at chopping it up
[01:43:47.520 --> 01:43:52.280]  into saying, look, this function didn't work.
[01:43:52.280 --> 01:43:53.800]  Maybe there's some code missing.
[01:43:53.800 --> 01:43:59.240]  And so you might need a person or a couple of people
[01:43:59.240 --> 01:44:02.720]  to sort of organize the project.
[01:44:02.720 --> 01:44:08.360]  But once you've got a community, they
[01:44:08.360 --> 01:44:12.040]  might be able to cooperate.
[01:44:12.040 --> 01:44:16.680]  A community already existing to be able to contribute.
[01:44:16.680 --> 01:44:18.960]  They might do it without a leader
[01:44:18.960 --> 01:44:23.200]  once the problems became clear enough, because.
[01:44:23.200 --> 01:44:27.160]  I would say that, I mean, there are some crowdsourced AI
[01:44:27.160 --> 01:44:27.640]  projects.
[01:44:27.640 --> 01:44:31.960]  Certainly, you go to SourceForge or the restaurant
[01:44:31.960 --> 01:44:33.640]  game of Jeff Horton and all that.
[01:44:33.640 --> 01:44:35.000]  That's their crowdsource.
[01:44:35.000 --> 01:44:36.600]  You can think of that as a crowdsource.
[01:44:36.600 --> 01:44:37.360]  They are a problem.
[01:44:37.360 --> 01:44:39.360]  But I don't think the crowdsourcing is really
[01:44:39.360 --> 01:44:43.600]  great for problems that demand a lot of creativity.
[01:44:43.600 --> 01:44:46.320]  You know, it's commands that are projects that
[01:44:46.320 --> 01:44:47.480]  are labor intensive.
[01:44:47.480 --> 01:44:50.680]  It's good for, like, setting at home and that kind of thing.
[01:44:50.680 --> 01:44:55.520]  But for projects that demand a lot of creativity,
[01:44:55.520 --> 01:44:57.400]  it kind of breaks my heart almost,
[01:44:57.400 --> 01:45:01.760]  because if you look at the open source work on Unix,
[01:45:01.760 --> 01:45:04.360]  they've done a great job at organizing people
[01:45:04.360 --> 01:45:08.920]  to work on Unix and Linux and three versions of Unix.
[01:45:08.920 --> 01:45:12.560]  But on the other hand, the software isn't very innovative.
[01:45:12.560 --> 01:45:16.760]  They just implement 60 versions of the mail probably.
[01:45:16.840 --> 01:45:21.720]  The Unix interface hasn't changed since the 1960s pretty much.
[01:45:21.720 --> 01:45:24.840]  So you know, it's everybody's still
[01:45:24.840 --> 01:45:27.320]  programming on terminal windows.
[01:45:27.320 --> 01:45:29.600]  So I think it's crowdsourcing is mainly
[01:45:29.600 --> 01:45:32.360]  good for projects that are labor intensive.
[01:45:32.360 --> 01:45:37.200]  But I think AI needs individual creativity more.
[01:45:37.200 --> 01:45:39.720]  It's like McCarthy said, it needs
[01:45:39.720 --> 01:45:44.760]  2.3 Einstein's and 1.7 Hatton projects.
[01:45:44.760 --> 01:45:48.600]  In fact, you know, it's probably good for the Hatton projects
[01:45:48.600 --> 01:45:51.400]  but not for the Unix studies.
[01:45:51.400 --> 01:45:55.240]  On the other hand, given we have the movie,
[01:45:55.240 --> 01:45:58.080]  it might be that the problem of getting this code
[01:45:58.080 --> 01:46:01.880]  to make that movie isn't so creative.
[01:46:01.880 --> 01:46:07.080]  You could start it up and see where it gets stuck.
[01:46:07.080 --> 01:46:09.120]  And it's worth it.
[01:46:09.120 --> 01:46:11.440]  A great possibility is if you have something
[01:46:11.440 --> 01:46:14.680]  long-winds of Watson that incorporates lots and lots
[01:46:14.680 --> 01:46:16.920]  of small programs, then you can have
[01:46:16.920 --> 01:46:18.480]  people contribute small programs.
[01:46:18.480 --> 01:46:21.000]  And whether they're good or bad, the system
[01:46:21.000 --> 01:46:23.080]  can figure it out in the more than one.
[01:46:23.080 --> 01:46:25.640]  Well, in some sense, Watson was crowdsourced
[01:46:25.640 --> 01:46:28.920]  because it wasn't only developed by that IBM group
[01:46:28.920 --> 01:46:34.200]  that they had lots of collaborators in ISI and CMU
[01:46:34.200 --> 01:46:35.520]  and other places.
[01:46:35.520 --> 01:46:39.360]  And they crowdsourced them by giving them little research
[01:46:39.360 --> 01:46:42.240]  grants to integrate their employee with the research
[01:46:42.240 --> 01:46:44.080]  into Watson.
[01:46:44.080 --> 01:46:49.360]  So I think you could argue that that actually was crowdsourced.
[01:46:49.360 --> 01:46:51.480]  Another thing we haven't tried is
[01:46:51.480 --> 01:46:53.000]  called throwing money at it.
[01:46:55.680 --> 01:47:01.760]  Suppose we got $5,000 or $1,000 and told some programmer,
[01:47:01.760 --> 01:47:03.120]  can you get this to work?
[01:47:03.920 --> 01:47:05.920]  Well, if you have good enough descriptions
[01:47:05.920 --> 01:47:09.920]  of various small programs that you
[01:47:09.920 --> 01:47:13.920]  can use for problem solving, then I
[01:47:13.920 --> 01:47:16.920]  think that part of the problem is creativity.
[01:47:16.920 --> 01:47:19.920]  Because some parts or some sub-programs
[01:47:19.920 --> 01:47:22.920]  could be more stupid.
[01:47:22.920 --> 01:47:24.920]  As long as you have a good description
[01:47:24.920 --> 01:47:26.920]  of what that program is doing, then you
[01:47:26.920 --> 01:47:29.920]  could have some really creative program.
[01:47:30.720 --> 01:47:35.720]  That might use those stupid programs as part of its reasoning.
[01:47:35.720 --> 01:47:37.720]  But you still need that one creative person
[01:47:37.720 --> 01:47:38.720]  to make the decision.
[01:47:38.720 --> 01:47:40.720]  Well, you still need that one creative person
[01:47:40.720 --> 01:47:41.720]  to make the decision.
[01:47:41.720 --> 01:47:44.720]  But you don't get so many if you organize it in Watson.
[01:47:44.720 --> 01:47:45.720]  Yeah.
[01:47:45.720 --> 01:47:47.720]  I'm going to just label it Bowman Warcraft 10.
[01:47:47.720 --> 01:47:48.720]  Leave it on the internet.
[01:47:48.720 --> 01:47:51.520]  Bowman Warcraft 10, leave it on the internet.
[01:48:05.520 --> 01:48:14.120]  My grandson was suspended from Warcraft for three weeks
[01:48:14.120 --> 01:48:17.200]  because he hacked the thing to get a higher
[01:48:17.200 --> 01:48:18.680]  priority on something.
[01:48:21.960 --> 01:48:25.520]  I think he was not.
[01:48:25.520 --> 01:48:28.840]  Do you know how old he was?
[01:48:28.840 --> 01:48:30.160]  Yeah.
[01:48:30.160 --> 01:48:31.000]  No, miles.
[01:48:34.600 --> 01:48:38.600]  No, I think he was about 10 or 12.
[01:48:38.600 --> 01:48:41.120]  And he had actually managed to get into the thing
[01:48:41.120 --> 01:48:43.360]  and get instant service.
[01:48:48.080 --> 01:48:49.880]  He was very proud of being banned.
[01:49:00.880 --> 01:49:03.480]  I give up.
[01:49:03.480 --> 01:49:06.400]  Any last request or idea?
[01:49:11.360 --> 01:49:13.400]  Thanks for coming.
2:05,000
Do you think there's ever going to be a way to crowdsource AI

1541
01:42:05,000 --> 01:42:06,800
research at all?

1542
01:42:07,800 --> 01:42:09,560
That's what I meant.

1543
01:42:09,560 --> 01:42:11,520
That's the expression I was looking for,

1544
01:42:11,520 --> 01:42:13,240
for fixing the push theses.

1545
01:42:13,240 --> 01:42:15,320
But it would be nice.

1546
01:42:15,320 --> 01:42:17,720
It wouldn't be a self-organizing thing.

1547
01:42:17,720 --> 01:42:20,080
Someone would have to do that.

1548
01:42:20,080 --> 01:42:24,200
I feel like that would be hard for people

1549
01:42:24,200 --> 01:42:26,720
to self-organize to do that.

1550
01:42:26,720 --> 01:42:28,880
But if there were already the structure,

1551
01:42:28,880 --> 01:42:32,560
and that minimal piece that everyone could do for AI

1552
01:42:32,560 --> 01:42:35,160
research was already defined.

1553
01:42:35,160 --> 01:42:38,320
Do you think AI research is structured in a way

1554
01:42:38,320 --> 01:42:40,560
that could ever be broken down?

1555
01:42:40,560 --> 01:42:44,120
Well, don't these crowd things usually start with some?

1556
01:42:44,120 --> 01:42:46,760
They must start with some sort of leader,

1557
01:42:46,760 --> 01:42:51,520
but then they become self-organizing.

1558
01:42:51,520 --> 01:42:55,320
I mean, they work because every participant

1559
01:42:55,320 --> 01:43:00,560
has a specific and distinct head, basically the same style

1560
01:43:00,560 --> 01:43:04,040
that they have.

1561
01:43:04,040 --> 01:43:08,320
I mean, they don't become more complex than that.

1562
01:43:08,320 --> 01:43:10,560
I mean, it becomes a community, or whatever.

1563
01:43:10,560 --> 01:43:15,920
But it doesn't, you know, like the idea of lowering the floor

1564
01:43:15,920 --> 01:43:23,000
of doing AI research so that more people can contribute.

1565
01:43:23,000 --> 01:43:25,240
It's a nice question.

1566
01:43:25,240 --> 01:43:26,800
Well, let's think about it.

1567
01:43:26,840 --> 01:43:37,000
If we take this PhD thesis code, it

1568
01:43:37,000 --> 01:43:40,840
wouldn't be much good to send the whole thing to everyone.

1569
01:43:40,840 --> 01:43:43,560
Well, of course, it wouldn't cost anything to do that.

1570
01:43:43,560 --> 01:43:47,520
But you need somebody to make a first pass at chopping it up

1571
01:43:47,520 --> 01:43:52,280
into saying, look, this function didn't work.

1572
01:43:52,280 --> 01:43:53,800
Maybe there's some code missing.

1573
01:43:53,800 --> 01:43:59,240
And so you might need a person or a couple of people

1574
01:43:59,240 --> 01:44:02,720
to sort of organize the project.

1575
01:44:02,720 --> 01:44:08,360
But once you've got a community, they

1576
01:44:08,360 --> 01:44:12,040
might be able to cooperate.

1577
01:44:12,040 --> 01:44:16,680
A community already existing to be able to contribute.

1578
01:44:16,680 --> 01:44:18,960
They might do it without a leader

1579
01:44:18,960 --> 01:44:23,200
once the problems became clear enough, because.

1580
01:44:23,200 --> 01:44:27,160
I would say that, I mean, there are some crowdsourced AI

1581
01:44:27,160 --> 01:44:27,640
projects.

1582
01:44:27,640 --> 01:44:31,960
Certainly, you go to SourceForge or the restaurant

1583
01:44:31,960 --> 01:44:33,640
game of Jeff Horton and all that.

1584
01:44:33,640 --> 01:44:35,000
That's their crowdsource.

1585
01:44:35,000 --> 01:44:36,600
You can think of that as a crowdsource.

1586
01:44:36,600 --> 01:44:37,360
They are a problem.

1587
01:44:37,360 --> 01:44:39,360
But I don't think the crowdsourcing is really

1588
01:44:39,360 --> 01:44:43,600
great for problems that demand a lot of creativity.

1589
01:44:43,600 --> 01:44:46,320
You know, it's commands that are projects that

1590
01:44:46,320 --> 01:44:47,480
are labor intensive.

1591
01:44:47,480 --> 01:44:50,680
It's good for, like, setting at home and that kind of thing.

1592
01:44:50,680 --> 01:44:55,520
But for projects that demand a lot of creativity,

1593
01:44:55,520 --> 01:44:57,400
it kind of breaks my heart almost,

1594
01:44:57,400 --> 01:45:01,760
because if you look at the open source work on Unix,

1595
01:45:01,760 --> 01:45:04,360
they've done a great job at organizing people

1596
01:45:04,360 --> 01:45:08,920
to work on Unix and Linux and three versions of Unix.

1597
01:45:08,920 --> 01:45:12,560
But on the other hand, the software isn't very innovative.

1598
01:45:12,560 --> 01:45:16,760
They just implement 60 versions of the mail probably.

1599
01:45:16,840 --> 01:45:21,720
The Unix interface hasn't changed since the 1960s pretty much.

1600
01:45:21,720 --> 01:45:24,840
So you know, it's everybody's still

1601
01:45:24,840 --> 01:45:27,320
programming on terminal windows.

1602
01:45:27,320 --> 01:45:29,600
So I think it's crowdsourcing is mainly

1603
01:45:29,600 --> 01:45:32,360
good for projects that are labor intensive.

1604
01:45:32,360 --> 01:45:37,200
But I think AI needs individual creativity more.

1605
01:45:37,200 --> 01:45:39,720
It's like McCarthy said, it needs

1606
01:45:39,720 --> 01:45:44,760
2.3 Einstein's and 1.7 Hatton projects.

1607
01:45:44,760 --> 01:45:48,600
In fact, you know, it's probably good for the Hatton projects

1608
01:45:48,600 --> 01:45:51,400
but not for the Unix studies.

1609
01:45:51,400 --> 01:45:55,240
On the other hand, given we have the movie,

1610
01:45:55,240 --> 01:45:58,080
it might be that the problem of getting this code

1611
01:45:58,080 --> 01:46:01,880
to make that movie isn't so creative.

1612
01:46:01,880 --> 01:46:07,080
You could start it up and see where it gets stuck.

1613
01:46:07,080 --> 01:46:09,120
And it's worth it.

1614
01:46:09,120 --> 01:46:11,440
A great possibility is if you have something

1615
01:46:11,440 --> 01:46:14,680
long-winds of Watson that incorporates lots and lots

1616
01:46:14,680 --> 01:46:16,920
of small programs, then you can have

1617
01:46:16,920 --> 01:46:18,480
people contribute small programs.

1618
01:46:18,480 --> 01:46:21,000
And whether they're good or bad, the system

1619
01:46:21,000 --> 01:46:23,080
can figure it out in the more than one.

1620
01:46:23,080 --> 01:46:25,640
Well, in some sense, Watson was crowdsourced

1621
01:46:25,640 --> 01:46:28,920
because it wasn't only developed by that IBM group

1622
01:46:28,920 --> 01:46:34,200
that they had lots of collaborators in ISI and CMU

1623
01:46:34,200 --> 01:46:35,520
and other places.

1624
01:46:35,520 --> 01:46:39,360
And they crowdsourced them by giving them little research

1625
01:46:39,360 --> 01:46:42,240
grants to integrate their employee with the research

1626
01:46:42,240 --> 01:46:44,080
into Watson.

1627
01:46:44,080 --> 01:46:49,360
So I think you could argue that that actually was crowdsourced.

1628
01:46:49,360 --> 01:46:51,480
Another thing we haven't tried is

1629
01:46:51,480 --> 01:46:53,000
called throwing money at it.

1630
01:46:55,680 --> 01:47:01,760
Suppose we got $5,000 or $1,000 and told some programmer,

1631
01:47:01,760 --> 01:47:03,120
can you get this to work?

1632
01:47:03,920 --> 01:47:05,920
Well, if you have good enough descriptions

1633
01:47:05,920 --> 01:47:09,920
of various small programs that you

1634
01:47:09,920 --> 01:47:13,920
can use for problem solving, then I

1635
01:47:13,920 --> 01:47:16,920
think that part of the problem is creativity.

1636
01:47:16,920 --> 01:47:19,920
Because some parts or some sub-programs

1637
01:47:19,920 --> 01:47:22,920
could be more stupid.

1638
01:47:22,920 --> 01:47:24,920
As long as you have a good description

1639
01:47:24,920 --> 01:47:26,920
of what that program is doing, then you

1640
01:47:26,920 --> 01:47:29,920
could have some really creative program.

1641
01:47:30,720 --> 01:47:35,720
That might use those stupid programs as part of its reasoning.

1642
01:47:35,720 --> 01:47:37,720
But you still need that one creative person

1643
01:47:37,720 --> 01:47:38,720
to make the decision.

1644
01:47:38,720 --> 01:47:40,720
Well, you still need that one creative person

1645
01:47:40,720 --> 01:47:41,720
to make the decision.

1646
01:47:41,720 --> 01:47:44,720
But you don't get so many if you organize it in Watson.

1647
01:47:44,720 --> 01:47:45,720
Yeah.

1648
01:47:45,720 --> 01:47:47,720
I'm going to just label it Bowman Warcraft 10.

1649
01:47:47,720 --> 01:47:48,720
Leave it on the internet.

1650
01:47:48,720 --> 01:47:51,520
Bowman Warcraft 10, leave it on the internet.

1651
01:48:05,520 --> 01:48:14,120
My grandson was suspended from Warcraft for three weeks

1652
01:48:14,120 --> 01:48:17,200
because he hacked the thing to get a higher

1653
01:48:17,200 --> 01:48:18,680
priority on something.

1654
01:48:21,960 --> 01:48:25,520
I think he was not.

1655
01:48:25,520 --> 01:48:28,840
Do you know how old he was?

1656
01:48:28,840 --> 01:48:30,160
Yeah.

1657
01:48:30,160 --> 01:48:31,000
No, miles.

1658
01:48:34,600 --> 01:48:38,600
No, I think he was about 10 or 12.

1659
01:48:38,600 --> 01:48:41,120
And he had actually managed to get into the thing

1660
01:48:41,120 --> 01:48:43,360
and get instant service.

1661
01:48:48,080 --> 01:48:49,880
He was very proud of being banned.

1662
01:49:00,880 --> 01:49:03,480
I give up.

1663
01:49:03,480 --> 01:49:06,400
Any last request or idea?

1664
01:49:11,360 --> 01:49:13,400
Thanks for coming.

